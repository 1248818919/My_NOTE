{"_id":"note:yd_fVndplf","title":"GPTv2","content":"# GPTv2 论文阅读笔记\n\nRadford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.\n\n## 1.OverView\n\n创建了WEBTEXT数据集，在Zero shot任务上表现比较优越。作者将原来卷精度转移到了卷Zero shot精度上面。 首先摘要中讲到他们的语言模型在一个包含数百万网页的WebText数据集上没有任何显式监督的情况下进行学习。提到了语言模型的容量是零次学习任务成功的关键，且呈对数线性关系。他们的GPT-2模型是一个有1.5B(15亿)参数的Transformer，在zero-shot setting下，在8个任务中的7个任务上都取得的sota。\n\n\n## 2.Introduction\n当前的机器学习对数据分布的微小变化脆弱而敏感，是狭隘的专家模型，而不是一个多面手。我们的目标是构建能执行很多任务的通用系统，最终不需要为每个任务创建和标准训练集。\n\n我们任务单任务，单领域任务的流行造成当前的机器学习系统缺乏泛化能力。而多任务学习是一个提升通用能力的很有前途的一个框架。但NLP中的多任务学习仍处于起步阶段，另一方面，当前的预训练和无监督微调的方式很流行，但仍需要监督训练。这篇工作连接这两个主线，证明语言模型可以在zero-shot setting下，不需要任何参数或架构修改执行下游任务，并取得相当不错的效果。\n\n## 3.Approach\n\n### 3.1 Model\nGPTv2的模型有稍微的变化，左边是原始的transformer模型，右边是经过改进的模型。Follow GPT, 使用Transformer Decoder，有些小的修改，将Layer Normalization一次每个sub-block的输入，类似于预激活残差网络，最后一个self-attention block之后加一个LN。使用了一个修正的初始化，考虑了残差路径岁模型深度的累积，将残差层的权重乘以$1/\\sqrt{N}$, N是残差层数。vocab size扩展到50257，将上下文大小从512增加到1024，batch size = 512。\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv2/pic1.png?raw=true)\n\n### 3.2 dataset\n\n在Reddit上爬取的外链，构建了WebText数据集，包含了这4500万个链接的文字子集，移除了所有的Wikipedia文档，因为它是很多下游任务的数据源，这是为了避免数据集重叠而影响评估。\n\n### 3.3 Input Representation\n\n一个通用的语言模型应该能够计算任何字符串的概率及生成字符串。当前的byte-level的LMs在大规模数据集如One Billion Word Benchmark上比不上word-level的LMs。\n\nByte Pair Encoding是character-level和word-level语言建模的折中方案，它高效的在频繁符号序列的word-level输入和不频繁符号序列的character-level输入之间进行插值。使用BPE实现时，经常操作Unicode码位，而不是字节序列。这导致在添加多符号token前，基本词汇表就超过13万，这与BPE经常使用的32000到64000个token差距巨大，相比之下，byte-level version的BPE只需要256 vacab size。然后，直接将BPE应用于字节序列会导致次优合并，因为BPE使用基于贪婪频率的启发式方法来构建词汇表。我们发现BPE包括常见单词的许多版本，比如 dog.  dog!  dog?，这导致了有限词汇表大学和模型容量的次优搭配。为了防止这个，我们阻止BPE合并任何字节序列的字符类别。\n\n这种输入表示使得我们将word-level LMs的经验优势和byte-level方法的通用性结合起来。由于我们的方法可以为任何Unicode字符串分配一个概率，这使得我们能够在任何数据集上，不考虑预处理，分词和词汇表大小，就能评估我们的语言模型。\n\n关于具体的BPE可以参见\n> https://zhuanlan.zhihu.com/p/424631681\n\n### 4.Experiment\n\n模型超参数：\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv2/pic2.png?raw=true)\n\n最小的模型和原始GPT相同大小，次小的模型和BERT最大模型相同，最大模型叫做GPT-2，比原始GPT参数量高一个数量级。\n\n由于模型在字节级别上操作，不需要有损的预处理和分词，可在任何语言模型benchmark上评估。\n\nGPT-2评估的任务包括：\n- Children's Book Test：用于检查LM使在不同类别单词(命名实体，名词，动词，介词)上性能。\n- LAMBADA：测试模型对文本中的长期依赖关系的建模能力。\n- Wingrad Schema challenge：测试模型解决文本中歧义的能力来衡量模型的常识推理能力。\n- Reading Comprehension: Tne Conversation Question Answering dataset (CoQA)\n- Summarization: CNN, Daily Mail dataset\n- Translation：WMT-14 English-French test set\n- QA：SQUAD\n\n评估结果： \n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv2/pic3.png?raw=true)\n\n## 5.Generalization vs Memorization\n\n这部分主要在分析多少的下游训练数据出现在预训练数据中，计算了每个任务的训练集中和WebText的8-grams的重叠比例。\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv2/pic4.png?raw=true)\n\n## 6.Conclusion\n当一个大学语言模型在足够大的多样化数据集上进行无监督训练，就能在很多领域数据集上执行任务。GPT-2取得了zero-shot setting下8个任务中7个sota的成绩。","tags":[],"folderPathname":"/NLP/GPT","data":{},"createdAt":"2024-01-08T03:29:08.781Z","updatedAt":"2024-01-08T13:07:09.339Z","trashed":false,"_rev":"matOS9gZs"}