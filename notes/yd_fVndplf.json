{"_id":"note:yd_fVndplf","title":"GPTv2","content":"# GPTv2 论文阅读笔记\n\nRadford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.\n\n## 1.OverView\n\n创建了WEBTEXT数据集，在Zero shot任务上表现比较优越。作者将原来卷精度转移到了卷Zero shot精度上面。 首先摘要中讲到他们的语言模型在一个包含数百万网页的WebText数据集上没有任何显式监督的情况下进行学习。提到了语言模型的容量是零次学习任务成功的关键，且呈对数线性关系。他们的GPT-2模型是一个有1.5B(15亿)参数的Transformer，在zero-shot setting下，在8个任务中的7个任务上都取得的sota。\n\n\n## 2.Introduction\n当前的机器学习对数据分布的微小变化脆弱而敏感，是狭隘的专家模型，而不是一个多面手。我们的目标是构建能执行很多任务的通用系统，最终不需要为每个任务创建和标准训练集。\n\n我们任务单任务，单领域任务的流行造成当前的机器学习系统缺乏泛化能力。而多任务学习是一个提升通用能力的很有前途的一个框架。但NLP中的多任务学习仍处于起步阶段，另一方面，当前的预训练和无监督微调的方式很流行，但仍需要监督训练。这篇工作连接这两个主线，证明语言模型可以在zero-shot setting下，不需要任何参数或架构修改执行下游任务，并取得相当不错的效果。\n\n## 3.Approach\n\n![model]()","tags":[],"folderPathname":"/NLP/GPT","data":{},"createdAt":"2024-01-08T03:29:08.781Z","updatedAt":"2024-01-08T12:41:11.889Z","trashed":false,"_rev":"qoCMl9NvL"}