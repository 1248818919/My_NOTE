{"_id":"note:VjZYgxWRQL","title":"DCI","content":"# DCI 论文阅读笔记\n\nUrbanek J, Bordes F, Astolfi P, et al. A picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 26700-26709.\n\n# 1.Introduction\n\n海量视觉语言数据集的管理方法需要在数据集大小和质量之间进行权衡。然而，即使是最高质量的可用策展标题也太短，无法捕捉图像中丰富的视觉细节。为了展示密集和高度对齐的图像-文本对的价值，我们收集了密集字幕图像(DCI)数据集，该数据集包含7805张自然图像，这些图像经过人工注释，每个描述平均超过1000个单词。通过与图像的特定部分相关联的精确和可靠的字幕，我们可以评估视觉语言模型(VLMs)对图像内容的理解，并将每个字幕与其相应的子裁剪相匹配。由于目前的模型通常仅限于77个文本标记，我们还引入了一个总结版本(sDCI)，其中每个标题的长度都是有限的。我们表明，在标准基准测试上取得进展的现代技术与基于sDCI的基准测试的显著改进并不相符。最后，我们使用sDCI对CLIP进行微调，尽管训练集很小，但仍显示出比基线有显著改善。通过发布第一个人类标注的密集图像字幕数据集，我们希望能够为下一代vlm开发新的基准或微调配方。\n\n## 2.1 Preparation\n\n为了收集数据，我们首先从SA-1B的随机隐私缓解子集中选择图像。然后，我们按程序提取每个图像的子区域进行注释，因为我们在最初的试验中发现，同时将区域和描述众包会使任务和成功注释率过于复杂。对于这个过程，我们转向分割任何模型(SAM)[15]，并采用他们的标准方法从图像中提取所有蒙版。\n\n在提取过程中，SAM通常依赖于整个图像上的点网格。为了增加选择值得注释的有趣区域的可能性，我们额外应用了一个精明的过滤器[4]，并从发现的边缘中选择半径内的随机点。然后我们运行SAM来检测使用网格和近边缘点的所有遮罩。一旦掩码返回，我们通过设定两个掩码之间重叠像素的阈值来建立子掩码的层次结构，以确定一个是另一个的子掩码，或者两个掩码是否应该连接。这有助于减少自动遮罩过程中引入的一些噪音，并为我们留下一个树形结构的遮罩。最后，我们移除任何太小的遮罩。我们注意到，经历这个过程并不会导致每个图像的每个细节都被选为注释的候选对象，并且在DCI数据集中的这样的实例不期望对图像中可以识别或讨论的所有元素具有完整的子掩码匹配覆盖。","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Caption","data":{},"createdAt":"2024-07-18T08:44:42.785Z","updatedAt":"2024-07-18T09:12:25.626Z","trashed":false,"_rev":"ORhqahGRI"}