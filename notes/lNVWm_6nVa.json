{"_id":"note:lNVWm_6nVa","title":"RWKV-CLIP","content":"# RWKV-CLIP 论文阅读笔记\n\n> Gu T, Yang K, An X, et al. RWKV-CLIP: A Robust Vision-Language Representation Learner[J]. arXiv preprint arXiv:2406.06973, 2024.\n\n## 1.Abstract\n\n对比语言-图像预训练(CLIP)通过使用从网站获取的图像-文本对扩展数据集，显著提高了各种视觉语言任务的性能。本文从数据和模型架构的角度进一步探讨了CLIP。为了解决噪声数据的普遍存在并提高从互联网抓取的大规模图像文本数据的质量，我们引入了一个多样化的描述生成框架，该框架可以利用大型语言模型(llm)来合成和精炼基于web的文本、合成字幕和检测标签的内容。此外，我们提出了RWKV-CLIP，这是第一个rwkv驱动的视觉语言表示学习模型，它结合了变压器的有效并行训练和rnn的有效推理。在各种模型尺度和预训练数据集上的综合实验表明，RWKV-CLIP是一种鲁棒且高效的视觉语言表征学习器，它在线性探测、零采样分类和零采样图像文本检索等下游任务中实现了最先进的性能。\n\n为了方便未来的研究，代码和预训练模型在https: //github.com/deepglint/RWKV-CLIP上发布\n\n## 2.Method\n\n### 2.1 数据集搭建方法\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Datasets","data":{},"createdAt":"2024-06-19T07:32:30.364Z","updatedAt":"2024-06-19T07:57:14.108Z","trashed":false,"_rev":"BJ2zwTE9a"}