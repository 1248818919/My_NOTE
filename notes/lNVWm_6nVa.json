{"_id":"note:lNVWm_6nVa","title":"RWKV-CLIP","content":"# RWKV-CLIP 论文阅读笔记\n\n> Gu T, Yang K, An X, et al. RWKV-CLIP: A Robust Vision-Language Representation Learner[J]. arXiv preprint arXiv:2406.06973, 2024.\n\n## 1.Abstract\n\n对比语言-图像预训练(CLIP)通过使用从网站获取的图像-文本对扩展数据集，显著提高了各种视觉语言任务的性能。本文从数据和模型架构的角度进一步探讨了CLIP。为了解决噪声数据的普遍存在并提高从互联网抓取的大规模图像文本数据的质量，我们引入了一个多样化的描述生成框架，该框架可以利用大型语言模型(llm)来合成和精炼基于web的文本、合成字幕和检测标签的内容。此外，我们提出了RWKV-CLIP，这是第一个rwkv驱动的视觉语言表示学习模型，它结合了变压器的有效并行训练和rnn的有效推理。在各种模型尺度和预训练数据集上的综合实验表明，RWKV-CLIP是一种鲁棒且高效的视觉语言表征学习器，它在线性探测、零采样分类和零采样图像文本检索等下游任务中实现了最先进的性能。\n\n为了方便未来的研究，代码和预训练模型在https: //github.com/deepglint/RWKV-CLIP上发布\n\n## 2.Method\n\n### 2.1 数据集搭建方法\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA/pic1.jpg?raw=true)\n\n参考ALIP一文,首先采用$OFA_{base}$模型为每张图像生成一个syn Caption,为了捕获图像中更细粒度的语义信息，我们结合了开放集图像标记模型RAM++ (Huang et al .， 2023)来提取每个图像的目标检测标记。然后先进入chatgpt生成文本，然后再在LLaMA3上进行微调。\n\n### 2.2 模型方法\n\n就是将模型换成了RWKV-CLIP模型\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA/pic2.jpg?raw=true)\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Datasets","data":{},"createdAt":"2024-06-19T07:32:30.364Z","updatedAt":"2024-06-19T08:05:23.916Z","trashed":false,"_rev":"Y5yeLF7FB"}