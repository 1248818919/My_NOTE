{"_id":"note:plwktRixmd","title":"GLEE","content":"# GLEE 论文阅读笔记\n\n> Wu J, Jiang Y, Liu Q, et al. General object foundation model for images and videos at scale[J]. arXiv preprint arXiv:2312.09158, 2023.\n\n## 1.Abstract\n\n\n\n## 2.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/GLEE/pic1.jpg?raw=true)\n\nGLEE由一个图像编码器、一个文本编码器、一个视觉提示器和一个对象解码器组成，如图所示。视觉提示器在交互式分割期间将用户输入(如点、边界框或涂鸦)编码为目标对象的相应视觉表示。然后将它们集成到检测器中，根据文本和视觉输入从图像中提取物体。\n\n\n首先提取具有ResNet等骨架的多尺度特征Z。然后将它们输入到目标解码器中，对解码器的输出Embedding $q^d∈R^{N×C}$, 之后采用分类、检测和分割三个预测头。\n\n遵循其他对象分割模型，构建1/4分辨率像素嵌入图$M_{p}\\in\\mathcal{R}^{C\\times\\frac{H}{4}\\times\\frac{W}{4}}$,这是通过对主干网和变压器编码器的多尺度特征图进行上采样和融合得到的。最后，我们得到了每个二值掩码的预测结果$m\\in\\mathcal{R}^{N\\times\\frac{H}{4}\\times\\frac{W}{4}}$，通过N个掩模嵌入图与像素嵌入图之间的点积\n$$\nm=FFN(q_d)\\otimes M_p\n$$\n\n为了支持任意词汇表和对象$En_{c_L}$中，并使用每个句子标记的平均值作为每个类别或描述的输出文本嵌入$e^t∈R^{K×D}$。然后我们计算对象嵌入和文本嵌入之间的对齐分数$S_{align}\\in\\mathcal{R}^{N\\times K}$:\n$$\nS_{align}=q_d\\cdot W_{i2t}\\otimes e_t\n$$\n\n其中$W_{i2t}\\in\\mathcal{R}^{C\\times D}$为图像到文本的投影权值。","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-05-05T02:46:24.951Z","updatedAt":"2024-05-05T04:41:46.800Z","trashed":false,"_rev":"Ag3uEYZvA"}