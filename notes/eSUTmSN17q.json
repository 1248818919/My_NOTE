{"_id":"note:eSUTmSN17q","title":"BLIP-2","content":"# BLIP-2 论文阅读笔记\n\n> Li J, Li D, Savarese S, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models[C]//International conference on machine learning. PMLR, 2023: 19730-19742.\n\n## 1.Abstract\n\n由于大规模模型的端到端训练，视觉和语言预训练的成本变得越来越高。本文提出了一种通用且高效的预训练策略BLIP-2，它从现成的冻结预训练图像编码器和冻结的大型语言模型中引导视觉语言预训练。BLIP-2使用轻量级查询转换器弥补了模式上的差距，该转换器分两个阶段进行预训练。第一阶段从固定图像编码器中引导视觉语言表示学习。第二阶段从一个固定的语言模型中引导视觉到语言的生成学习。尽管比现有方法具有更少的可训练参数，但BLIP-2在各种视觉语言任务上实现了最先进的性能。例如，我们的模型在零射击VQAv2上的表现比Flamingo80B高出8.7%，可训练参数减少了54倍。我们还演示了该模型可以遵循自然语言指令的零镜头图像到文本生成功能。\n\n## 2.Method\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Foundation Model","data":{},"createdAt":"2024-08-02T02:08:55.316Z","updatedAt":"2024-08-02T07:23:57.465Z","trashed":false,"_rev":"u_bpcdPP5"}