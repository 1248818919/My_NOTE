{"_id":"note:AgiPLIih-f","title":"GPTv3","content":"# GPTv3 论文阅读笔记\n\nBrown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.\n\n## 1.Introduction\n\n最近的工作表明，通过在大量文本语料库上进行预训练，然后在特定任务上进行微调，在许多NLP任务和基准上取得了实质性的进展。但是这种方法需要特定的大量数据集和根据任务进行微调。消除这个限制是必要的，因为如下的原因：\n\n- 对于许多这样的任务来说，很难收集到一个大型的监督训练数据集，特别是当每个新任务都必须重复这个过程时。\n- 其次，利用训练数据中虚假相关性的可能性从根本上随着模型的表达能力和训练分布的狭窄而增长。大模型在小任务上表现良好并不能说明他的泛化性能优越，可能是过拟合产生的\n- 对于某些任务来说，人类并不需要过多的数据集。\n\n作者定义了Meta-learning和in-context learning两个概念，前者是说训练一个大模型，后者是说不通过梯度更新的方式，仅仅依靠对话来让模型进行学习。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv3/pic1.jpg?raw=true)\n\n模型大小的增加会提升模型在文本生成和下游NLP任务的性能。\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv3/pic3.jpg?raw=true)\n\nGPT-3在许多NLP数据集上都有很强的表现，包括翻译、问答和完形填空任务，以及一些需要实时推理或领域适应的任务，如解乱单词、在句子中使用新单词或执行3位数算术。与此同时，我们也发现了一些GPT-3的少数镜头学习仍然挣扎的数据集，以及一些GPT-3面临与大型网络语料库训练相关的方法问题的数据集。最后，我们发现GPT-3可以生成新闻文章的样本，人类评估者很难将其与人类撰写的文章区分开来。\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv3/pic2.jpg?raw=true)\n\n## 2.Approach\n\n### 2.1 Model and Architectures\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/GPTv3/pic4.jpg?raw=true)\n\n模型还是采用了GPTv2的架构，但是结合了Sparse Transformer的改进。\n\n### 2.2 Training Dataset\n\n还是使用了Common Crawl dataset这个数据集，不过作者使用了如下的改进，第一步是利用线性回归剔除了质量不高的语料文本，第二个是做去重的工作，使用了LSH算法，再往其中加了高质量的数据集，相当于多个数据集的融合。没压缩前是45TB，压缩后是540GB，采样比例如下","tags":[],"folderPathname":"/NLP/GPT","data":{},"createdAt":"2024-01-10T02:33:08.102Z","updatedAt":"2024-01-11T01:37:02.598Z","trashed":false,"_rev":"YcTFbYTE0"}