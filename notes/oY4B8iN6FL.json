{"_id":"note:oY4B8iN6FL","title":"TransCenter","content":"# TransCenter 论文阅读笔记\n\n> Xu Y, Ban Y, Delorme G, et al. TransCenter: Transformers with dense representations for multiple-object tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, 45(6): 7820-7835.\n\n## 1.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/TransCenter/pic1.jpg?raw=true)\n\n给定$t$时刻和$t-1$时刻的RGB图片，然后送入参数共享的Transformer编码器中，他们是QLN的输入，产生检测queries，检测Memory，追踪queries，追踪Memory。此外，TransCenter Decoder利用可变形 Transformer，用于将DQ/TQ与DM/TM相关联。具体来说，TQ在TransCenter Decoder的交叉注意模块中与TM相互作用，从而产生跟踪特征(TF)。同样，检测特征(DF)是DQ和DM之间交叉注意的输出。为了产生输出密集表示，DF用于估计对象大小St和中心热图Ct。同时用TF估计跟踪位移Tt\n\n### 1.1 QLN：Query Learning Networks\n\nDQ是密集的和图像相关的，可以精确和丰富地发现物体的位置。TQ是稀疏的，目的是寻找两个不同帧之间的物体位移，因此TQ和TM应该由不同时间步长的输入特征产生。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/TransCenter/pic2.jpg?raw=true)\n\n### 1.2 TransCenter Decoder\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/TransCenter/pic3.jpg?raw=true)\n\n### 1.3 The Center, the Size, and the Tracking Branches\n\nCenter Branches 和 the Size Branches，区别是一个是$$\\mathbf{C}_{t}\\in[0,1]^{H/4\\times W/4}$$，一个是$$\\mathbf{S}_{t}\\in\\mathbb{R}^{H/4\\times W/4\\times2}$$\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/TransCenter/pic4.jpg?raw=true)\n\n对于跟踪branch，TF是稀疏的，和$t-1$拥有相同的大小，一个track query对应一个$t-1$时刻跟踪轨迹。TF连同物体在t−1处的位置(在稀疏TQ的情况下)或中心热图Ct−1和DF(密集TQ)被输入到两个具有ReLU激活的全连接层。后一层预测相邻帧中t−1处物体的水平和垂直位移Tt。\n\n## 2.Result\n\n\n\n\n\n\n\n## 3.Code\n\n**encoder进行编码**\n\n    def forward(self, src, pre_src=None, pre_cts=None, pre_memories=None, masks_flatten=None):\n        assert pre_src.tensors.shape == src.tensors.shape\n        spatial_shapes = []\n        memories = []\n        hs = []\n        gather_feat_t = None\n        pre_reference_points = []\n        \n        # mask添加\n        if masks_flatten is None:\n            masks_flatten = []\n        \n        # 是由使用前一帧的东西\n        if pre_memories is None:\n            no_pre = True\n        else:\n            no_pre = False\n        \n        # 使用特征金字塔进行特征提取\n        src_tensor = src.tensors\n        b, c, h, w = src_tensor.shape\n        h, w = h//2, w//2\n        with torch.cuda.amp.autocast(self.half):\n            outs = self.pvt_encoder(src_tensor)\n            \n        if no_pre:\n            with torch.no_grad():\n                pre_src_tensor = pre_src.tensors\n                with torch.cuda.amp.autocast(self.half):\n                    pre_outs = self.pvt_encoder(pre_src_tensor)\n                # pre info for the decoder\n                pre_memories = []\n\n\n** QLN 网络 **\n        \n        # 特征金字塔有四层，通道数分别是32,64,128,256\n        for stage in range(4):\n            # 1/(2**(stage+3))\n            h, w = h // 2, w // 2\n\n            # for detection memory we use 1/4 当前的memory\n            # input proj for projecting all channel numbers to hidden_dim \n            with torch.cuda.amp.autocast(self.half):\n                hs.append(self.input_proj[stage](outs[stage]))\n            memories.append(hs[-1].flatten(2).transpose(1, 2).detach().clone())\n\n            spatial_shapes.append((h, w))\n            if isinstance(masks_flatten, list):\n                # todo can be optimized #\n                # get memory with src #\n                mask = src.mask.clone()\n                mask = F.interpolate(mask[None].float(), size=(h, w)).to(torch.bool)[0]\n                # for inference speed up\n                masks_flatten.append(mask.flatten(1))\n\n            # only gather 1/4 appearance features #\n            if stage == 0:\n                # get pre_memory with pre_src #\n                if no_pre:\n                    with torch.no_grad():\n                        # # Prepare pre_mask, valid ratio, spatial shape #\n                        pre_memory = self.input_proj[stage](pre_outs[stage]).detach()\n                else:\n                    pre_memory = pre_memories[0]\n                \n                # 针对第一stage，进行调整\n                if len(pre_memory.shape) == 3:\n                    b, h_w, c = pre_memory.shape\n                    # to bchw\n                    pre_memory = pre_memory.view(b, h, w, c).permute(0, 3, 1, 2).contiguous()\n\n                # gather pre_features and reference pts #\n                # todo can use box and roi_aligned here.\n                assert pre_memory.shape[2] == h and pre_memory.shape[3] == w\n                # (x,y) to index，提供采样的点\n                pre_sample = pre_cts.clone()\n                pre_sample[:, :, 0].clamp_(min=0, max=w - 1)\n                pre_sample[:, :, 1].clamp_(min=0, max=h - 1)\n\n                pre_sample[:, :, 0] /= w\n                pre_sample[:, :, 1] /= h\n                \n                # https://blog.csdn.net/qq_40968179/article/details/128093033\n                # 上面这个连接可以看到这个函数的作用\n                # 就是提供一个输入和一个网格进行采样\n                gather_feat_t = F.grid_sample(pre_memory, (2.0 * pre_sample - 1.0).unsqueeze(1),\n                                              mode='bilinear', padding_mode='zeros', align_corners=False)[:, :, 0, :].transpose(1, 2)\n\n                # make reference pts #\n                pre_reference_points.append(pre_sample)\n\n        if no_pre:\n            del pre_outs\n        del outs\n        # print(spatial_shapes)\n        # 存储当前图片大小信息\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src.tensors.device)\n        # 生成一个spatial_shapes 形状相同的全零张量\n        level_start_indexes = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n\n        # transform to queries #\n        # 根据提取的特征进行加工，对应QLN中的FFN\n        pre_query_embed = self.my_forward_ffn(gather_feat_t).float() \n        # 参考点进行堆叠\n        pre_reference_points = torch.stack(pre_reference_points, dim=2).repeat(1,1,4,1)\n\n        if isinstance(masks_flatten, list):\n            masks_flatten = torch.cat(masks_flatten, 1)\n\n        # decoder\n        # pre_query_embed is from sampled features of pre_memories,\n        # pre_ref_pts are tracks pos at t-1, these are queries to question memory t\n        # print([m.shape for m in masks_flatten])\n        # 对应TransCenter的Decoder\n        pre_hs = self.decoder(pre_tgt=pre_query_embed,\n                              src_spatial_shapes=spatial_shapes, src_level_start_index=level_start_indexes,\n                              pre_query_pos=pre_query_embed, src_padding_mask=masks_flatten,\n                              src=torch.cat(memories, 1).float(), pre_ref_pts=pre_reference_points)\n\n        # for inference speed up #\n        return [[hs, pre_hs]], memories, gather_feat_t, masks_flatten\n\n\n** TransCenter Decoder **\n\n    class DeformableTransformerDecoderLayer(nn.Module):\n        def __init__(self, d_model=256, d_ffn=1024, dropout=0.1, activation=\"relu\", n_levels=4, n_heads=8, n_points=4):\n            super().__init__()\n\n            # self attention\n            # 对应图中的TQSA\n            self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n            self.dropout2 = nn.Dropout(dropout)\n            self.norm2 = nn.LayerNorm(d_model)\n\n            # cross attention\n            # 对应图中的TDQA\n            self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n            self.dropout1 = nn.Dropout(dropout)\n            self.norm1 = nn.LayerNorm(d_model)\n\n            # ffn pre\n            self.pre_linear1 = nn.Linear(d_model, d_ffn)\n            self.pre_dropout3 = nn.Dropout(dropout)\n            self.pre_linear2 = nn.Linear(d_ffn, d_model)\n            self.pre_dropout4 = nn.Dropout(dropout)\n            self.pre_norm3 = nn.LayerNorm(d_model)\n            self.activation = _get_activation_fn(activation)\n\n        @staticmethod\n        def with_pos_embed(tensor, pos):\n            return tensor if pos is None else tensor + pos\n\n        def forward_ffn_pre(self, pre_tgt):\n            pre_tgt2 = self.pre_linear2(self.pre_dropout3(self.activation(self.pre_linear1(pre_tgt))))\n            pre_tgt = pre_tgt + self.pre_dropout4(pre_tgt2)\n            pre_tgt = self.pre_norm3(pre_tgt)\n            return pre_tgt\n\n        def forward(self, pre_tgt, pre_query_pos, src_spatial_shapes,\n                    level_start_index,  src_padding_mask=None, src=None, pre_ref_pts=None):\n            # self attention #\n            # print(\"pre tgt.shape\", pre_tgt.shape)\n            # 很诡异，不知道为什么这里的embed是这样的！\n            q = k = self.with_pos_embed(pre_tgt, pre_query_pos)\n            # print(\"q.sum() \", q.sum())\n            # print(\"k.sum() \", k.sum())\n            pre_tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), pre_tgt.transpose(0, 1))[0].transpose(0, 1)\n            # print(\"pre_tgt2.sum() \", pre_tgt2.sum())\n            pre_tgt = self.norm2(pre_tgt + self.dropout2(pre_tgt2))\n            # print(\"pre_tgt.sum()  1 \", pre_tgt.sum())\n\n            # cross attention, find objects at t with queries at t-1 #\n            pre_tgt = pre_tgt + self.dropout1(self.cross_attn(self.with_pos_embed(pre_tgt, pre_query_pos),\n                                                              pre_ref_pts, src, src_spatial_shapes, level_start_index,\n                                                              src_padding_mask))\n            # ffn: 2 fc layers with dropout, 256 -> 1024-> 256\n            return self.forward_ffn_pre(self.norm1(pre_tgt))\n\n\n    class DeformableTransformerDecoder(nn.Module):\n        def __init__(self, decoder_layer, num_layers):\n            super().__init__()\n            self.layers = _get_clones(decoder_layer, num_layers)\n            self.num_layers = num_layers\n\n        # xyh #\n        def forward(self, pre_tgt, src_spatial_shapes, src_level_start_index,\n                    pre_query_pos=None, src_padding_mask=None, src=None, pre_ref_pts=None):\n\n            pre_output = pre_tgt\n            for lid, layer in enumerate(self.layers):\n                pre_output = layer(pre_tgt=pre_output, pre_query_pos=pre_query_pos,\n                                   src_spatial_shapes=src_spatial_shapes,\n                                   level_start_index=src_level_start_index,\n                                   src_padding_mask=src_padding_mask,\n                                   src=src, pre_ref_pts=pre_ref_pts)\n            # print(\"pre_output.sum() \", pre_output.sum())\n            return pre_output\n","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2024-01-30T02:18:45.382Z","updatedAt":"2024-02-01T10:55:41.059Z","trashed":false,"_rev":"r95WkJYCy"}