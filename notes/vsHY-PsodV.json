{"_id":"note:vsHY-PsodV","title":"BEVFormer","content":"# BEVFormer 论文阅读笔记\n\n>Li Z, Wang W, Li H, et al. Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 1-18.\n\n## 1.Introduction\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/BEVFormer/pic1.jpg?raw=true)\n\n三维空间的感知在自动驾驶领域是十分重要的，常用的单目相机框架方法的缺点是单独处理不同的视图，无法跨相机捕获信息，导致性能和效率低下。所以我们需要一个更统一的框架，从多摄像机图像中提取整体表示。鸟瞰图（BEV）是周围场景的常用表示，因为它清楚地显示了物体的位置和规模，然而目前依赖depth信息的BEV框架对深度值或者深度分布准确性很敏感，所以作者利用transformer生成BEV；此外之前的研究中常常忽略时间维度的影响，所以作者也将他加了进去。\n\n- 我们提出了BEVFormer，这是一种时空变换编码器，它将多摄像机和/或时间戳输入投影到BEV表示中。凭借统一的纯电动汽车特征，我们的模型可以同时支持多个自动驾驶感知任务，包括3D检测和地图分割。\n- 我们设计了可学习的纯电动汽车查询以及空间交叉注意力层和时间自注意力层，分别查找来自跨摄像机的空间特征和来自历史BEV的时间特征，然后将它们聚合为统一的BEV特征。\n- 我们在多个具有挑战性的基准上评估了所提出的BEVFormer，包括nuScenes和Waymo。与现有技术相比，我们的BEVFormer始终实现了改进的性能。\n\n## 2.Related Work\n\n可变形的注意力：\n$$\n\\text{DeformAttn}(q,p,x)=\\sum_{i=1}^{N_{\\text{head}}}\\mathcal{W}_{i}\\sum_{j=1}^{N_{\\text{key}}}\\mathcal{A}_{ij}\\cdot\\mathcal{W}_{i}^{\\prime}x(p+\\Delta p_{ij}),\n$$\n\nwhere $q,p,x$ represent the query, reference point and input features, respectively. $i$ indexes the attention head, and $N_\\mathrm{head}$ denotes the total number of attention heads. $j$ indexes the sampled keys, and $N_\\mathrm{key}$ is the total sampled key number for each head. $W_i\\in \\mathbb{R} ^{C\\times ( C/H_\\mathrm{head}) }$ and $W_i^{\\prime}\\in \\mathbb{R} ^{( C/H_\\mathrm{head}) \\times C}$ are the Íearnable weights, where $C$ is the feature dimension. $A_{ij}\\in[0,1]$ is the predicted attention weight, and is normalized by $\\sum_{j=1}^{N_\\mathrm{key}}A_{ij}=1.\\:\\Delta p_{ij}\\in\\mathbb{R}^2$ are the predicted offsets to the reference point $p$. $x(p+\\Delta p_{ij})$ represents the feature at location $p+\\Delta p_{ij}$, which is extracted by bilinear interpolation as in Dai $etal$. \n\n## 3.BEVFormer\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/BEVFormer/pic2.jpg?raw=true)\n\n### 3.1 BEV Queries\n\nBEV Queries是一个可学习的参数$Q\\in\\mathbb{R}^{H\\times W\\times C}$,其中$Q_{p}\\in\\mathbb{R}^{1\\times C}$对应BEV平面上$p=(x,y)$这一个点，BEV平面中的每个网格单元对应于s米的真实世界大小，默认以汽车为中心。\n\n## 3.2 Spatial Cross-Attention\n\n首先，获取Query，将2D BEV平面（Query）转换成3D平面（图2），只取C通道上面的四个点，然后采样$N_{ref}$个点投影成2D view，，对于一个BEV查询，投影的二维点只能落在某些视图上，我们称之为$V_{hit}$。\n\n然后，获取Key，首先计算他在真实世界的坐标,s代表两个视图之间的比例尺\n\n$$\nx^{\\prime}=(x-\\frac W2)\\times s;\\quad y^{\\prime}=(y-\\frac H2)\\times s.\n$$\n\n由于真实世界中的坐标常常是3维的，所以作者在Z轴上预设值了一组点$\\{z_j^{\\prime}\\}_{j=1}^{N_{\\mathrm{ref}}}$,然后就得到了一组3D reference points$(x^{\\prime},y^{\\prime},z_{j}^{\\prime})_{j=1}^{N_{\\mathrm{ref}}}$,最后，我们通过相机的投影矩阵将3D参考点投影到不同的图像视图，可以写成：\n\n$$\n\\begin{aligned}\\mathcal{P}(p,i,j)&=(x_{ij},y_{ij})\\\\\\mathrm{where}\\space z_{ij}\\cdot\\begin{bmatrix}x_{ij}&y_{ij}&1\\end{bmatrix}^T&=T_i\\cdot\\begin{bmatrix}x'&y'&z'_j&1\\end{bmatrix}^T.\\end{aligned}\n$$\n\n$$\\text{Here},\\mathcal{P}(p,i,j)\\text{ is the 2D point on }i\\text{-th view projected from }j\\text{-th 3D point }(x^{\\prime},y^{\\prime},z_j^{\\prime}),T_i\\in\\mathbb{R}^{3\\times4}\\text{ is the known projection matrix of the }i\\text{-th camera}.$$\n\n## 3.3 Temporal Self-Attention\n\n对于给定的BEV queries $Q$和历史BEV特征图$B^{t-1}$,我们首先根据自我(以汽车为中心)运动将与$Q$对齐，以使同一网格处的特征对应于相同的真实世界位置。在这里，我们将对齐后的$B_{t−1}$称之为$B_{t−1}^{'}$\n$$\n\\mathrm{TSA}(Q_p,\\{Q,B_{t-1}^{\\prime}\\})=\\sum_{V\\in\\{Q,B_{t-1}^{\\prime}\\}}\\mathrm{DeformAttn}(Q_p,p,V)\n$$\n\n对于 Temporal Self-Attention 模块输入包含 prev_bev时，value = [prev_bev，bev_query]，对应的参考点 ref_point = [ref_2d + shift，ref_2d]；如果输入不包含 prev_bev时，value = [bev_query，bev_query]，对应的参考点ref_point = [ref_2d，ref_2d]。\n\n![deformable attention](https://pic3.zhimg.com/80/v2-0be2083bdeddf04affdc33949937c0ee_1440w.webp)\n\n## 4.Experiment\n\n作者在两个数据集上进行了测试 nuScenes，Waymo，测试的任务是3D目标检测和map segmentation\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/BEVFormer/pic3.jpg?raw=true)\n\n# Code\n\n这里展示一下他的投影方法\n\n    def point_sampling(self, reference_points, pc_range,  img_metas):\n        # NOTE: close tf32 here.\n        allow_tf32 = torch.backends.cuda.matmul.allow_tf32\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n        \n        # 还原坐标系到Lidar坐标系下。其中pc_range是BEV特征表征的真实的物理空间大小。\n        lidar2img = []\n        for img_meta in img_metas:\n            lidar2img.append(img_meta['lidar2img'])\n        lidar2img = np.asarray(lidar2img)\n        lidar2img = reference_points.new_tensor(lidar2img)  # (B, N, 4, 4)\n        reference_points = reference_points.clone() # (bs, 一个pillar的中point的数量, H*W, 3)\n\n        reference_points[..., 0:1] = reference_points[..., 0:1] * \\\n            (pc_range[3] - pc_range[0]) + pc_range[0]\n        reference_points[..., 1:2] = reference_points[..., 1:2] * \\\n            (pc_range[4] - pc_range[1]) + pc_range[1]\n        reference_points[..., 2:3] = reference_points[..., 2:3] * \\\n            (pc_range[5] - pc_range[2]) + pc_range[2]\n        \n        # 调整为齐次坐标\n        reference_points = torch.cat(\n            (reference_points, torch.ones_like(reference_points[..., :1])), -1)\n        \n        # Reference Points坐标投影到像素坐标系。\n        # (一个pillar的中point的数量, bs, H*W, 3)\n        reference_points = reference_points.permute(1, 0, 2, 3)\n        D, B, num_query = reference_points.size()[:3]\n        num_cam = lidar2img.size(1)\n        \n        # (D, B, num_cams, H*W, 4)\n        reference_points = reference_points.view(\n            D, B, 1, num_query, 4).repeat(1, 1, num_cam, 1, 1).unsqueeze(-1)\n\n        lidar2img = lidar2img.view(\n            1, B, num_cam, 1, 4, 4).repeat(D, 1, 1, num_query, 1, 1)\n        \n        reference_points_cam = torch.matmul(lidar2img.to(torch.float32),\n                                            reference_points.to(torch.float32)).squeeze(-1)\n        eps = 1e-5\n\n        bev_mask = (reference_points_cam[..., 2:3] > eps)\n        reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(\n            reference_points_cam[..., 2:3], torch.ones_like(reference_points_cam[..., 2:3]) * eps)\n\n        reference_points_cam[..., 0] /= img_metas[0]['img_shape'][0][1]\n        reference_points_cam[..., 1] /= img_metas[0]['img_shape'][0][0]\n\n        bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)\n                    & (reference_points_cam[..., 1:2] < 1.0)\n                    & (reference_points_cam[..., 0:1] < 1.0)\n                    & (reference_points_cam[..., 0:1] > 0.0))\n        if digit_version(TORCH_VERSION) >= digit_version('1.8'):\n            bev_mask = torch.nan_to_num(bev_mask)\n        else:\n            bev_mask = bev_mask.new_tensor(\n                np.nan_to_num(bev_mask.cpu().numpy()))\n        \n        # (num_cams, Bs, H*W, D, 4)\n        reference_points_cam = reference_points_cam.permute(2, 1, 3, 0, 4)\n        # (H*W, Bs, D, num_cams, 4)\n        bev_mask = bev_mask.permute(2, 1, 3, 0, 4).squeeze(-1)\n\n        torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n        torch.backends.cudnn.allow_tf32 = allow_tf32\n\n        return reference_points_cam, bev_mask","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-01-14T02:01:10.275Z","updatedAt":"2024-04-08T07:57:52.903Z","trashed":false,"_rev":"xTRPXt3dw"}