{"_id":"note:Ut_Gt_2hJJ","title":"Saving and loading checkpoints","content":"## checkpoint知识\n\n> A Lightning checkpoint contains a dump of the model’s entire internal state. Unlike plain PyTorch, Lightning saves everything you need to restore a model even in the most complex distributed training environments.\n\n> Inside a Lightning checkpoint you’ll find:\n- 16-bit scaling factor (if using 16-bit precision training)\n- Current epoch\n- Global step\n- LightningModule’s state_dict\n- State of all optimizers\n- State of all learning rate schedulers\n- State of all callbacks (for stateful callbacks)\n- State of datamodule (for stateful datamodules)\n- The hyperparameters (init arguments) with which the model was created\n- The hyperparameters (init arguments) with which the datamodule was created\n- State of Loops\n\n\n## 保存checkpoint\n\nLightning 会自动保存在当前工作文件夹下。\n\n    # simply by using the Trainer you get automatic checkpointing\n    trainer = Trainer()\n\n可以通过一下方式进行修改\n\n    # saves checkpoints to 'some/path/' at every epoch end\n    trainer = Trainer(default_root_dir=\"some/path/\")\n    \n## 加载断点\n\n    model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n\n    # disable randomness, dropout, etc...\n    model.eval()\n\n    # predict with the model\n    y_hat = model(x)\n    \n## 保存超参数\n\nLightningModule 会自动保存传递给 \\_init\\_ 的参数，保存的方法是调用self.save_hyperparameters()\n\n    class MyLightningModule(LightningModule):\n        def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n            super().__init__()\n            self.save_hyperparameters()\n            \n超参数保存在\"hyper_parameters\"这个关键字下\n\n    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n    print(checkpoint[\"hyper_parameters\"])\n    # {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n\n加载方式是\n\n    model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n    print(model.learning_rate)\n    \n如果中断后要更改超参数，可以采用如下方式\n\n    # if you train and save the model like this it will use these values when loading\n    # the weights. But you can overwrite this\n    LitModel(in_dim=32, out_dim=10)\n\n    # uses in_dim=32, out_dim=10\n    model = LitModel.load_from_checkpoint(PATH)\n\n    # uses in_dim=128, out_dim=10\n    model = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n\nIn some cases, we may also pass entire PyTorch modules to the \\__init\\__ method, which you don’t want to save as hyperparameters due to their large size. If you didn’t call self.save_hyperparameters() or ignore parameters via save_hyperparameters(ignore=...), then you must pass the missing positional arguments or keyword arguments when calling load_from_checkpoint method:\n\n    class LitAutoencoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            ...\n\n        ...\n\n\n    model = LitAutoEncoder.load_from_checkpoint(PATH, encoder=encoder, decoder=decoder)\n\n## 关闭checkpoint\n\n    trainer = Trainer(enable_checkpointing=False)\n    \n## 不加载权重进行训练\n\n相当于去掉了上述“加载断点”的代码，直接执行以下代码：\n\n    model = LitModel()\n    trainer = Trainer()\n\n    # automatically restores model, epoch, step, LR schedulers, etc...\n    trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")","tags":[],"folderPathname":"/Tutorial/Pytorch_Lightning/Basic/Add a Validation and test set","data":{},"createdAt":"2024-02-22T05:35:25.789Z","updatedAt":"2024-02-22T05:53:16.757Z","trashed":false,"_rev":"eEIcIIV7D"}