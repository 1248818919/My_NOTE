{"_id":"note:wb5dYEA235","title":"RK-Net","content":"# RK-Net论文阅读笔记\n\n>Lin J, Zheng Z, Zhong Z, et al. Joint representation learning and keypoint detection for cross-view geo-localization[J]. IEEE Transactions on Image Processing, 2022, 31: 3780-3792.\n\n## 1.Introduction\n\nGeo-localization通常被视为图像检索任务,已经广泛应用于现实世界的任务中，包括无人机导航、事件检测、无人机投递等。在寻找两座建筑之间的差异时，人类的视觉系统首先关注建筑的一般属性，如形状、风格和颜色。如果在粗略比较中难以从全局特征中识别出正面图像，则人类视觉系统进一步提取一些具有判别性的细粒度信息的视图不变关键点来确定正面候选图像。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic1.jpg?raw=true)\n\n- 我们提出了一种新的框架，称为RK-Net，可以在不需要额外注释的前提下共同学习判别表示并检测跨视图地理定位的关键点。\n- 我们设计了一个单元减法注意模块(USAM)作为RK-Net的主要组成部分，可以自动发现具有代表性的关键点。关键点迫使模型关注突出的区域，产生针对视点的健壮特征。\n- 我们的框架可以与大多数现有方法集成，并且可以显着提高性能，例如，增强基于全局的模型[18]和基于局部的AD hoc模型[24]。大量实验表明，该方法在大学-1652、CVUSA和CV ACT三个交叉视角地理定位数据集上取得了相当好的结果。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic2.jpg?raw=true)\n\n## 2.RELATED WORK\n\n略\n\n## 3.Method\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic3.jpg?raw=true)\n\n### 3.1 Unit Subtraction Convolution\n\n传统的卷积表示为：\n$$\nF^{\\prime}\\left(i,j\\right)=\\sum_{u,v\\in A}\\psi\\left(u,v\\right)\\cdot F\\left(i-u,j-v\\right)\n$$\n作者想的是对比方法，所以使用减法，将某个点与附近的点进行比较得到特征图，于是他将卷积变成了如下的形式：\n$$\nF'\\left(i,j\\right)=K\\cdot F\\left(i,j\\right)-\\sum_{u,v\\in A}F\\left(i-u,j-v\\right).\n$$\n上面这种形式可以使用卷积操作快速的实现\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic4.jpg?raw=true)\n\n### 3.2 Keypoint Attention Mask\n\n对于中间特征图$F_l\\in\\mathbb{R}^{c*h*w}$，作者为了将所有通道都纳入考虑范围，它采用了SumPool操作，其实就是很简单的将沿着通道维度将所有特征相加而已,产生Mask\n\n    import numpy as np\n\n    # 假设有3个样本，每个样本有4个特征\n    features = np.array([[1, 2, 3, 4],\n                         [2, 3, 4, 5],\n                         [3, 4, 5, 6]])\n\n    # 对每个样本的特征做sum pooling\n    pooled_features = np.sum(features, axis=1)\n\n    # 打印池化后的特征值\n    print(pooled_features)\n    \n    # 结果\n    [10 14 18]\n\n$$\nF_a=\\operatorname{SumPool}\\left(F_l\\right) \\\\\nM=\\mathrm{ReLU}\\left(\\mathrm{BN}\\left(\\mathrm{USC}\\left(F_a\\right)\\right)\\right)\n$$\n\n### 3.3 Residual Attention Fusion\n\n为了获得特别重要的区域，作者进行了Expand，$F_l'=F_l\\cdot\\operatorname{Expand}(M,c)$,为了方式丢失重要信息，作者再采用了的残差连接的方式$F_l^*=F_l+F_l^{\\prime}.$\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic5.jpg?raw=true)\n\n### 3.4 模型训练\n\n作者采用了使用了三分支的CNN网络模型，他们分别给卫星视角，无人机视角和街景视角使用，无人机视角和卫星视角的分支是参数共享的，作者使用ResNet-50作为骨干网络，最后的分类网络替换成FC+BN+Cls。\n\n损失函数使用的是instance loss，具体来说，实例损失将每个位置视为一个单独的类，并以分类的方式训练模型。\n\n$$\n\\begin{aligned}\np(I_{v}^{y})& =\\mathrm{SoftMax}(Cls(F_{v}(I_{v}^{y}))),  \\\\\nL_{instance}& =-log(p(I_{v}^{y})[y]), \n\\end{aligned}\n$$\n\n作者在ResNet-50的第一个和第二个stage中插入了USAM，然后训练的时间和推理的时间并没有大幅增加。\n\n## 4.Experiment\n\n作者使用了很多的损失函数来进行比较，结果如下\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic8.jpg?raw=true)\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic9.jpg?raw=true)\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic10.jpg?raw=true)","tags":[],"folderPathname":"/Computer Vision/Geo-localization","data":{},"createdAt":"2024-01-09T08:04:46.882Z","updatedAt":"2024-01-09T10:53:24.114Z","trashed":false,"_rev":"j7Bx11R7J"}