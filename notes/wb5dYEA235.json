{"_id":"note:wb5dYEA235","title":"RK-Net","content":"# RK-Net论文阅读笔记\n\n>Lin J, Zheng Z, Zhong Z, et al. Joint representation learning and keypoint detection for cross-view geo-localization[J]. IEEE Transactions on Image Processing, 2022, 31: 3780-3792.\n\n## 1.Introduction\n\nGeo-localization通常被视为图像检索任务,已经广泛应用于现实世界的任务中，包括无人机导航、事件检测、无人机投递等。在寻找两座建筑之间的差异时，人类的视觉系统首先关注建筑的一般属性，如形状、风格和颜色。如果在粗略比较中难以从全局特征中识别出正面图像，则人类视觉系统进一步提取一些具有判别性的细粒度信息的视图不变关键点来确定正面候选图像。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic1.jpg?raw=true)\n\n- 我们提出了一种新的框架，称为RK-Net，可以在不需要额外注释的前提下共同学习判别表示并检测跨视图地理定位的关键点。\n- 我们设计了一个单元减法注意模块(USAM)作为RK-Net的主要组成部分，可以自动发现具有代表性的关键点。关键点迫使模型关注突出的区域，产生针对视点的健壮特征。\n- 我们的框架可以与大多数现有方法集成，并且可以显着提高性能，例如，增强基于全局的模型[18]和基于局部的AD hoc模型[24]。大量实验表明，该方法在大学-1652、CVUSA和CV ACT三个交叉视角地理定位数据集上取得了相当好的结果。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/RK-Net/pic2.jpg?raw=true)\n\n## 2.RELATED WORK\n\n略\n\n## 3.Method\n### 3.1 Unit Subtraction Convolution\n\n传统的卷积表示为：\n$$\nF^{\\prime}\\left(i,j\\right)=\\sum_{u,v\\in A}\\psi\\left(u,v\\right)\\cdot F\\left(i-u,j-v\\right)\n$$\n作者想的是对比方法，所以使用减法，将某个点与附近的点进行比较得到特征图，于是他将卷积变成了如下的形式：\n$$\nF'\\left(i,j\\right)=K\\cdot F\\left(i,j\\right)-\\sum_{u,v\\in A}F\\left(i-u,j-v\\right).\n$$\n上面这种形式可以使用卷积操作快速的实现\n!","tags":[],"folderPathname":"/Computer Vision/Geo-localization","data":{},"createdAt":"2024-01-09T08:04:46.882Z","updatedAt":"2024-01-09T08:46:00.340Z","trashed":false,"_rev":"_Ry1Ntilh"}