{"_id":"note:jlYTqJauHm","title":"TrackFormer","content":"# TrackFormer\n\nMeinhardt T, Kirillov A, Leal-Taixe L, et al. Trackformer: Multi-object tracking with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 8844-8854.\n\n## 1.abstract\n\n多目标跟踪(MOT)是一项具有挑战性的任务，需要同时对目标轨迹的初始化、检测目标身份检测和目标时空轨迹的关联这三个任务进行推理。本文将此任务表述为帧到帧之间的预测问题，并提出了一种新的模型Trackformer。这是一种基于编码器-解码器Transformer架构的端到端可训练的MOT方法。该模型通过视频序列预测出一组轨迹预测，通过关注实现帧之间的数据关联。Transformer解码器从static object queries开始初始化新的轨道，并使用track queries在空间和时间上自动回归地跟踪现有的轨道。这两种查询类型都受益于自注意力以及编码器-解码器对全局帧级特征的关注，从而省去了任何额外的图形优化或运动和/或外观建模。Trackformer引入了一种新的关注跟踪范式，虽然其设计简单，但能够在多目标跟踪(MOT17和MOT20)和分割(MOTS20)任务上实现最先进的性能。\n\n## 2.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/TrackFormer/pic1.jpg?raw=true)\n\n以上是模型的打一个大概架构图，这里就不细看了。\n\n### Training\n\n这里主要介绍一下该模型是如何进行损失计算的。首先，作者训练模型的时候，只使用了前后两帧的相关信息，第一帧所预测的结果分配给对应的gt。从第二帧开始，模型预测的结果是$N_{object}+track queries$，其中track queries指的是上一帧中的预测目标，作者将最后得到的queries保存，通过MLP将其转换为bbox和cls，通过解决指派问题的算法来进行目标框的分配从而计算损失。如果上一时刻的gt与当前时刻的gt是相同的人，在第二帧计算损失的时候，直接进行指派，如果在第二帧中，这个人不见了，他的trackquery的gt设置成背景，只计算类别损失，如果当前时刻新出现了一个人，则通过较为复杂的指派算法来将其与$N_{object}$中的某个目标进行关联，最后的损失计算公式：\n\n$$\n\n\\mathcal{L}_{\\text{query}}=\\begin{cases}-\\lambda_{\\text{cls}}\\log\\hat{p}_i(c_{\\pi=i})+\\mathcal{L}_{\\text{box}}(b_{\\pi=i},\\hat{b}_i),&\\text{if}i\\in\\pi\\\\-\\lambda_{\\text{cls}}\\log\\hat{p}_i(0),&\\text{if}i\\notin\\pi.\\end{cases}\n\n$$\n\n### Augmentation\n\n- 前一帧的图像可能不是前一帧，而是尤其相近的时刻的图像代替\n- 随机移除一些track queries，使其成为当前时刻的新的目标\n- 在track queries中添加一些错误的queries，这些queries来自上一时刻被归为背景的的queries。","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2024-04-01T09:17:15.761Z","updatedAt":"2024-04-01T09:58:59.863Z","trashed":false,"_rev":"kmDxTJflr"}