{"_id":"note:BtmH8bC9QY","title":"BLIP","content":"# BLIP 论文阅读笔记\n\n> Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[C]//International conference on machine learning. PMLR, 2022: 12888-12900.\n\n## 1.Abastract\n\n视觉语言预训练(VLP)提高了许多视觉语言任务的性能。然而，大多数现有的预训练模型只擅长基于理解的任务或基于生成的任务。此外，性能的提高很大程度上是通过使用从网络收集的噪声图像-文本对扩展数据集来实现的，这是一种次优的监督来源。本文提出了一种新的VLP框架BLIP，它可以灵活地转移到视觉语言理解和生成任务中。BLIP通过引导标题有效地利用了带有噪声的web数据，其中标题生成合成标题，滤波器去除噪声。我们在广泛的视觉语言任务上取得了最先进的结果，例如图像文本检索(平均+2.7% recall@1)，图像字幕(CIDEr +2.8%)和VQA (VQA分数+1.6%)。当以零射击的方式直接应用于视频语言任务时，BLIP也表现出较强的泛化能力。\n\n## 2.Method\n\n我们提出了一个统一的VLP框架BLIP，用于从噪声图像-文本对中学习。本节首先介绍我们的新模型架构MED及其预训练目标，然后描述用于数据集引导的CapFilt。","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Foundation Model","data":{},"createdAt":"2024-08-01T10:18:35.908Z","updatedAt":"2024-08-01T10:25:02.136Z","trashed":false,"_rev":"sLOLj5yG8"}