{"_id":"note:BtmH8bC9QY","title":"BLIP","content":"# BLIP 论文阅读笔记\n\n> Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[C]//International conference on machine learning. PMLR, 2022: 12888-12900.\n\n## 1.Abastract\n\n视觉语言预训练(VLP)提高了许多视觉语言任务的性能。然而，大多数现有的预训练模型只擅长基于理解的任务或基于生成的任务。此外，性能的提高很大程度上是通过使用从网络收集的噪声图像-文本对扩展数据集来实现的，这是一种次优的监督来源。本文提出了一种新的VLP框架BLIP，它可以灵活地转移到视觉语言理解和生成任务中。BLIP通过引导标题有效地利用了带有噪声的web数据，其中标题生成合成标题，滤波器去除噪声。我们在广泛的视觉语言任务上取得了最先进的结果，例如图像文本检索(平均+2.7% recall@1)，图像字幕(CIDEr +2.8%)和VQA (VQA分数+1.6%)。当以零射击的方式直接应用于视频语言任务时，BLIP也表现出较强的泛化能力。\n\n## 2.Method\n\n我们提出了一个统一的VLP框架BLIP，用于从噪声图像-文本对中学习。本节首先介绍我们的新模型架构MED及其预训练目标，然后描述用于数据集引导的CapFilt。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/Foundation%20Model/BLIP/pic1.jpg?raw=true)\n\n1.图像-文本对比损失(ITC)激活单峰编码器。它的目的是通过鼓励正面的图像-文本对与负面的图像-文本对具有相似的表示来对齐视觉转换器和文本转换器的特征空间。它已被证明是提高视觉和语言理解的有效目标(Radford et al, 2021;Li et al .， 2021a)。我们遵循Li等人(2021a)的ITC损失，其中引入动量编码器来产生特征，并从动量编码器创建软标签作为训练目标，以解释负对中的潜在正。\n\n2.图像-文本匹配损失(ITM)激活图像接地文本编码器。它旨在学习图像-文本多模态表示，以捕获视觉和语言之间的细粒度对齐。ITM是一个二元分类任务，其中模型使用ITM头(线性层)来预测给定图像-文本对的多模态特征是正的(匹配的)还是负的(不匹配的)。为了找到更多信息丰富的负，我们采用Li等人(2021a)的硬负挖掘策略，其中批中具有较高对比相似性的负对更有可能被选择来计算损失。\n\n3.语言建模损失(LM)激活基于图像的文本解码器，其目的是生成给定图像的文本描述。它优化了交叉熵损失，训练模型以自回归的方式最大化文本的可能性。在计算损失时，我们使用0.1的标签平滑。与广泛用于VLP的MLM损失相比，LM使具有泛化能力的模型能够将视觉信息转换为连贯的字幕。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/Foundation%20Model/BLIP/pic2.jpg?raw=true)\n\n本文提出了一种提高文本语料库质量的新方法CapFilt。图3给出了CapFilt的示例。它引入了两个模块:一个用于生成给定web图像的标题的captioner，以及一个用于去除噪声图像-文本对的过滤器。captioner和filter都是从相同的预训练MED模型初始化的，并在COCO数据集上分别进行微调。\n\n调优是一个轻量级的过程。具体地说，captioner是一个基于图像的文本解码器。它与LM目标进行了微调，以解码给定图像的文本。给定web图像Iw, captioner生成合成字幕t，每个图像一个字幕。\n\n该滤波器是一个基于图像的文本编码器。它与ITC和ITM目标进行了微调，以了解文本是否与图像匹配。过滤器去除原始web文本Tw和合成文本Ts中的噪声文本，如果ITM头预测文本与图像不匹配，则认为文本是噪声文本。最后，我们将过滤后的图像文本对与人工注释的图像文本对结合起来形成一个新的数据集，我们使用它来预训练一个新的模型","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Foundation Model","data":{},"createdAt":"2024-08-01T10:18:35.908Z","updatedAt":"2024-08-01T14:30:47.247Z","trashed":false,"_rev":"7b6vN4JDj"}