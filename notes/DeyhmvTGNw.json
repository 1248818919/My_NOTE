{"_id":"note:DeyhmvTGNw","title":"DETR","content":"# DETR 论文阅读笔记\n\n>Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 213-229.\n\n## 1.Introduction\n\n目前detection任务都是通过在大量的提议，锚框和窗口中心定义代理回归和分类任务，这就导致了他们的性能很容易受到后处理步骤的影响（后处理用来剔除大量重复的boxes），比如通过锚集的设计和将目标框分配给锚的启发式方法。目前端到端的思路很流行并且也取得了巨大的成功，但还没有在detection领域内普及。然后作者就提出了DETR，通过删除多个手工设计的组件来简化检测流程，这些组件编码先验知识，如空间锚定或非最大抑制。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic1.jpg?raw=true)\n\n## 2.Related Work\n\n### 2.1 Set Prediction\n\n没有规范的深度学习模型可以直接预测集合。基本的集合预测任务是多标签分类，其中基线方法，1 -vs-rest，不适用于诸如检测元素之间存在底层结构的问题(即，几乎相同的盒子)。所以第一个困难就是避免重复，但是现在方法大都以靠后处理，对于set prediction来说应该是无需后处理的，他们需要全局推理方案来模拟所有预测元素之间的相互作用，以避免冗余。对于恒定大小的集合预测，密集的全连接网络是足够的，但成本很高。一般的方法是使用自回归序列模型，如循环神经网络，在所有情况下，损失函数通过预测的排列应该是不变的。\n\n## 3.DETR Model\n\n### 3.1 Object detection set prediction loss\n\n在通过解码器的单次传递中，DETR推断出固定大小的$N$个预测集，其中$N$被设置为明显大于图像中典型对象的数量。训练的主要困难之一是根据真实情况对预测对象(类别、位置、大小)进行评分。\n\n定义预测结果为$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$,$y$是gt数量，然后给$y$进行padding填充到和$\\hat{y}$一样的大小。损失函数如下，其中$\\mathcal{L}_{match}$表示配对成本，该公式由匈牙利算法进行优化。\n\n$$\n\\hat{\\sigma}=\\arg\\min_{\\sigma\\in\\mathfrak{S}_N}\\sum_i\\mathcal{L}_{\\text{match}} ( y _ i , \\hat { y }_{\\sigma(i)}),\n$$\n\n匹配成本既考虑了类预测，也考虑了预测真值盒与真实真值盒的相似性。\n\n$$\n-1_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(b_i,\\hat{b}_{\\sigma(i)})\n$$\n\n在获得配对后，就是要计算真正的损失\n\n$$\n\\mathcal{L}_\\text{Hungarian}{ ( y , \\hat { y })}=\\sum_{i=1}^N\\left[-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(b_i,\\hat{b}_{\\hat{\\sigma}}(i))\\right]\n$$\n\nbox损失如下：\n$$\n\\lambda_\\text{iou}{ \\mathcal{L}_\\text{iou}} ( \\overline { b _ i , \\hat { b }_{\\sigma(i)}})+\\lambda_{\\text{L}1}||b_i-\\hat{b}_{\\sigma(i)}||_1\\\\\n\\mathcal{L}_{\\mathrm{iou}}(b_{\\sigma(i)},\\hat{b}_i)=1-\\left(\\frac{|b_{\\sigma(i)}\\cap\\hat{b}_i|}{|b_{\\sigma(i)}\\cup\\hat{b}_i|}-\\frac{|B(b_{\\sigma(i)},\\hat{b}_i)\\setminus b_{\\sigma(i)}\\cup\\hat{b}_i|}{|B(b_{\\sigma(i)},\\hat{b}_i)|}\\right)\n$$\n\n### 3.2 DETR architecture\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic2.jpg?raw=true)\n\nBackbone. Starting from the initial image $x_{\\mathrm{img}}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$ (with 3 color channels$)$, a conventional CNN backbone generates a lower-resolution activation $\\operatorname*{map}f\\in\\mathbb{R}^{C\\times H\\times W}.$ Typical values we use are $C=2048$ and $H,W=\\frac{H_0}{32},\\frac{W_0}{32}.$\n\n然后一个$1\\times1$的卷积减少通道维度，从D减少到d，特征图表示为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$,然后调整到$d{\\times}HW$，然后送入到transformer中\n\n与原始变压器的不同之处在于，我们的模型在每个解码器层并行解码N个对象，而Vaswani等人[47]使用自回归模型，每次预测一个元素的输出序列。\n\n最终的预测由一个具有ReLU激活函数和隐藏维数d的3层感知器和一个线性投影层计算。坐标框是中心点，高宽四个数据组成\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic3.jpg?raw=true)\n\n## 4.代码\n\n### 4.1 数据加载\n\n以下是数据加载的代码，这里直接使用了torchvision.datasets.CocoDetection作为父类，所以使用的代码比较少。除此之外，数据增强这一块，作者独自写了一个transforms.py文件，作用是进行数据增强以及对标签进行相应的处理。\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    COCO dataset which returns image_id for evaluation.\n\n    Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n    \"\"\"\n    from pathlib import Path\n\n    import torch\n    import torch.utils.data\n    import torchvision\n    from pycocotools import mask as coco_mask\n\n    import datasets.transforms as T\n\n\n    class CocoDetection(torchvision.datasets.CocoDetection):\n        def __init__(self, img_folder, ann_file, transforms, return_masks):\n            super(CocoDetection, self).__init__(img_folder, ann_file)\n            self._transforms = transforms\n            self.prepare = ConvertCocoPolysToMask(return_masks)\n\n        def __getitem__(self, idx):\n            img, target = super(CocoDetection, self).__getitem__(idx)\n            image_id = self.ids[idx]\n            target = {'image_id': image_id, 'annotations': target}\n            img, target = self.prepare(img, target)\n            if self._transforms is not None:\n                img, target = self._transforms(img, target)\n            return img, target\n\n\n    def convert_coco_poly_to_mask(segmentations, height, width):\n        masks = []\n        for polygons in segmentations:\n            rles = coco_mask.frPyObjects(polygons, height, width)\n            mask = coco_mask.decode(rles)\n            if len(mask.shape) < 3:\n                mask = mask[..., None]\n            mask = torch.as_tensor(mask, dtype=torch.uint8)\n            mask = mask.any(dim=2)\n            masks.append(mask)\n        if masks:\n            masks = torch.stack(masks, dim=0)\n        else:\n            masks = torch.zeros((0, height, width), dtype=torch.uint8)\n        return masks\n\n\n    class ConvertCocoPolysToMask(object):\n        def __init__(self, return_masks=False):\n            self.return_masks = return_masks\n\n        def __call__(self, image, target):\n            w, h = image.size\n\n            image_id = target[\"image_id\"]\n            image_id = torch.tensor([image_id])\n\n            anno = target[\"annotations\"]\n            \n            # 只取目标没有重叠的\n            anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n\n            boxes = [obj[\"bbox\"] for obj in anno]\n            # guard against no boxes via resizing\n            boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n            boxes[:, 2:] += boxes[:, :2]\n            boxes[:, 0::2].clamp_(min=0, max=w)\n            boxes[:, 1::2].clamp_(min=0, max=h)\n\n            classes = [obj[\"category_id\"] for obj in anno]\n            classes = torch.tensor(classes, dtype=torch.int64)\n\n            if self.return_masks:\n                segmentations = [obj[\"segmentation\"] for obj in anno]\n                masks = convert_coco_poly_to_mask(segmentations, h, w)\n\n            keypoints = None\n            if anno and \"keypoints\" in anno[0]:\n                keypoints = [obj[\"keypoints\"] for obj in anno]\n                keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n                num_keypoints = keypoints.shape[0]\n                if num_keypoints:\n                    keypoints = keypoints.view(num_keypoints, -1, 3)\n\n            keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n            boxes = boxes[keep]\n            classes = classes[keep]\n            if self.return_masks:\n                masks = masks[keep]\n            if keypoints is not None:\n                keypoints = keypoints[keep]\n\n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = classes\n            if self.return_masks:\n                target[\"masks\"] = masks\n            target[\"image_id\"] = image_id\n            if keypoints is not None:\n                target[\"keypoints\"] = keypoints\n\n            # for conversion to coco api\n            area = torch.tensor([obj[\"area\"] for obj in anno])\n            iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n            target[\"area\"] = area[keep]\n            target[\"iscrowd\"] = iscrowd[keep]\n\n            target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n            target[\"size\"] = torch.as_tensor([int(h), int(w)])\n\n            return image, target\n\n\n    def make_coco_transforms(image_set):\n\n        normalize = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n        scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n\n        if image_set == 'train':\n            return T.Compose([\n                T.RandomHorizontalFlip(),\n                T.RandomSelect(\n                    T.RandomResize(scales, max_size=1333),\n                    T.Compose([\n                        T.RandomResize([400, 500, 600]),\n                        T.RandomSizeCrop(384, 600),\n                        T.RandomResize(scales, max_size=1333),\n                    ])\n                ),\n                normalize,\n            ])\n\n        if image_set == 'val':\n            return T.Compose([\n                T.RandomResize([800], max_size=1333),\n                normalize,\n            ])\n\n        raise ValueError(f'unknown {image_set}')\n\n\n    def build(image_set, args):\n        root = Path(args.coco_path)\n        assert root.exists(), f'provided COCO path {root} does not exist'\n        mode = 'instances'\n        PATHS = {\n            \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n            \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n        }\n\n        img_folder, ann_file = PATHS[image_set]\n        dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)\n        return dataset\n  \n  \n### 4.2 Position Embedding\n\n作者实现了两个Position Embedding，第一个就是Transformer所使用的sin，cos，第二个是一个可学习的Embedding。\n第一个的公式如下：\n$$\n\\begin{aligned}PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{\\mathrm{mokel}}})\\\\PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{\\mathrm{mokel}}})\\end{aligned}\n$$\n\n代码实现：\n\n    class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, tensor_list: NestedTensor):\n        x = tensor_list.tensors  # (B, C, H, W)\n        mask = tensor_list.mask  # (B, H, W) 表示当前的像素是pad过来的还是原来就有的 \n        assert mask is not None\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32) # (B, H, W)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32) # (B, H, W)\n        if self.normalize:\n            eps = 1e-6\n            # 这里使用-1：是为了保持维度，如果直接使用-1，会导致少一个维度\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale \n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats) # (self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t # 这里使用了广播机制，None的作用是增加一个维度\n        pos_y = y_embed[:, :, :, None] / dim_t # (C, H, W, self.num_pos_feats)\n        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2) # (B, 2*self.num_pos_feats, H, W)\n        return pos\n\n以下是可学习的Position Embedding，和上面的Position有些类似\n\n    class PositionEmbeddingLearned(nn.Module):\n        \"\"\"\n        Absolute pos embedding, learned.\n        \"\"\"\n        def __init__(self, num_pos_feats=256):\n            super().__init__()\n            # 特征图h,w都要小于50,要求h和w一样\n            self.row_embed = nn.Embedding(50, num_pos_feats) \n            self.col_embed = nn.Embedding(50, num_pos_feats)\n            self.reset_parameters()\n\n        def reset_parameters(self):\n            nn.init.uniform_(self.row_embed.weight)\n            nn.init.uniform_(self.col_embed.weight)\n\n        def forward(self, tensor_list: NestedTensor):\n            x = tensor_list.tensors\n            h, w = x.shape[-2:]\n            i = torch.arange(w, device=x.device)\n            j = torch.arange(h, device=x.device)\n            x_emb = self.col_embed(i) # (50, 256)\n            y_emb = self.row_embed(j)\n            pos = torch.cat([\n                x_emb.unsqueeze(0).repeat(h, 1, 1), # (h, 50, 256)\n                y_emb.unsqueeze(1).repeat(1, w, 1), # (w, 50, 256)\n            ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1) # (B, h, w, 512)\n            return pos\n\n### 4.3 Transformer\n\n这里给一张Transformer的参考图就能比较快的理解了\n\n![](https://raw.githubusercontent.com/1248818919/My_NOTE/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/transformer.webp)\n\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    DETR Transformer class.\n\n    Copy-paste from torch.nn.Transformer with modifications:\n        * positional encodings are passed in MHattention\n        * extra LN at the end of encoder is removed\n        * decoder returns a stack of activations from all decoding layers\n    \"\"\"\n    import copy\n    from typing import Optional, List\n\n    import torch\n    import torch.nn.functional as F\n    from torch import nn, Tensor\n\n\n    class Transformer(nn.Module):\n\n        def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                     num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False,\n                     return_intermediate_dec=False):\n            super().__init__()\n\n            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                                    dropout, activation, normalize_before)\n            encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n                                                    dropout, activation, normalize_before)\n            decoder_norm = nn.LayerNorm(d_model)\n            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n                                              return_intermediate=return_intermediate_dec)\n\n            self._reset_parameters()\n\n            self.d_model = d_model\n            self.nhead = nhead\n\n        def _reset_parameters(self):\n            for p in self.parameters():\n                if p.dim() > 1:\n                    nn.init.xavier_uniform_(p)\n\n        def forward(self, src, mask, query_embed, pos_embed):\n            # flatten NxCxHxW to HWxNxC\n            bs, c, h, w = src.shape\n            src = src.flatten(2).permute(2, 0, 1)\n            pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n            mask = mask.flatten(1)\n\n            tgt = torch.zeros_like(query_embed)\n            memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n            hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n                              pos=pos_embed, query_pos=query_embed)\n            return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n\n\n    class TransformerEncoder(nn.Module):\n\n        def __init__(self, encoder_layer, num_layers, norm=None):\n            super().__init__()\n            self.layers = _get_clones(encoder_layer, num_layers)\n            self.num_layers = num_layers\n            self.norm = norm\n\n        def forward(self, src,\n                    mask: Optional[Tensor] = None,\n                    src_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n            output = src\n\n            for layer in self.layers:\n                output = layer(output, src_mask=mask,\n                               src_key_padding_mask=src_key_padding_mask, pos=pos)\n\n            if self.norm is not None:\n                output = self.norm(output)\n\n            return output\n\n\n    class TransformerDecoder(nn.Module):\n\n        def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n            super().__init__()\n            self.layers = _get_clones(decoder_layer, num_layers)\n            self.num_layers = num_layers\n            self.norm = norm\n            self.return_intermediate = return_intermediate\n\n        def forward(self, tgt, memory,\n                    tgt_mask: Optional[Tensor] = None,\n                    memory_mask: Optional[Tensor] = None,\n                    tgt_key_padding_mask: Optional[Tensor] = None,\n                    memory_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None,\n                    query_pos: Optional[Tensor] = None):\n            output = tgt\n\n            intermediate = []\n\n            for layer in self.layers:\n                output = layer(output, memory, tgt_mask=tgt_mask,\n                               memory_mask=memory_mask,\n                               tgt_key_padding_mask=tgt_key_padding_mask,\n                               memory_key_padding_mask=memory_key_padding_mask,\n                               pos=pos, query_pos=query_pos)\n                if self.return_intermediate:\n                    intermediate.append(self.norm(output))\n\n            if self.norm is not None:\n                output = self.norm(output)\n                if self.return_intermediate:\n                    intermediate.pop()\n                    intermediate.append(output)\n\n            if self.return_intermediate:\n                return torch.stack(intermediate)\n\n            return output.unsqueeze(0)\n\n\n    class TransformerEncoderLayer(nn.Module):\n\n        def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False):\n            super().__init__()\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            # Implementation of Feedforward model\n            self.linear1 = nn.Linear(d_model, dim_feedforward)\n            self.dropout = nn.Dropout(dropout)\n            self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n            self.norm1 = nn.LayerNorm(d_model)\n            self.norm2 = nn.LayerNorm(d_model)\n            self.dropout1 = nn.Dropout(dropout)\n            self.dropout2 = nn.Dropout(dropout)\n\n            self.activation = _get_activation_fn(activation)\n            self.normalize_before = normalize_before\n\n        def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n            return tensor if pos is None else tensor + pos\n\n        def forward_post(self,\n                         src,\n                         src_mask: Optional[Tensor] = None,\n                         src_key_padding_mask: Optional[Tensor] = None,\n                         pos: Optional[Tensor] = None):\n            q = k = self.with_pos_embed(src, pos)\n            src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n                                  key_padding_mask=src_key_padding_mask)[0]\n            src = src + self.dropout1(src2)\n            src = self.norm1(src)\n            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n            src = src + self.dropout2(src2)\n            src = self.norm2(src)\n            return src\n\n        def forward_pre(self, src,\n                        src_mask: Optional[Tensor] = None,\n                        src_key_padding_mask: Optional[Tensor] = None,\n                        pos: Optional[Tensor] = None):\n            src2 = self.norm1(src)\n            q = k = self.with_pos_embed(src2, pos)\n            src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n                                  key_padding_mask=src_key_padding_mask)[0]\n            src = src + self.dropout1(src2)\n            src2 = self.norm2(src)\n            src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n            src = src + self.dropout2(src2)\n            return src\n\n        def forward(self, src,\n                    src_mask: Optional[Tensor] = None,\n                    src_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n            if self.normalize_before:\n                return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n            return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n\n\n    class TransformerDecoderLayer(nn.Module):\n\n        def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False):\n            super().__init__()\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            # Implementation of Feedforward model\n            self.linear1 = nn.Linear(d_model, dim_feedforward)\n            self.dropout = nn.Dropout(dropout)\n            self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n            self.norm1 = nn.LayerNorm(d_model)\n            self.norm2 = nn.LayerNorm(d_model)\n            self.norm3 = nn.LayerNorm(d_model)\n            self.dropout1 = nn.Dropout(dropout)\n            self.dropout2 = nn.Dropout(dropout)\n            self.dropout3 = nn.Dropout(dropout)\n\n            self.activation = _get_activation_fn(activation)\n            self.normalize_before = normalize_before\n\n        def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n            return tensor if pos is None else tensor + pos\n\n        def forward_post(self, tgt, memory,\n                         tgt_mask: Optional[Tensor] = None,\n                         memory_mask: Optional[Tensor] = None,\n                         tgt_key_padding_mask: Optional[Tensor] = None,\n                         memory_key_padding_mask: Optional[Tensor] = None,\n                         pos: Optional[Tensor] = None,\n                         query_pos: Optional[Tensor] = None):\n            q = k = self.with_pos_embed(tgt, query_pos)\n            tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)[0]\n            tgt = tgt + self.dropout1(tgt2)\n            tgt = self.norm1(tgt)\n            tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                                       key=self.with_pos_embed(memory, pos),\n                                       value=memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)[0]\n            tgt = tgt + self.dropout2(tgt2)\n            tgt = self.norm2(tgt)\n            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n            tgt = tgt + self.dropout3(tgt2)\n            tgt = self.norm3(tgt)\n            return tgt\n\n        def forward_pre(self, tgt, memory,\n                        tgt_mask: Optional[Tensor] = None,\n                        memory_mask: Optional[Tensor] = None,\n                        tgt_key_padding_mask: Optional[Tensor] = None,\n                        memory_key_padding_mask: Optional[Tensor] = None,\n                        pos: Optional[Tensor] = None,\n                        query_pos: Optional[Tensor] = None):\n            tgt2 = self.norm1(tgt)\n            q = k = self.with_pos_embed(tgt2, query_pos)\n            tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)[0]\n            tgt = tgt + self.dropout1(tgt2)\n            tgt2 = self.norm2(tgt)\n            tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n                                       key=self.with_pos_embed(memory, pos),\n                                       value=memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)[0]\n            tgt = tgt + self.dropout2(tgt2)\n            tgt2 = self.norm3(tgt)\n            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n            tgt = tgt + self.dropout3(tgt2)\n            return tgt\n\n        def forward(self, tgt, memory,\n                    tgt_mask: Optional[Tensor] = None,\n                    memory_mask: Optional[Tensor] = None,\n                    tgt_key_padding_mask: Optional[Tensor] = None,\n                    memory_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None,\n                    query_pos: Optional[Tensor] = None):\n            if self.normalize_before:\n                return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n                                        tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n            return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n                                     tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n                            \n                            \n### 4.4 Detr\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    DETR model and criterion classes.\n    \"\"\"\n    import torch\n    import torch.nn.functional as F\n    from torch import nn\n\n    from util import box_ops\n    from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n                           accuracy, get_world_size, interpolate,\n                           is_dist_avail_and_initialized)\n\n    from .backbone import build_backbone\n    from .matcher import build_matcher\n    from .segmentation import (DETRsegm, PostProcessPanoptic, PostProcessSegm,\n                               dice_loss, sigmoid_focal_loss)\n    from .transformer import build_transformer\n\n\n    class DETR(nn.Module):\n        \"\"\" This is the DETR module that performs object detection \"\"\"\n        def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n            \"\"\" Initializes the model.\n            Parameters:\n                backbone: torch module of the backbone to be used. See backbone.py\n                transformer: torch module of the transformer architecture. See transformer.py\n                num_classes: number of object classes\n                num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n                             DETR can detect in a single image. For COCO, we recommend 100 queries.\n                aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n            \"\"\"\n            super().__init__()\n            self.num_queries = num_queries\n            self.transformer = transformer\n            hidden_dim = transformer.d_model\n            self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n            self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n            self.query_embed = nn.Embedding(num_queries, hidden_dim)\n            self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n            self.backbone = backbone\n            self.aux_loss = aux_loss\n\n        def forward(self, samples: NestedTensor):\n            \"\"\" The forward expects a NestedTensor, which consists of:\n                   - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n                   - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n\n                It returns a dict with the following elements:\n                   - \"pred_logits\": the classification logits (including no-object) for all queries.\n                                    Shape= [batch_size x num_queries x (num_classes + 1)]\n                   - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n                                   (center_x, center_y, height, width). These values are normalized in [0, 1],\n                                   relative to the size of each individual image (disregarding possible padding).\n                                   See PostProcess for information on how to retrieve the unnormalized bounding box.\n                   - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n                                    dictionnaries containing the two above keys for each decoder layer.\n            \"\"\"\n            if isinstance(samples, (list, torch.Tensor)):\n                samples = nested_tensor_from_tensor_list(samples)\n            features, pos = self.backbone(samples)\n\n            src, mask = features[-1].decompose()\n            assert mask is not None\n            hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n\n            outputs_class = self.class_embed(hs)\n            outputs_coord = self.bbox_embed(hs).sigmoid()\n            out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n            if self.aux_loss:\n                out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n            return out\n\n        @torch.jit.unused\n        def _set_aux_loss(self, outputs_class, outputs_coord):\n            # this is a workaround to make torchscript happy, as torchscript\n            # doesn't support dictionary with non-homogeneous values, such\n            # as a dict having both a Tensor and a list.\n            return [{'pred_logits': a, 'pred_boxes': b}\n                    for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n\n\n### 4.5 计算损失\n\n\n    class SetCriterion(nn.Module):\n        \"\"\" This class computes the loss for DETR.\n        The process happens in two steps:\n            1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n            2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n        \"\"\"\n        def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n            \"\"\" Create the criterion.\n            Parameters:\n                num_classes: number of object categories, omitting the special no-object category\n                matcher: module able to compute a matching between targets and proposals\n                weight_dict: dict containing as key the names of the losses and as values their relative weight.\n                eos_coef: relative classification weight applied to the no-object category\n                losses: list of all the losses to be applied. See get_loss for list of available losses.\n            \"\"\"\n            super().__init__()\n            self.num_classes = num_classes\n            self.matcher = matcher\n            self.weight_dict = weight_dict\n            self.eos_coef = eos_coef\n            self.losses = losses\n            empty_weight = torch.ones(self.num_classes + 1)\n            empty_weight[-1] = self.eos_coef\n            self.register_buffer('empty_weight', empty_weight)\n\n        def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n            \"\"\"Classification loss (NLL)\n            targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n            \"\"\"\n            assert 'pred_logits' in outputs\n            src_logits = outputs['pred_logits']\n\n            idx = self._get_src_permutation_idx(indices)\n            target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n            target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n                                        dtype=torch.int64, device=src_logits.device)\n            target_classes[idx] = target_classes_o\n\n            loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n            losses = {'loss_ce': loss_ce}\n\n            if log:\n                # TODO this should probably be a separate loss, not hacked in this one here\n                losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n            return losses\n\n        @torch.no_grad()\n        def loss_cardinality(self, outputs, targets, indices, num_boxes):\n            \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n            This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n            \"\"\"\n            pred_logits = outputs['pred_logits']\n            device = pred_logits.device\n            tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n            # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n            card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n            card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n            losses = {'cardinality_error': card_err}\n            return losses\n\n        def loss_boxes(self, outputs, targets, indices, num_boxes):\n            \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n               targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n               The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n            \"\"\"\n            assert 'pred_boxes' in outputs\n            idx = self._get_src_permutation_idx(indices)\n            src_boxes = outputs['pred_boxes'][idx]\n            target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n\n            loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n\n            losses = {}\n            losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n\n            loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(\n                box_ops.box_cxcywh_to_xyxy(src_boxes),\n                box_ops.box_cxcywh_to_xyxy(target_boxes)))\n            losses['loss_giou'] = loss_giou.sum() / num_boxes\n            return losses\n\n        def loss_masks(self, outputs, targets, indices, num_boxes):\n            \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n               targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n            \"\"\"\n            assert \"pred_masks\" in outputs\n\n            src_idx = self._get_src_permutation_idx(indices)\n            tgt_idx = self._get_tgt_permutation_idx(indices)\n            src_masks = outputs[\"pred_masks\"]\n            src_masks = src_masks[src_idx]\n            masks = [t[\"masks\"] for t in targets]\n            # TODO use valid to mask invalid areas due to padding in loss\n            target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n            target_masks = target_masks.to(src_masks)\n            target_masks = target_masks[tgt_idx]\n\n            # upsample predictions to the target size\n            src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n                                    mode=\"bilinear\", align_corners=False)\n            src_masks = src_masks[:, 0].flatten(1)\n\n            target_masks = target_masks.flatten(1)\n            target_masks = target_masks.view(src_masks.shape)\n            losses = {\n                \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n                \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n            }\n            return losses\n\n        def _get_src_permutation_idx(self, indices):\n            # permute predictions following indices\n            batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n            src_idx = torch.cat([src for (src, _) in indices])\n            return batch_idx, src_idx\n\n        def _get_tgt_permutation_idx(self, indices):\n            # permute targets following indices\n            batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n            tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n            return batch_idx, tgt_idx\n\n        def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n            loss_map = {\n                'labels': self.loss_labels,\n                'cardinality': self.loss_cardinality,\n                'boxes': self.loss_boxes,\n                'masks': self.loss_masks\n            }\n            assert loss in loss_map, f'do you really want to compute {loss} loss?'\n            return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n\n        def forward(self, outputs, targets):\n            \"\"\" This performs the loss computation.\n            Parameters:\n                 outputs: dict of tensors, see the output specification of the model for the format\n                 targets: list of dicts, such that len(targets) == batch_size.\n                          The expected keys in each dict depends on the losses applied, see each loss' doc\n            \"\"\"\n            outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n\n            # Retrieve the matching between the outputs of the last layer and the targets\n            indices = self.matcher(outputs_without_aux, targets)\n\n            # Compute the average number of target boxes accross all nodes, for normalization purposes\n            num_boxes = sum(len(t[\"labels\"]) for t in targets)\n            num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n            if is_dist_avail_and_initialized():\n                torch.distributed.all_reduce(num_boxes)\n            num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n\n            # Compute all the requested losses\n            losses = {}\n            for loss in self.losses:\n                losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n\n            # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n            if 'aux_outputs' in outputs:\n                for i, aux_outputs in enumerate(outputs['aux_outputs']):\n                    indices = self.matcher(aux_outputs, targets)\n                    for loss in self.losses:\n                        if loss == 'masks':\n                            # Intermediate masks losses are too costly to compute, we ignore them.\n                            continue\n                        kwargs = {}\n                        if loss == 'labels':\n                            # Logging is enabled only for the last layer\n                            kwargs = {'log': False}\n                        l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n                        l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n                        losses.update(l_dict)\n\n            return losses\n            \n### 4.6 Matcher\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    Modules to compute the matching cost and solve the corresponding LSAP.\n    \"\"\"\n    import torch\n    from scipy.optimize import linear_sum_assignment\n    from torch import nn\n\n    from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n\n\n    class HungarianMatcher(nn.Module):\n        \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n        For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n        there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n        while the others are un-matched (and thus treated as non-objects).\n        \"\"\"\n\n        def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n            \"\"\"Creates the matcher\n\n            Params:\n                cost_class: This is the relative weight of the classification error in the matching cost\n                cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n                cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n            \"\"\"\n            super().__init__()\n            self.cost_class = cost_class\n            self.cost_bbox = cost_bbox\n            self.cost_giou = cost_giou\n            assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n\n        @torch.no_grad()\n        def forward(self, outputs, targets):\n            \"\"\" Performs the matching\n\n            Params:\n                outputs: This is a dict that contains at least these entries:\n                     \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                     \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n\n                targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n                     \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n                               objects in the target) containing the class labels\n                     \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n\n            Returns:\n                A list of size batch_size, containing tuples of (index_i, index_j) where:\n                    - index_i is the indices of the selected predictions (in order)\n                    - index_j is the indices of the corresponding selected targets (in order)\n                For each batch element, it holds:\n                    len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n            \"\"\"\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n\n            # We flatten to compute the cost matrices in a batch\n            out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n            out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n\n            # Also concat the target labels and boxes\n            tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n            tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n\n            # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n            # but approximate it in 1 - proba[target class].\n            # The 1 is a constant that doesn't change the matching, it can be ommitted.\n            cost_class = -out_prob[:, tgt_ids]\n\n            # Compute the L1 cost between boxes\n            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n\n            # Compute the giou cost betwen boxes\n            cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n\n            # Final cost matrix\n            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n            C = C.view(bs, num_queries, -1).cpu()\n\n            sizes = [len(v[\"boxes\"]) for v in targets]\n            indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n            return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n\n\n    def build_matcher(args):\n        return HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-01-15T11:18:49.274Z","updatedAt":"2024-03-23T03:50:05.765Z","trashed":false,"_rev":"bdaC8X7i3"}