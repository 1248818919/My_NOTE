{"_id":"note:DeyhmvTGNw","title":"DETR","content":"# DETR 论文阅读笔记\n\n>Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 213-229.\n\n## 1.Introduction\n\n目前detection任务都是通过在大量的提议，锚框和窗口中心定义代理回归和分类任务，这就导致了他们的性能很容易受到后处理步骤的影响（后处理用来剔除大量重复的boxes），比如通过锚集的设计和将目标框分配给锚的启发式方法。目前端到端的思路很流行并且也取得了巨大的成功，但还没有在detection领域内普及。然后作者就提出了DETR，通过删除多个手工设计的组件来简化检测流程，这些组件编码先验知识，如空间锚定或非最大抑制。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic1.jpg?raw=true)\n\n## 2.Related Work\n\n### 2.1 Set Prediction\n\n没有规范的深度学习模型可以直接预测集合。基本的集合预测任务是多标签分类，其中基线方法，1 -vs-rest，不适用于诸如检测元素之间存在底层结构的问题(即，几乎相同的盒子)。所以第一个困难就是避免重复，但是现在方法大都以靠后处理，对于set prediction来说应该是无需后处理的，他们需要全局推理方案来模拟所有预测元素之间的相互作用，以避免冗余。对于恒定大小的集合预测，密集的全连接网络是足够的，但成本很高。一般的方法是使用自回归序列模型，如循环神经网络，在所有情况下，损失函数通过预测的排列应该是不变的。\n\n## 3.DETR Model\n\n### 3.1 Object detection set prediction loss\n\n在通过解码器的单次传递中，DETR推断出固定大小的$N$个预测集，其中$N$被设置为明显大于图像中典型对象的数量。训练的主要困难之一是根据真实情况对预测对象(类别、位置、大小)进行评分。\n\n定义预测结果为$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$,$y$是gt数量，然后给$y$进行padding填充到和$\\hat{y}$一样的大小。损失函数如下，其中$\\mathcal{L}_{match}$表示配对成本，该公式由匈牙利算法进行优化。\n\n$$\n\\hat{\\sigma}=\\arg\\min_{\\sigma\\in\\mathfrak{S}_N}\\sum_i\\mathcal{L}_{\\text{match}} ( y _ i , \\hat { y }_{\\sigma(i)}),\n$$\n\n匹配成本既考虑了类预测，也考虑了预测真值盒与真实真值盒的相似性。\n\n$$\n-1_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(b_i,\\hat{b}_{\\sigma(i)})\n$$\n\n在获得配对后，就是要计算真正的损失\n\n$$\n\\mathcal{L}_\\text{Hungarian}{ ( y , \\hat { y })}=\\sum_{i=1}^N\\left[-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\mathrm{box}}(b_i,\\hat{b}_{\\hat{\\sigma}}(i))\\right]\n$$\n\nbox损失如下：\n$$\n\\lambda_\\text{iou}{ \\mathcal{L}_\\text{iou}} ( \\overline { b _ i , \\hat { b }_{\\sigma(i)}})+\\lambda_{\\text{L}1}||b_i-\\hat{b}_{\\sigma(i)}||_1\\\\\n\\mathcal{L}_{\\mathrm{iou}}(b_{\\sigma(i)},\\hat{b}_i)=1-\\left(\\frac{|b_{\\sigma(i)}\\cap\\hat{b}_i|}{|b_{\\sigma(i)}\\cup\\hat{b}_i|}-\\frac{|B(b_{\\sigma(i)},\\hat{b}_i)\\setminus b_{\\sigma(i)}\\cup\\hat{b}_i|}{|B(b_{\\sigma(i)},\\hat{b}_i)|}\\right)\n$$\n\n### 3.2 DETR architecture\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic2.jpg?raw=true)\n\nBackbone. Starting from the initial image $x_{\\mathrm{img}}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$ (with 3 color channels$)$, a conventional CNN backbone generates a lower-resolution activation $\\operatorname*{map}f\\in\\mathbb{R}^{C\\times H\\times W}.$ Typical values we use are $C=2048$ and $H,W=\\frac{H_0}{32},\\frac{W_0}{32}.$\n\n然后一个$1\\times1$的卷积减少通道维度，从D减少到d，特征图表示为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$,然后调整到$d{\\times}HW$，然后送入到transformer中\n\n与原始变压器的不同之处在于，我们的模型在每个解码器层并行解码N个对象，而Vaswani等人[47]使用自回归模型，每次预测一个元素的输出序列。\n\n最终的预测由一个具有ReLU激活函数和隐藏维数d的3层感知器和一个线性投影层计算。坐标框是中心点，高宽四个数据组成\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/DETR/pic3.jpg?raw=true)\n\n## 4.代码\n\n### 4.1 数据加载\n\n以下是数据加载的代码，这里直接使用了torchvision.datasets.CocoDetection作为父类，所以使用的代码比较少。除此之外，数据增强这一块，作者独自写了一个transforms.py文件，作用是进行数据增强以及对标签进行相应的处理。\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    COCO dataset which returns image_id for evaluation.\n\n    Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n    \"\"\"\n    from pathlib import Path\n\n    import torch\n    import torch.utils.data\n    import torchvision\n    from pycocotools import mask as coco_mask\n\n    import datasets.transforms as T\n\n\n    class CocoDetection(torchvision.datasets.CocoDetection):\n        def __init__(self, img_folder, ann_file, transforms, return_masks):\n            super(CocoDetection, self).__init__(img_folder, ann_file)\n            self._transforms = transforms\n            self.prepare = ConvertCocoPolysToMask(return_masks)\n\n        def __getitem__(self, idx):\n            img, target = super(CocoDetection, self).__getitem__(idx)\n            image_id = self.ids[idx]\n            target = {'image_id': image_id, 'annotations': target}\n            img, target = self.prepare(img, target)\n            if self._transforms is not None:\n                img, target = self._transforms(img, target)\n            return img, target\n\n\n    def convert_coco_poly_to_mask(segmentations, height, width):\n        masks = []\n        for polygons in segmentations:\n            rles = coco_mask.frPyObjects(polygons, height, width)\n            mask = coco_mask.decode(rles)\n            if len(mask.shape) < 3:\n                mask = mask[..., None]\n            mask = torch.as_tensor(mask, dtype=torch.uint8)\n            mask = mask.any(dim=2)\n            masks.append(mask)\n        if masks:\n            masks = torch.stack(masks, dim=0)\n        else:\n            masks = torch.zeros((0, height, width), dtype=torch.uint8)\n        return masks\n\n\n    class ConvertCocoPolysToMask(object):\n        def __init__(self, return_masks=False):\n            self.return_masks = return_masks\n\n        def __call__(self, image, target):\n            w, h = image.size\n\n            image_id = target[\"image_id\"]\n            image_id = torch.tensor([image_id])\n\n            anno = target[\"annotations\"]\n            \n            # 只取目标没有重叠的\n            anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n\n            boxes = [obj[\"bbox\"] for obj in anno]\n            # guard against no boxes via resizing\n            boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n            boxes[:, 2:] += boxes[:, :2]\n            boxes[:, 0::2].clamp_(min=0, max=w)\n            boxes[:, 1::2].clamp_(min=0, max=h)\n\n            classes = [obj[\"category_id\"] for obj in anno]\n            classes = torch.tensor(classes, dtype=torch.int64)\n\n            if self.return_masks:\n                segmentations = [obj[\"segmentation\"] for obj in anno]\n                masks = convert_coco_poly_to_mask(segmentations, h, w)\n\n            keypoints = None\n            if anno and \"keypoints\" in anno[0]:\n                keypoints = [obj[\"keypoints\"] for obj in anno]\n                keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n                num_keypoints = keypoints.shape[0]\n                if num_keypoints:\n                    keypoints = keypoints.view(num_keypoints, -1, 3)\n\n            keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n            boxes = boxes[keep]\n            classes = classes[keep]\n            if self.return_masks:\n                masks = masks[keep]\n            if keypoints is not None:\n                keypoints = keypoints[keep]\n\n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = classes\n            if self.return_masks:\n                target[\"masks\"] = masks\n            target[\"image_id\"] = image_id\n            if keypoints is not None:\n                target[\"keypoints\"] = keypoints\n\n            # for conversion to coco api\n            area = torch.tensor([obj[\"area\"] for obj in anno])\n            iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n            target[\"area\"] = area[keep]\n            target[\"iscrowd\"] = iscrowd[keep]\n\n            target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n            target[\"size\"] = torch.as_tensor([int(h), int(w)])\n\n            return image, target\n\n\n    def make_coco_transforms(image_set):\n\n        normalize = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n        scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n\n        if image_set == 'train':\n            return T.Compose([\n                T.RandomHorizontalFlip(),\n                T.RandomSelect(\n                    T.RandomResize(scales, max_size=1333),\n                    T.Compose([\n                        T.RandomResize([400, 500, 600]),\n                        T.RandomSizeCrop(384, 600),\n                        T.RandomResize(scales, max_size=1333),\n                    ])\n                ),\n                normalize,\n            ])\n\n        if image_set == 'val':\n            return T.Compose([\n                T.RandomResize([800], max_size=1333),\n                normalize,\n            ])\n\n        raise ValueError(f'unknown {image_set}')\n\n\n    def build(image_set, args):\n        root = Path(args.coco_path)\n        assert root.exists(), f'provided COCO path {root} does not exist'\n        mode = 'instances'\n        PATHS = {\n            \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n            \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n        }\n\n        img_folder, ann_file = PATHS[image_set]\n        dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)\n        return dataset\n  \n  \n### 4.2 Position Embedding\n\n作者实现了两个Position Embedding，第一个就是Transformer所使用的sin，cos，第二个是一个可学习的Embedding。\n第一个的公式如下：\n$$\n\\begin{aligned}PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{\\mathrm{mokel}}})\\\\PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{\\mathrm{mokel}}})\\end{aligned}\n$$\n\n代码实现：\n\n    class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, tensor_list: NestedTensor):\n        x = tensor_list.tensors  # (B, C, H, W)\n        mask = tensor_list.mask  # (B, H, W) 表示当前的像素是pad过来的还是原来就有的 \n        assert mask is not None\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32) # (B, H, W)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32) # (B, H, W)\n        if self.normalize:\n            eps = 1e-6\n            # 这里使用-1：是为了保持维度，如果直接使用-1，会导致少一个维度\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale \n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats) # (self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t # 这里使用了广播机制，None的作用是增加一个维度\n        pos_y = y_embed[:, :, :, None] / dim_t # (C, H, W, self.num_pos_feats)\n        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2) # (B, 2*self.num_pos_feats, H, W)\n        return pos\n\n以下是可学习的Position Embedding，和上面的Position有些类似\n\n    class PositionEmbeddingLearned(nn.Module):\n        \"\"\"\n        Absolute pos embedding, learned.\n        \"\"\"\n        def __init__(self, num_pos_feats=256):\n            super().__init__()\n            # 特征图h,w都要小于50,要求h和w一样\n            self.row_embed = nn.Embedding(50, num_pos_feats) \n            self.col_embed = nn.Embedding(50, num_pos_feats)\n            self.reset_parameters()\n\n        def reset_parameters(self):\n            nn.init.uniform_(self.row_embed.weight)\n            nn.init.uniform_(self.col_embed.weight)\n\n        def forward(self, tensor_list: NestedTensor):\n            x = tensor_list.tensors\n            h, w = x.shape[-2:]\n            i = torch.arange(w, device=x.device)\n            j = torch.arange(h, device=x.device)\n            x_emb = self.col_embed(i) # (50, 256)\n            y_emb = self.row_embed(j)\n            pos = torch.cat([\n                x_emb.unsqueeze(0).repeat(h, 1, 1), # (h, 50, 256)\n                y_emb.unsqueeze(1).repeat(1, w, 1), # (w, 50, 256)\n            ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1) # (B, h, w, 512)\n            return pos\n\n### 4.3 Transformer\n\n这里给一张Transformer的参考图就能比较快的理解了\n\n\n\n\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n    \"\"\"\n    DETR Transformer class.\n\n    Copy-paste from torch.nn.Transformer with modifications:\n        * positional encodings are passed in MHattention\n        * extra LN at the end of encoder is removed\n        * decoder returns a stack of activations from all decoding layers\n    \"\"\"\n    import copy\n    from typing import Optional, List\n\n    import torch\n    import torch.nn.functional as F\n    from torch import nn, Tensor\n\n\n    class Transformer(nn.Module):\n\n        def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                     num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False,\n                     return_intermediate_dec=False):\n            super().__init__()\n\n            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                                    dropout, activation, normalize_before)\n            encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n                                                    dropout, activation, normalize_before)\n            decoder_norm = nn.LayerNorm(d_model)\n            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n                                              return_intermediate=return_intermediate_dec)\n\n            self._reset_parameters()\n\n            self.d_model = d_model\n            self.nhead = nhead\n\n        def _reset_parameters(self):\n            for p in self.parameters():\n                if p.dim() > 1:\n                    nn.init.xavier_uniform_(p)\n\n        def forward(self, src, mask, query_embed, pos_embed):\n            # flatten NxCxHxW to HWxNxC\n            bs, c, h, w = src.shape\n            src = src.flatten(2).permute(2, 0, 1)\n            pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n            mask = mask.flatten(1)\n\n            tgt = torch.zeros_like(query_embed)\n            memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n            hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n                              pos=pos_embed, query_pos=query_embed)\n            return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n\n\n    class TransformerEncoder(nn.Module):\n\n        def __init__(self, encoder_layer, num_layers, norm=None):\n            super().__init__()\n            self.layers = _get_clones(encoder_layer, num_layers)\n            self.num_layers = num_layers\n            self.norm = norm\n\n        def forward(self, src,\n                    mask: Optional[Tensor] = None,\n                    src_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n            output = src\n\n            for layer in self.layers:\n                output = layer(output, src_mask=mask,\n                               src_key_padding_mask=src_key_padding_mask, pos=pos)\n\n            if self.norm is not None:\n                output = self.norm(output)\n\n            return output\n\n\n    class TransformerDecoder(nn.Module):\n\n        def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n            super().__init__()\n            self.layers = _get_clones(decoder_layer, num_layers)\n            self.num_layers = num_layers\n            self.norm = norm\n            self.return_intermediate = return_intermediate\n\n        def forward(self, tgt, memory,\n                    tgt_mask: Optional[Tensor] = None,\n                    memory_mask: Optional[Tensor] = None,\n                    tgt_key_padding_mask: Optional[Tensor] = None,\n                    memory_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None,\n                    query_pos: Optional[Tensor] = None):\n            output = tgt\n\n            intermediate = []\n\n            for layer in self.layers:\n                output = layer(output, memory, tgt_mask=tgt_mask,\n                               memory_mask=memory_mask,\n                               tgt_key_padding_mask=tgt_key_padding_mask,\n                               memory_key_padding_mask=memory_key_padding_mask,\n                               pos=pos, query_pos=query_pos)\n                if self.return_intermediate:\n                    intermediate.append(self.norm(output))\n\n            if self.norm is not None:\n                output = self.norm(output)\n                if self.return_intermediate:\n                    intermediate.pop()\n                    intermediate.append(output)\n\n            if self.return_intermediate:\n                return torch.stack(intermediate)\n\n            return output.unsqueeze(0)\n\n\n    class TransformerEncoderLayer(nn.Module):\n\n        def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False):\n            super().__init__()\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            # Implementation of Feedforward model\n            self.linear1 = nn.Linear(d_model, dim_feedforward)\n            self.dropout = nn.Dropout(dropout)\n            self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n            self.norm1 = nn.LayerNorm(d_model)\n            self.norm2 = nn.LayerNorm(d_model)\n            self.dropout1 = nn.Dropout(dropout)\n            self.dropout2 = nn.Dropout(dropout)\n\n            self.activation = _get_activation_fn(activation)\n            self.normalize_before = normalize_before\n\n        def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n            return tensor if pos is None else tensor + pos\n\n        def forward_post(self,\n                         src,\n                         src_mask: Optional[Tensor] = None,\n                         src_key_padding_mask: Optional[Tensor] = None,\n                         pos: Optional[Tensor] = None):\n            q = k = self.with_pos_embed(src, pos)\n            src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n                                  key_padding_mask=src_key_padding_mask)[0]\n            src = src + self.dropout1(src2)\n            src = self.norm1(src)\n            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n            src = src + self.dropout2(src2)\n            src = self.norm2(src)\n            return src\n\n        def forward_pre(self, src,\n                        src_mask: Optional[Tensor] = None,\n                        src_key_padding_mask: Optional[Tensor] = None,\n                        pos: Optional[Tensor] = None):\n            src2 = self.norm1(src)\n            q = k = self.with_pos_embed(src2, pos)\n            src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n                                  key_padding_mask=src_key_padding_mask)[0]\n            src = src + self.dropout1(src2)\n            src2 = self.norm2(src)\n            src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n            src = src + self.dropout2(src2)\n            return src\n\n        def forward(self, src,\n                    src_mask: Optional[Tensor] = None,\n                    src_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n            if self.normalize_before:\n                return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n            return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n\n\n    class TransformerDecoderLayer(nn.Module):\n\n        def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                     activation=\"relu\", normalize_before=False):\n            super().__init__()\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n            # Implementation of Feedforward model\n            self.linear1 = nn.Linear(d_model, dim_feedforward)\n            self.dropout = nn.Dropout(dropout)\n            self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n            self.norm1 = nn.LayerNorm(d_model)\n            self.norm2 = nn.LayerNorm(d_model)\n            self.norm3 = nn.LayerNorm(d_model)\n            self.dropout1 = nn.Dropout(dropout)\n            self.dropout2 = nn.Dropout(dropout)\n            self.dropout3 = nn.Dropout(dropout)\n\n            self.activation = _get_activation_fn(activation)\n            self.normalize_before = normalize_before\n\n        def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n            return tensor if pos is None else tensor + pos\n\n        def forward_post(self, tgt, memory,\n                         tgt_mask: Optional[Tensor] = None,\n                         memory_mask: Optional[Tensor] = None,\n                         tgt_key_padding_mask: Optional[Tensor] = None,\n                         memory_key_padding_mask: Optional[Tensor] = None,\n                         pos: Optional[Tensor] = None,\n                         query_pos: Optional[Tensor] = None):\n            q = k = self.with_pos_embed(tgt, query_pos)\n            tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)[0]\n            tgt = tgt + self.dropout1(tgt2)\n            tgt = self.norm1(tgt)\n            tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                                       key=self.with_pos_embed(memory, pos),\n                                       value=memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)[0]\n            tgt = tgt + self.dropout2(tgt2)\n            tgt = self.norm2(tgt)\n            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n            tgt = tgt + self.dropout3(tgt2)\n            tgt = self.norm3(tgt)\n            return tgt\n\n        def forward_pre(self, tgt, memory,\n                        tgt_mask: Optional[Tensor] = None,\n                        memory_mask: Optional[Tensor] = None,\n                        tgt_key_padding_mask: Optional[Tensor] = None,\n                        memory_key_padding_mask: Optional[Tensor] = None,\n                        pos: Optional[Tensor] = None,\n                        query_pos: Optional[Tensor] = None):\n            tgt2 = self.norm1(tgt)\n            q = k = self.with_pos_embed(tgt2, query_pos)\n            tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)[0]\n            tgt = tgt + self.dropout1(tgt2)\n            tgt2 = self.norm2(tgt)\n            tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n                                       key=self.with_pos_embed(memory, pos),\n                                       value=memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)[0]\n            tgt = tgt + self.dropout2(tgt2)\n            tgt2 = self.norm3(tgt)\n            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n            tgt = tgt + self.dropout3(tgt2)\n            return tgt\n\n        def forward(self, tgt, memory,\n                    tgt_mask: Optional[Tensor] = None,\n                    memory_mask: Optional[Tensor] = None,\n                    tgt_key_padding_mask: Optional[Tensor] = None,\n                    memory_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None,\n                    query_pos: Optional[Tensor] = None):\n            if self.normalize_before:\n                return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n                                        tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n            return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n                                     tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-01-15T11:18:49.274Z","updatedAt":"2024-03-23T03:34:13.904Z","trashed":false,"_rev":"RVqbNGAIP"}