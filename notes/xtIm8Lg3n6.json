{"_id":"note:xtIm8Lg3n6","title":"GPTv1","content":"# GPTv1论文笔记\n\n>Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.\n\n## 1.Overview\n\n尽管大量未标记的文本语料库丰富，但用于学习这些特定任务的标记数据很少，这使得判别训练模型难以充分执行。我们证明，通过在不同的未标记文本语料库上对语言模型进行生成式预训练，然后对每个特定任务进行判别性微调，可以实现这些任务的巨大收益。与以前的方法相反，我们在微调期间利用任务感知输入转换来实现有效的传输，同时需要对模型体系结构进行最小的更改。我们在自然语言理解的广泛基准上证明了我们的方法的有效性。\n\n简单说一下NLP的困境，NLP目前来说存在未标注的语料集资料很多，但是标注的语料集较少的问题，而为未标记的语料集进行标注费时费力，所以作者想利用未标注的语料集进行预训练，再进行微调。然而，预训练未标注的语料集存在如下的问题，第一个是什么样的优化目标再对于文本级别的理解上最有效并且可迁移；第二个难点是还没有有效的方法进行预训练模型的迁移，具体的现在方法多是改变网络模型，使用复杂的训练方式，添加辅助学习目标。这些不确定性使得开发有效的半监督学习语言处理方法变得困难。\n\n## 2.Framework\n\n作者的模型有两个阶段，第一个阶段在大量文本语料库上学习高容量语言模型。第二个阶段是将模型调整为具有标记数据的判别任务。\n\n### 3.1 无监督预训练\n\n给定一个无监督的语料库tokens，记为$\\mathcal{U}=\\left\\{u_1, \\ldots, u_n\\right\\}$，作者使用标准语言建模目标来优化下面的极大似然函数，其中，k是文本窗口的大小，P指的是网络模型\n\n$$\nL_1(\\mathcal{U})=\\sum_i \\log P\\left(u_i \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)\n$$\n\n作者使用了多层Transformer decoder作为语言模型","tags":[],"folderPathname":"/NLP/GPT","data":{},"createdAt":"2024-01-05T02:01:17.673Z","updatedAt":"2024-01-05T03:08:01.881Z","trashed":false,"_rev":"J-o8AXwMf"}