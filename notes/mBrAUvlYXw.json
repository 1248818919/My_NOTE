{"_id":"note:mBrAUvlYXw","title":"RegionClip","content":"# RegionClip 阅读笔记\n\n> Zhong Y, Yang J, Zhang P, et al. Regionclip: Region-based language-image pretraining[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16793-16803.\n\n## 1.Abstract\n\n使用图像-文本对的对比语言-图像预训练(CLIP)在零射击和迁移学习设置下的图像分类上都取得了令人印象深刻的结果。然而，我们表明，直接应用这样的模型来识别图像区域进行对象检测会导致由于域移位而导致性能不佳:CLIP被训练为将图像作为一个整体与文本描述相匹配，而没有捕获图像区域和文本范围之间的细粒度对齐。为了缓解这一问题，我们提出了一种名为RegionCLIP的新方法，该方法大大扩展了CLIP来学习区域级视觉表示，从而实现图像区域和文本概念之间的细粒度对齐。我们的方法利用CLIP模型将图像区域与模板标题匹配，然后预训练我们的模型以在特征空间中对齐这些区域-文本对。当将我们的预训练模型转移到开放词汇表对象检测任务时，我们的方法在COCO和LVIS数据集上对新类别的性能分别优于现有的3.8 AP50和2.2 AP。此外，学习到的区域表示支持目标检测的零射击推理，在COCO和LVIS数据集上都显示出有希望的结果。\n\n## 2.Method\n\n\n## 3.Code \n\n    class CLIPFastRCNN(nn.Module):\n        \"\"\"\n        Fast R-CNN style where the cropping is conducted on feature maps instead of raw images.\n        It contains the following two components: \n        1. Localization branch: pretrained backbone+RPN or equivalent modules, and is able to output object proposals\n        2. Recognition branch: is able to recognize zero-shot regions\n        \"\"\"\n        @configurable\n        def __init__(\n            self,\n            *,\n            offline_backbone: Backbone,\n            backbone: Backbone,\n            offline_proposal_generator: nn.Module,\n            language_encoder: nn.Module, \n            roi_heads: nn.Module,\n            pixel_mean: Tuple[float],\n            pixel_std: Tuple[float],\n            input_format: Optional[str] = None,\n            vis_period: int = 0,\n            clip_crop_region_type: str = 'GT',\n            use_clip_c4: False,\n            use_clip_attpool: False,\n            offline_input_format: Optional[str] = None,\n            offline_pixel_mean: Tuple[float],\n            offline_pixel_std: Tuple[float],\n        ):\n            \"\"\"\n            Args:\n                backbone: a backbone module, must follow detectron2's backbone interface\n                proposal_generator: a module that generates proposals using backbone features\n                roi_heads: a ROI head that performs per-region computation\n                pixel_mean, pixel_std: list or tuple with #channels element, representing\n                    the per-channel mean and std to be used to normalize the input image\n                input_format: describe the meaning of channels of input. Needed by visualization\n                vis_period: the period to run visualization. Set to 0 to disable.\n            \"\"\"\n            super().__init__()\n            self.offline_backbone = offline_backbone\n            self.backbone = backbone\n            self.lang_encoder = language_encoder\n            self.offline_proposal_generator = offline_proposal_generator\n            self.roi_heads = roi_heads\n\n            self.input_format = input_format\n            self.vis_period = vis_period\n            if vis_period > 0:\n                assert input_format is not None, \"input_format is required for visualization!\"\n\n            # input format, pixel mean and std for offline modules\n            self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n            self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n            assert (\n                self.pixel_mean.shape == self.pixel_std.shape\n            ), f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"\n            if np.sum(pixel_mean) < 3.0: # converrt pixel value to range [0.0, 1.0] by dividing 255.0\n                assert input_format == 'RGB'\n                self.div_pixel = True\n            else:\n                self.div_pixel = False\n\n            if offline_input_format and offline_pixel_mean and offline_pixel_std:\n                self.offline_input_format = offline_input_format\n                self.register_buffer(\"offline_pixel_mean\", torch.tensor(offline_pixel_mean).view(-1, 1, 1), False)\n                self.register_buffer(\"offline_pixel_std\", torch.tensor(offline_pixel_std).view(-1, 1, 1), False)\n                if np.sum(offline_pixel_mean) < 3.0: # converrt pixel value to range [0.0, 1.0] by dividing 255.0\n                    assert offline_input_format == 'RGB'\n                    self.offline_div_pixel = True\n                else:\n                    self.offline_div_pixel = False\n\n            self.clip_crop_region_type = clip_crop_region_type\n            self.use_clip_c4 = use_clip_c4 # if True, use C4 mode where roi_head uses the last resnet layer from backbone \n            self.use_clip_attpool = use_clip_attpool # if True (C4+text_emb_as_classifier), use att_pool to replace default mean pool\n\n        @classmethod\n        def from_config(cls, cfg):\n            # create independent backbone & RPN\n            if cfg.MODEL.CLIP.CROP_REGION_TYPE == \"RPN\": \n                # create offline cfg for the pretrained backbone & RPN\n                from detectron2.config import get_cfg\n                offline_cfg = get_cfg()\n                offline_cfg.merge_from_file(cfg.MODEL.CLIP.OFFLINE_RPN_CONFIG)\n                if cfg.MODEL.CLIP.OFFLINE_RPN_LSJ_PRETRAINED: # large-scale jittering (LSJ) pretrained RPN\n                    offline_cfg.MODEL.BACKBONE.FREEZE_AT = 0 # make all fronzon layers to \"SyncBN\"\n                    offline_cfg.MODEL.RESNETS.NORM = \"SyncBN\" # 5 resnet layers\n                    offline_cfg.MODEL.FPN.NORM = \"SyncBN\" # fpn layers\n                    offline_cfg.MODEL.RPN.CONV_DIMS = [-1, -1] # rpn layers\n                if cfg.MODEL.CLIP.OFFLINE_RPN_NMS_THRESH:\n                    offline_cfg.MODEL.RPN.NMS_THRESH = cfg.MODEL.CLIP.OFFLINE_RPN_NMS_THRESH  # 0.9\n                if cfg.MODEL.CLIP.OFFLINE_RPN_POST_NMS_TOPK_TEST:\n                    offline_cfg.MODEL.RPN.POST_NMS_TOPK_TEST = cfg.MODEL.CLIP.OFFLINE_RPN_POST_NMS_TOPK_TEST # 1000\n\n                # create offline backbone and RPN\n                offline_backbone = build_backbone(offline_cfg)\n                offline_rpn = build_proposal_generator(offline_cfg, offline_backbone.output_shape())\n\n                # convert to evaluation mode\n                for p in offline_backbone.parameters(): p.requires_grad = False\n                for p in offline_rpn.parameters(): p.requires_grad = False\n                offline_backbone.eval()\n                offline_rpn.eval()\n            # region proposals are ground-truth boxes\n            elif cfg.MODEL.CLIP.CROP_REGION_TYPE == \"GT\":\n                offline_backbone = None\n                offline_rpn = None\n                offline_cfg = None\n\n            backbone = build_backbone(cfg)\n            # build language encoder\n            if cfg.MODEL.CLIP.GET_CONCEPT_EMB: # extract concept embeddings\n                language_encoder = build_clip_language_encoder(cfg)\n            else:\n                language_encoder = None\n            roi_heads = build_roi_heads(cfg, backbone.output_shape())\n\n            return {\n                \"offline_backbone\": offline_backbone,\n                \"offline_proposal_generator\": offline_rpn, \n                \"backbone\": backbone,\n                \"language_encoder\": language_encoder, \n                \"roi_heads\": roi_heads, \n                \"input_format\": cfg.INPUT.FORMAT,\n                \"vis_period\": cfg.VIS_PERIOD,\n                \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n                \"pixel_std\": cfg.MODEL.PIXEL_STD,\n                \"clip_crop_region_type\" : cfg.MODEL.CLIP.CROP_REGION_TYPE,\n                \"use_clip_c4\": cfg.MODEL.BACKBONE.NAME == \"build_clip_resnet_backbone\",\n                \"use_clip_attpool\": cfg.MODEL.ROI_HEADS.NAME in ['CLIPRes5ROIHeads', 'CLIPStandardROIHeads'] and cfg.MODEL.CLIP.USE_TEXT_EMB_CLASSIFIER,\n                \"offline_input_format\": offline_cfg.INPUT.FORMAT if offline_cfg else None,\n                \"offline_pixel_mean\": offline_cfg.MODEL.PIXEL_MEAN if offline_cfg else None,\n                \"offline_pixel_std\": offline_cfg.MODEL.PIXEL_STD if offline_cfg else None,\n            }\n\n        @property\n        def device(self):\n            return self.pixel_mean.device\n\n        def forward(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Args:\n                batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                    Each item in the list contains the inputs for one image.\n                    For now, each item in the list is a dict that contains:\n\n                    * image: Tensor, image in (C, H, W) format.\n                    * instances (optional): groundtruth :class:`Instances`\n                    * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                    Other information that's included in the original dicts, such as:\n\n                    * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                      See :meth:`postprocess` for details.\n\n            Returns:\n                list[dict]:\n                    Each dict is the output for one input image.\n                    The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                    The :class:`Instances` object has the following keys:\n                    \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n            \"\"\"\n            if not self.training:\n                return self.inference(batched_inputs)\n            if \"instances\" in batched_inputs[0]:\n                gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n            else:\n                gt_instances = None\n\n            # localization branch: offline modules to get the region proposals\n            with torch.no_grad():  \n                if self.clip_crop_region_type == \"GT\":  # from ground-truth\n                    proposals = []\n                    for r_i, b_input in enumerate(batched_inputs): \n                        this_gt = copy.deepcopy(b_input[\"instances\"])  # Instance\n                        gt_boxes = this_gt._fields['gt_boxes'].to(self.device)\n                        this_gt._fields = {'proposal_boxes': gt_boxes, 'objectness_logits': torch.ones(gt_boxes.tensor.size(0)).to(self.device)}\n                        proposals.append(this_gt)                \n                elif self.clip_crop_region_type == \"RPN\": # from the backbone & RPN of standard Mask-RCNN, trained on base classes\n                    if self.offline_backbone.training or self.offline_proposal_generator.training:  #  was set to True in training script\n                        self.offline_backbone.eval() \n                        self.offline_proposal_generator.eval()  \n                    images = self.offline_preprocess_image(batched_inputs)\n                    features = self.offline_backbone(images.tensor)\n                    if self.offline_proposal_generator is not None:\n                        proposals, _ = self.offline_proposal_generator(images, features, None)     \n\n            # recognition branch: get 2D feature maps using the backbone of recognition branch\n            images = self.preprocess_image(batched_inputs)\n            features = self.backbone(images.tensor)\n\n            # Given the proposals, crop region features from 2D image features and classify the regions\n            if self.use_clip_c4: # use C4 + resnet weights from CLIP\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, res5=self.backbone.layer4, attnpool=self.backbone.attnpool)\n                else: # use mean pool\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, res5=self.backbone.layer4)\n            else:  # regular detector setting\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, attnpool=self.backbone.bottom_up.attnpool)\n                else: # use mean pool\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n            if self.vis_period > 0:\n                storage = get_event_storage()\n                if storage.iter % self.vis_period == 0:\n                    self.visualize_training(batched_inputs, proposals)\n            #visualize_proposals(batched_inputs, proposals, self.input_format)\n\n            losses = {}\n            losses.update(detector_losses)\n            return losses\n\n        def inference(\n            self,\n            batched_inputs: List[Dict[str, torch.Tensor]],\n            detected_instances: Optional[List[Instances]] = None,\n            do_postprocess: bool = True,\n        ):\n            \"\"\"\n            Run inference on the given inputs.\n\n            Args:\n                batched_inputs (list[dict]): same as in :meth:`forward`\n                detected_instances (None or list[Instances]): if not None, it\n                    contains an `Instances` object per image. The `Instances`\n                    object contains \"pred_boxes\" and \"pred_classes\" which are\n                    known boxes in the image.\n                    The inference will then skip the detection of bounding boxes,\n                    and only predict other per-ROI outputs.\n                do_postprocess (bool): whether to apply post-processing on the outputs.\n\n            Returns:\n                When do_postprocess=True, same as in :meth:`forward`.\n                Otherwise, a list[Instances] containing raw network outputs.\n            \"\"\"\n            assert not self.training\n\n            # localization branch: offline modules to get the region proposals\n            if self.clip_crop_region_type == \"GT\":  # from ground-truth\n                proposals = []\n                for r_i, b_input in enumerate(batched_inputs): \n                    this_gt = copy.deepcopy(b_input[\"instances\"])  # Instance\n                    gt_boxes = this_gt._fields['gt_boxes'].to(self.device)\n                    this_gt._fields = {'proposal_boxes': gt_boxes} #, 'objectness_logits': None}\n                    proposals.append(this_gt)                \n            elif self.clip_crop_region_type == \"RPN\": # from the backbone & RPN of standard Mask-RCNN, trained on base classes\n                images = self.offline_preprocess_image(batched_inputs)\n                features = self.offline_backbone(images.tensor)\n                if detected_instances is None:\n                    if self.offline_proposal_generator is not None:\n                        proposals, _ = self.offline_proposal_generator(images, features, None)     \n\n            # recognition branch: get 2D feature maps using the backbone of recognition branch\n            images = self.preprocess_image(batched_inputs)\n            features = self.backbone(images.tensor)\n\n            # Given the proposals, crop region features from 2D image features and classify the regions\n            if self.use_clip_c4: # use C4 + resnet weights from CLIP\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    results, _ = self.roi_heads(images, features, proposals, None, res5=self.backbone.layer4, attnpool=self.backbone.attnpool)\n                else: # use mean pool\n                    results, _ = self.roi_heads(images, features, proposals, None, res5=self.backbone.layer4)\n            else:  # regular detector setting\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    results, _  = self.roi_heads(images, features, proposals, None, attnpool=self.backbone.bottom_up.attnpool)\n                else:\n                    results, _  = self.roi_heads(images, features, proposals, None)\n\n            #visualize_proposals(batched_inputs, proposals, self.input_format)\n            if do_postprocess:\n                assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n                return CLIPFastRCNN._postprocess(results, batched_inputs)\n            else:\n                return results\n\n        def offline_preprocess_image(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Normalize, pad and batch the input images. Use detectron2 default processing (pixel mean & std).\n            Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n            \"\"\"\n            images = [x[\"image\"].to(self.device) for x in batched_inputs]\n            if (self.input_format == 'RGB' and self.offline_input_format == 'BGR') or \\\n                (self.input_format == 'BGR' and self.offline_input_format == 'RGB'):\n                images = [x[[2,1,0],:,:] for x in images]\n            if self.offline_div_pixel:\n                images = [((x / 255.0) - self.offline_pixel_mean) / self.offline_pixel_std for x in images]\n            else:\n                images = [(x - self.offline_pixel_mean) / self.offline_pixel_std for x in images]\n            images = ImageList.from_tensors(images, self.offline_backbone.size_divisibility)\n            return images\n\n        def preprocess_image(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Normalize, pad and batch the input images. Use CLIP default processing (pixel mean & std).\n            Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n            \"\"\"\n            images = [x[\"image\"].to(self.device) for x in batched_inputs]\n            if self.div_pixel:\n                images = [((x / 255.0) - self.pixel_mean) / self.pixel_std for x in images]\n            else:\n                images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n            images = ImageList.from_tensors(images, self.backbone.size_divisibility)\n            return images\n\n        @staticmethod\n        def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Rescale the output instances to the target size.\n            \"\"\"\n            # note: private function; subject to changes\n            processed_results = []\n            for results_per_image, input_per_image in zip(\n                instances, batched_inputs):\n                height = input_per_image[\"height\"]  # original image size, before resizing\n                width = input_per_image[\"width\"]  # original image size, before resizing\n                r = detector_postprocess(results_per_image, height, width)\n                processed_results.append({\"instances\": r})\n            return processed_results","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-26T03:17:51.499Z","updatedAt":"2024-04-26T08:15:42.511Z","trashed":false,"_rev":"wQ08HPkyd"}