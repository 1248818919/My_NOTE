{"_id":"note:mBrAUvlYXw","title":"RegionClip","content":"# RegionClip 阅读笔记\n\n> Zhong Y, Yang J, Zhang P, et al. Regionclip: Region-based language-image pretraining[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16793-16803.\n\n## 1.Abstract\n\n使用图像-文本对的对比语言-图像预训练(CLIP)在零射击和迁移学习设置下的图像分类上都取得了令人印象深刻的结果。然而，我们表明，直接应用这样的模型来识别图像区域进行对象检测会导致由于域移位而导致性能不佳:CLIP被训练为将图像作为一个整体与文本描述相匹配，而没有捕获图像区域和文本范围之间的细粒度对齐。为了缓解这一问题，我们提出了一种名为RegionCLIP的新方法，该方法大大扩展了CLIP来学习区域级视觉表示，从而实现图像区域和文本概念之间的细粒度对齐。我们的方法利用CLIP模型将图像区域与模板标题匹配，然后预训练我们的模型以在特征空间中对齐这些区域-文本对。当将我们的预训练模型转移到开放词汇表对象检测任务时，我们的方法在COCO和LVIS数据集上对新类别的性能分别优于现有的3.8 AP50和2.2 AP。此外，学习到的区域表示支持目标检测的零射击推理，在COCO和LVIS数据集上都显示出有希望的结果。\n\n## 2.Method\n\n区域-文本对的对齐。我们利用教师视觉编码器Vt来创建图像区域和我们创建的文本之间的对应关系(表示为语义嵌入)。同样，区域ri的视觉表示vti是通过将局部图像区域的特征与RoIAlign池化，从教师编码器Vt中提取出来的。然后计算vti与每个嵌入lj的概念之间的匹配分数。匹配分数S(v, l)由\n$$\nS(v,l)=\\frac{v^T\\cdot l}{||v||\\cdot||l||}.\n$$\n\n选择匹配分数最高的对象概念lm，并将其链接到区域ri。最后，我们得到每个区域的伪标签，即{vi, lm}对。\n\n我们的预训练利用了创建的区域-文本对和来自互联网的图像-文本对。给定对齐的区域-文本对(由{vi, lm}表示)，我们基于不同图像的图像区域使用对比损失和蒸馏损失预训练视觉编码器。对比损耗计算为\n$$\nL_{cntrst}=\\frac{1}{N}\\sum_i-\\log(p(v_i,l_m)),\n$$\n$$\\begin{aligned}&\\mathrm{where~}p(v_{i},l_{m})\\text{ is given by}\\\\&p(v_{i},l_{m})=\\frac{\\exp(S(v_{i},l_{m})/\\tau)}{\\exp(S(v_{i},l_{m})/\\tau)+\\sum_{k\\in\\mathcal{N}_{r_{i}}}\\exp(S(v_{i},l_{k})/\\tau)}.\\end{aligned}\n$$\nτ是一个预定义的温度。Nri表示区域ri的一组负文本样本，即与区域ri不匹配但与批处理中其他区域匹配的对象概念。除了正面和负面区域文本对的对比学习之外，我们还考虑了所有对象概念上每个图像区域的知识蒸馏。蒸馏损失定义为\n$$\nL_{dist}=\\frac1N\\sum_iL_{KL}(q_i^t,q_i),\n$$\n式中LKL为KL散度损失;qti和qi都是所有对象概念的概率。qti是教师模型中的软目标，计算公式为softmax (S(vti, l1)/τ，…， S(vti, lC)/τ)。qi计算为softmax (S(vi, l1)/τ，…， S(vi, lC)/τ)来自我们的学生模型。给定从互联网上收集的图像-文本对，我们的区域级对比损失Lcntrst可以自然地扩展到图像级对比损失Lcntrst - img。可以认为是一种特殊情况，即:(1)提取覆盖整个图像的单个全局框的视觉表示;(2)文本描述是从互联网上收集的，(3)负样本是其他图像附带的文本描述。最后，我们的整体损失函数由\n$$\nL=L_{cntrst}+L_{dist}+L_{cntrst-img}.\n$$\n\n在预训练中，我们的视觉编码器从教师模型创建的区域文本对齐中学习。这种对齐不需要人为的努力，但它不可避免地会产生噪音和弱。当对图像区域的强监督可用时(例如，人工注释的检测标签)，我们的视觉编码器可以通过简单地替换区域描述来进一步微调，如图2的面板3所示。\n\n具体来说，我们通过初始化目标检测器的视觉主干，将预训练好的视觉编码器转移到目标检测器上。为了检测图像对象，与我们的预训练一样，我们使用现成的RPN来定位对象区域，并通过匹配它们的视觉区域表示和目标对象类(例如，检测数据集中的对象类)的语义嵌入来识别这些区域。\n\n开放词汇目标检测的训练[59]。在这种情况下，检测器通过对基本类别的注释进行训练，同时期望检测检测器训练中从未见过的新类别。特别地，我们使用分类加权交叉熵损失来训练我们的检测器。(1)对于基本类别，受焦点损失[32]的启发，我们应用焦点缩放并计算基本类别的权重为(1−pb)γ，其中pb为该基本类别的softmax后的概率，γ为超参数。从经验上看，焦点缩放可以有效地减轻预训练中先前学习过的对象概念的遗忘，特别是当数据集中的基本类别很少时(例如COCO)。我们推测检测器可能会过度拟合到小的基本类别集，从而损害对新类别的泛化。(2)对于背景分类，我们使用固定的全零嵌入，并对以下背景区域应用预定义的权重[59]。\n\n## 3.Code \n\n    class CLIPFastRCNN(nn.Module):\n        \"\"\"\n        Fast R-CNN style where the cropping is conducted on feature maps instead of raw images.\n        It contains the following two components: \n        1. Localization branch: pretrained backbone+RPN or equivalent modules, and is able to output object proposals\n        2. Recognition branch: is able to recognize zero-shot regions\n        \"\"\"\n        @configurable\n        def __init__(\n            self,\n            *,\n            offline_backbone: Backbone,\n            backbone: Backbone,\n            offline_proposal_generator: nn.Module,\n            language_encoder: nn.Module, \n            roi_heads: nn.Module,\n            pixel_mean: Tuple[float],\n            pixel_std: Tuple[float],\n            input_format: Optional[str] = None,\n            vis_period: int = 0,\n            clip_crop_region_type: str = 'GT',\n            use_clip_c4: False,\n            use_clip_attpool: False,\n            offline_input_format: Optional[str] = None,\n            offline_pixel_mean: Tuple[float],\n            offline_pixel_std: Tuple[float],\n        ):\n            \"\"\"\n            Args:\n                backbone: a backbone module, must follow detectron2's backbone interface\n                proposal_generator: a module that generates proposals using backbone features\n                roi_heads: a ROI head that performs per-region computation\n                pixel_mean, pixel_std: list or tuple with #channels element, representing\n                    the per-channel mean and std to be used to normalize the input image\n                input_format: describe the meaning of channels of input. Needed by visualization\n                vis_period: the period to run visualization. Set to 0 to disable.\n            \"\"\"\n            super().__init__()\n            self.offline_backbone = offline_backbone\n            self.backbone = backbone\n            self.lang_encoder = language_encoder\n            self.offline_proposal_generator = offline_proposal_generator\n            self.roi_heads = roi_heads\n\n            self.input_format = input_format\n            self.vis_period = vis_period\n            if vis_period > 0:\n                assert input_format is not None, \"input_format is required for visualization!\"\n\n            # input format, pixel mean and std for offline modules\n            self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n            self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n            assert (\n                self.pixel_mean.shape == self.pixel_std.shape\n            ), f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"\n            if np.sum(pixel_mean) < 3.0: # converrt pixel value to range [0.0, 1.0] by dividing 255.0\n                assert input_format == 'RGB'\n                self.div_pixel = True\n            else:\n                self.div_pixel = False\n\n            if offline_input_format and offline_pixel_mean and offline_pixel_std:\n                self.offline_input_format = offline_input_format\n                self.register_buffer(\"offline_pixel_mean\", torch.tensor(offline_pixel_mean).view(-1, 1, 1), False)\n                self.register_buffer(\"offline_pixel_std\", torch.tensor(offline_pixel_std).view(-1, 1, 1), False)\n                if np.sum(offline_pixel_mean) < 3.0: # converrt pixel value to range [0.0, 1.0] by dividing 255.0\n                    assert offline_input_format == 'RGB'\n                    self.offline_div_pixel = True\n                else:\n                    self.offline_div_pixel = False\n\n            self.clip_crop_region_type = clip_crop_region_type\n            self.use_clip_c4 = use_clip_c4 # if True, use C4 mode where roi_head uses the last resnet layer from backbone \n            self.use_clip_attpool = use_clip_attpool # if True (C4+text_emb_as_classifier), use att_pool to replace default mean pool\n\n        @classmethod\n        def from_config(cls, cfg):\n            # create independent backbone & RPN\n            if cfg.MODEL.CLIP.CROP_REGION_TYPE == \"RPN\": \n                # create offline cfg for the pretrained backbone & RPN\n                from detectron2.config import get_cfg\n                offline_cfg = get_cfg()\n                offline_cfg.merge_from_file(cfg.MODEL.CLIP.OFFLINE_RPN_CONFIG)\n                if cfg.MODEL.CLIP.OFFLINE_RPN_LSJ_PRETRAINED: # large-scale jittering (LSJ) pretrained RPN\n                    offline_cfg.MODEL.BACKBONE.FREEZE_AT = 0 # make all fronzon layers to \"SyncBN\"\n                    offline_cfg.MODEL.RESNETS.NORM = \"SyncBN\" # 5 resnet layers\n                    offline_cfg.MODEL.FPN.NORM = \"SyncBN\" # fpn layers\n                    offline_cfg.MODEL.RPN.CONV_DIMS = [-1, -1] # rpn layers\n                if cfg.MODEL.CLIP.OFFLINE_RPN_NMS_THRESH:\n                    offline_cfg.MODEL.RPN.NMS_THRESH = cfg.MODEL.CLIP.OFFLINE_RPN_NMS_THRESH  # 0.9\n                if cfg.MODEL.CLIP.OFFLINE_RPN_POST_NMS_TOPK_TEST:\n                    offline_cfg.MODEL.RPN.POST_NMS_TOPK_TEST = cfg.MODEL.CLIP.OFFLINE_RPN_POST_NMS_TOPK_TEST # 1000\n\n                # create offline backbone and RPN\n                offline_backbone = build_backbone(offline_cfg)\n                offline_rpn = build_proposal_generator(offline_cfg, offline_backbone.output_shape())\n\n                # convert to evaluation mode\n                for p in offline_backbone.parameters(): p.requires_grad = False\n                for p in offline_rpn.parameters(): p.requires_grad = False\n                offline_backbone.eval()\n                offline_rpn.eval()\n            # region proposals are ground-truth boxes\n            elif cfg.MODEL.CLIP.CROP_REGION_TYPE == \"GT\":\n                offline_backbone = None\n                offline_rpn = None\n                offline_cfg = None\n\n            backbone = build_backbone(cfg)\n            # build language encoder\n            if cfg.MODEL.CLIP.GET_CONCEPT_EMB: # extract concept embeddings\n                language_encoder = build_clip_language_encoder(cfg)\n            else:\n                language_encoder = None\n            roi_heads = build_roi_heads(cfg, backbone.output_shape())\n\n            return {\n                \"offline_backbone\": offline_backbone,\n                \"offline_proposal_generator\": offline_rpn, \n                \"backbone\": backbone,\n                \"language_encoder\": language_encoder, \n                \"roi_heads\": roi_heads, \n                \"input_format\": cfg.INPUT.FORMAT,\n                \"vis_period\": cfg.VIS_PERIOD,\n                \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n                \"pixel_std\": cfg.MODEL.PIXEL_STD,\n                \"clip_crop_region_type\" : cfg.MODEL.CLIP.CROP_REGION_TYPE,\n                \"use_clip_c4\": cfg.MODEL.BACKBONE.NAME == \"build_clip_resnet_backbone\",\n                \"use_clip_attpool\": cfg.MODEL.ROI_HEADS.NAME in ['CLIPRes5ROIHeads', 'CLIPStandardROIHeads'] and cfg.MODEL.CLIP.USE_TEXT_EMB_CLASSIFIER,\n                \"offline_input_format\": offline_cfg.INPUT.FORMAT if offline_cfg else None,\n                \"offline_pixel_mean\": offline_cfg.MODEL.PIXEL_MEAN if offline_cfg else None,\n                \"offline_pixel_std\": offline_cfg.MODEL.PIXEL_STD if offline_cfg else None,\n            }\n\n        @property\n        def device(self):\n            return self.pixel_mean.device\n\n        def forward(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Args:\n                batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                    Each item in the list contains the inputs for one image.\n                    For now, each item in the list is a dict that contains:\n\n                    * image: Tensor, image in (C, H, W) format.\n                    * instances (optional): groundtruth :class:`Instances`\n                    * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                    Other information that's included in the original dicts, such as:\n\n                    * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                      See :meth:`postprocess` for details.\n\n            Returns:\n                list[dict]:\n                    Each dict is the output for one input image.\n                    The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                    The :class:`Instances` object has the following keys:\n                    \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n            \"\"\"\n            if not self.training:\n                return self.inference(batched_inputs)\n            if \"instances\" in batched_inputs[0]:\n                gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n            else:\n                gt_instances = None\n\n            # localization branch: offline modules to get the region proposals\n            with torch.no_grad():  \n                if self.clip_crop_region_type == \"GT\":  # from ground-truth\n                    proposals = []\n                    for r_i, b_input in enumerate(batched_inputs): \n                        this_gt = copy.deepcopy(b_input[\"instances\"])  # Instance\n                        gt_boxes = this_gt._fields['gt_boxes'].to(self.device)\n                        this_gt._fields = {'proposal_boxes': gt_boxes, 'objectness_logits': torch.ones(gt_boxes.tensor.size(0)).to(self.device)}\n                        proposals.append(this_gt)                \n                elif self.clip_crop_region_type == \"RPN\": # from the backbone & RPN of standard Mask-RCNN, trained on base classes\n                    if self.offline_backbone.training or self.offline_proposal_generator.training:  #  was set to True in training script\n                        self.offline_backbone.eval() \n                        self.offline_proposal_generator.eval()  \n                    images = self.offline_preprocess_image(batched_inputs)\n                    features = self.offline_backbone(images.tensor)\n                    if self.offline_proposal_generator is not None:\n                        proposals, _ = self.offline_proposal_generator(images, features, None)     \n\n            # recognition branch: get 2D feature maps using the backbone of recognition branch\n            images = self.preprocess_image(batched_inputs)\n            features = self.backbone(images.tensor)\n\n            # Given the proposals, crop region features from 2D image features and classify the regions\n            if self.use_clip_c4: # use C4 + resnet weights from CLIP\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, res5=self.backbone.layer4, attnpool=self.backbone.attnpool)\n                else: # use mean pool\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, res5=self.backbone.layer4)\n            else:  # regular detector setting\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances, attnpool=self.backbone.bottom_up.attnpool)\n                else: # use mean pool\n                    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n            if self.vis_period > 0:\n                storage = get_event_storage()\n                if storage.iter % self.vis_period == 0:\n                    self.visualize_training(batched_inputs, proposals)\n            #visualize_proposals(batched_inputs, proposals, self.input_format)\n\n            losses = {}\n            losses.update(detector_losses)\n            return losses\n\n        def inference(\n            self,\n            batched_inputs: List[Dict[str, torch.Tensor]],\n            detected_instances: Optional[List[Instances]] = None,\n            do_postprocess: bool = True,\n        ):\n            \"\"\"\n            Run inference on the given inputs.\n\n            Args:\n                batched_inputs (list[dict]): same as in :meth:`forward`\n                detected_instances (None or list[Instances]): if not None, it\n                    contains an `Instances` object per image. The `Instances`\n                    object contains \"pred_boxes\" and \"pred_classes\" which are\n                    known boxes in the image.\n                    The inference will then skip the detection of bounding boxes,\n                    and only predict other per-ROI outputs.\n                do_postprocess (bool): whether to apply post-processing on the outputs.\n\n            Returns:\n                When do_postprocess=True, same as in :meth:`forward`.\n                Otherwise, a list[Instances] containing raw network outputs.\n            \"\"\"\n            assert not self.training\n\n            # localization branch: offline modules to get the region proposals\n            if self.clip_crop_region_type == \"GT\":  # from ground-truth\n                proposals = []\n                for r_i, b_input in enumerate(batched_inputs): \n                    this_gt = copy.deepcopy(b_input[\"instances\"])  # Instance\n                    gt_boxes = this_gt._fields['gt_boxes'].to(self.device)\n                    this_gt._fields = {'proposal_boxes': gt_boxes} #, 'objectness_logits': None}\n                    proposals.append(this_gt)                \n            elif self.clip_crop_region_type == \"RPN\": # from the backbone & RPN of standard Mask-RCNN, trained on base classes\n                images = self.offline_preprocess_image(batched_inputs)\n                features = self.offline_backbone(images.tensor)\n                if detected_instances is None:\n                    if self.offline_proposal_generator is not None:\n                        proposals, _ = self.offline_proposal_generator(images, features, None)     \n\n            # recognition branch: get 2D feature maps using the backbone of recognition branch\n            images = self.preprocess_image(batched_inputs)\n            features = self.backbone(images.tensor)\n\n            # Given the proposals, crop region features from 2D image features and classify the regions\n            if self.use_clip_c4: # use C4 + resnet weights from CLIP\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    results, _ = self.roi_heads(images, features, proposals, None, res5=self.backbone.layer4, attnpool=self.backbone.attnpool)\n                else: # use mean pool\n                    results, _ = self.roi_heads(images, features, proposals, None, res5=self.backbone.layer4)\n            else:  # regular detector setting\n                if self.use_clip_attpool: # use att_pool from CLIP to match dimension\n                    results, _  = self.roi_heads(images, features, proposals, None, attnpool=self.backbone.bottom_up.attnpool)\n                else:\n                    results, _  = self.roi_heads(images, features, proposals, None)\n\n            #visualize_proposals(batched_inputs, proposals, self.input_format)\n            if do_postprocess:\n                assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n                return CLIPFastRCNN._postprocess(results, batched_inputs)\n            else:\n                return results\n\n        def offline_preprocess_image(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Normalize, pad and batch the input images. Use detectron2 default processing (pixel mean & std).\n            Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n            \"\"\"\n            images = [x[\"image\"].to(self.device) for x in batched_inputs]\n            if (self.input_format == 'RGB' and self.offline_input_format == 'BGR') or \\\n                (self.input_format == 'BGR' and self.offline_input_format == 'RGB'):\n                images = [x[[2,1,0],:,:] for x in images]\n            if self.offline_div_pixel:\n                images = [((x / 255.0) - self.offline_pixel_mean) / self.offline_pixel_std for x in images]\n            else:\n                images = [(x - self.offline_pixel_mean) / self.offline_pixel_std for x in images]\n            images = ImageList.from_tensors(images, self.offline_backbone.size_divisibility)\n            return images\n\n        def preprocess_image(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Normalize, pad and batch the input images. Use CLIP default processing (pixel mean & std).\n            Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n            \"\"\"\n            images = [x[\"image\"].to(self.device) for x in batched_inputs]\n            if self.div_pixel:\n                images = [((x / 255.0) - self.pixel_mean) / self.pixel_std for x in images]\n            else:\n                images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n            images = ImageList.from_tensors(images, self.backbone.size_divisibility)\n            return images\n\n        @staticmethod\n        def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]]):\n            \"\"\"\n            Rescale the output instances to the target size.\n            \"\"\"\n            # note: private function; subject to changes\n            processed_results = []\n            for results_per_image, input_per_image in zip(\n                instances, batched_inputs):\n                height = input_per_image[\"height\"]  # original image size, before resizing\n                width = input_per_image[\"width\"]  # original image size, before resizing\n                r = detector_postprocess(results_per_image, height, width)\n                processed_results.append({\"instances\": r})\n            return processed_results","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-26T03:17:51.499Z","updatedAt":"2024-04-26T08:43:27.476Z","trashed":false,"_rev":"V8UB9Vqui"}