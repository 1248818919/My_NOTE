{"_id":"note:tnVSHvqq4X","title":"SCIENCEQA","content":"# SCIENCEQA 阅读笔记\n\n> Lu P, Mishra S, Xia T, et al. Learn to explain: Multimodal reasoning via thought chains for science question answering[J]. Advances in Neural Information Processing Systems, 2022, 35: 2507-2521.\n\n## 1.Introduction\n\n总而言之，我们的贡献有三个方面:(a)为了弥补科学领域现有数据集的差距，我们构建了科学问答(SCIENCEQA)，这是一个包含21,208个具有丰富领域多样性的多模态科学问题的新数据集。据我们所知，SCIENCEQA是第一个大规模的多模态数据集，可以为原理和答案的解释进行注释。(b)我们表明，通过生成解释来提高模型的性能和可靠性，CoT在少量和微调学习中都有利于大型语言模型。(c)我们进一步探索了GPT-3的上界，并表明CoT有助于语言模型从更少的数据中学习。\n\n## 2.Experiment\n\n这边采用的是选择题的形式。","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/VQA","data":{},"createdAt":"2024-06-30T07:11:29.491Z","updatedAt":"2024-07-01T10:18:53.753Z","trashed":false,"_rev":"GBx77OxoU"}