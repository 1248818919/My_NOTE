{"_id":"note:ZYpzxTw1hV","title":"CatLIP","content":"# CatLIP 论文阅读笔记\n\n> Mehta S, Horton M, Faghri F, et al. CatLIP: CLIP-level Visual Recognition Accuracy with 2.7 x Faster Pre-training on Web-scale Image-Text Data[J]. arXiv preprint arXiv:2404.15653, 2024.\n\n## 1.Abstract\n\n对比学习已经成为一种变革性的方法，通过图像和文本嵌入的对齐来学习有效的视觉表征。然而，图像和文本对对比损失的两两相似度计算带来了计算上的挑战。本文提出了一种基于web规模图像文本数据的视觉模型弱监督预训练方法。该方法将图像-文本数据的预训练重构为分类任务。因此，它消除了对比损失中两两相似度计算的需要，与在web规模数据上的对比学习相比，在训练速度上实现了2.7倍的显著加速。通过广泛的实验，跨越不同的视觉任务，包括检测和分割，我们证明了该方法保持了较高的表示质量。\n\n## 2.Method\n\nCLIP对从独立的图像和文本编码器获得的图像和文本嵌入进行对齐。这种对齐是通过对比损失来实现的，对比损失计算图像和文本嵌入之间的成对相似性。然而，全局对相似度的计算要求很高。本文将图像-文本预训练作为分类任务。我们的方法包括从文本标题中提取名词并将其映射到WordNet同义词集(Miller, 1995)(第3.1节)。我们使用Dosovitskiy等人(2020)的视觉转换模型对CC3M数据集进行预训练，并通过在ImageNet-1k数据集上的线性探测精度评估其性能。与CLIP不同，所提出的方法在扩展训练中表现出不饱和(第3.2节)。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/CatLIP/pic1.jpg?raw=true)\n\n简单的来说，就是首先将一个句子中所有的名词进行提取，计算Embedding，然后将所有的类别标签进行Embedding，计算其中的相似度，找到最相似的作为这个图片的标签。","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-05-04T01:45:03.294Z","updatedAt":"2024-05-04T01:58:57.166Z","trashed":false,"_rev":"pcPt_5A31"}