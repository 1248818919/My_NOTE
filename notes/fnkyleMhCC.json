{"_id":"note:fnkyleMhCC","title":"CenterTrack","content":"# CenterTrack论文笔记\n\n## 1.OverView\n\n这里展示了这篇论文的全部方法，其中CNN代表的是CenterNet，当然也可以换成其他的网络。\n\n![Overview](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/CenterTrack/overlook.png?raw=true)\n\n\n## 2.数据增强部分\n\n针对使用时候会遇到的几种情况：丢失跟踪目标，错误定位目标，假阳性等，作者采用了一下三种办法进行数据增强。\n\n1. 给前一帧随机增加偏移（这里使用的高斯噪声），具体公式可以见如下：\n\n    $p_{0i} = (x_i + r \\times \\lambda_{jt} \\times w_i, y_i + r \\times \\lambda_{jt} \\times h_i)$\n\n    其中，$r$对高斯分布进行采样得到，$\\lambda_{jt}$取固定值0.05。\n2. 针对假阳性问题，作者在原来的目标附近添加了噪声，$\\lambda_{fp}$是添加噪声的概率。\n3. 针对假阴性问题，作者随机移除目标框，从而得以实现。\n\n以下展示了本次数据增强部分主要的代码，当然还有一些其他常用的，就不进行展示了。\n    \n    # 这一段代码是添加扰动的，对应了4.3中的数据增强\n    if h > 0 and w > 0:\n        # 计算高斯半径，对应的就是目标边界框的大小可接受范围\n        radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n        radius = max(0, int(radius))\n        max_rad = max(max_rad, radius)\n        # 原来的目标中心点\n        ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n        ct0 = ct.copy()\n        conf = 1\n        # 对其增加扰动，其实first和second的公式都是类似的\n        ct[0] = ct[0] + np.random.randn() * self.opt.hm_disturb * w\n        ct[1] = ct[1] + np.random.randn() * self.opt.hm_disturb * h\n        # 对应了first和third，简单来说，作者将他们和起来了\n        conf = 1 if np.random.random() > self.opt.lost_disturb else 0   \n\n        ct_int = ct.astype(np.int32)\n        if conf == 0:\n            pre_cts.append(ct / down_ratio)\n        else:\n            pre_cts.append(ct0 / down_ratio)\n\n        track_ids.append(ann['track_id'] if 'track_id' in ann else -1)\n        if reutrn_hm:\n            draw_umich_gaussian(pre_hm[0], ct_int, radius, k=conf)\n\n        if np.random.random() < self.opt.fp_disturb and reutrn_hm:     # 对应了Second\n            ct2 = ct0.copy()\n            # Hard code heatmap disturb ratio, haven't tried other numbers.\n            ct2[0] = ct2[0] + np.random.randn() * 0.05 * w\n            ct2[1] = ct2[1] + np.random.randn() * 0.05 * h\n            ct2_int = ct2.astype(np.int32)\n            draw_umich_gaussian(pre_hm[0], ct2_int, radius, k=conf)\n        \n## 3. 模型部分\n\n我根据理解将其分成了四个阶段，分别是load（加载图片）, pre（预处理）, net（网络推理）, post（后处理，包括了track）四个部分。其中load阶段比较简单，就先略过了。\n\n### 3.1 预处理阶段\n\n预处理阶段的代码还是比较简单的，就是简单的 Crop, resize, and normalize图像，并且给出该图像的meta信息。\n        \n    def pre_process(self, image, scale, input_meta={})：\n    \"\"\"\n        Crop, resize, and normalize image. Gather meta data for post processing and tracking.\n        返回了一个resized的img，相关的meta信息，包括输入图像大小，输出图像大小，图像的仿射变换矩阵。\n    \"\"\"\n        resized_image, c, s, inp_width, inp_height, height, width = self._transform_scale(image)\n        # 特征图CenterNet输出的特征图到输入的矩阵\n        trans_input = get_affine_transform(c, s, 0, [inp_width, inp_height])\n        out_height = inp_height // self.opt.down_ratio\n        out_width = inp_width // self.opt.down_ratio\n        # 特征图CenterNet输出的特征图到输出的矩阵\n        trans_output = get_affine_transform(c, s, 0, [out_width, out_height])\n        \n        inp_image = cv2.warpAffine(\n            resized_image, trans_input, (inp_width, inp_height),\n            flags=cv2.INTER_LINEAR)\n        inp_image = ((inp_image / 255. - self.mean) / self.std).astype(np.float32)\n\n        images = inp_image.transpose(2, 0, 1).reshape(1, 3, inp_height, inp_width)\n        # 水平翻转，这里代码写的挺简练的\n        if self.opt.flip_test:\n            images = np.concatenate((images, images[:, :, :, ::-1]), axis=0)\n        images = torch.from_numpy(images)\n        meta = {'calib': np.array(input_meta['calib'], dtype=np.float32) \\\n                if 'calib' in input_meta else \\\n                self._get_default_calib(width, height)}\n        meta.update({'c': c, 's': s, 'height': height, 'width': width,\n                     'out_height': out_height, 'out_width': out_width,\n                     'inp_height': inp_height, 'inp_width': inp_width,\n                     'trans_input': trans_input, 'trans_output': trans_output})\n        if 'pre_dets' in input_meta:\n            meta['pre_dets'] = input_meta['pre_dets']\n        if 'cur_dets' in input_meta:\n            meta['cur_dets'] = input_meta['cur_dets']\n        return images, meta\n        \n### 3.2 网络模型推理阶段\n\n在这篇论文中，作者提供了mobilenet，resnet，dla三种网络模型结构，关于DLA网络的具体信息我会在后面的bloc中给出的。以下给出了网络模型推理阶段的代码部分，\n\n值得一提的是，这里的非极大值抑制使用的最大池化操作。\n\n    def process(self, images, pre_images=None, pre_hms=None, pre_inds=None, return_time=False):\n        with torch.no_grad():\n            torch.cuda.synchronize()\n            # 利用CenterNet网络模型获取heatmap等东西\n            output = self.model(images, pre_images, pre_hms)[-1]\n            # 已经得到了各种分支的输出，对需要的分支进行sigmoid操作\n            output = self._sigmoid_output(output)\n            output.update({'pre_inds': pre_inds})\n            if self.opt.flip_test:\n                output = self._flip_output(output)\n            torch.cuda.synchronize()\n            forward_time = time.time()\n            # 解码阶段，将特征图所获取到的信息映射到原图上\n            dets = generic_decode(output, K=self.opt.K, opt=self.opt)\n            torch.cuda.synchronize()\n            for k in dets:\n                dets[k] = dets[k].detach().cpu().numpy()\n        if return_time:\n            return output, dets, forward_time\n        else:\n            return output, dets\n\n### 3.3 追踪阶段\n    \n作者专门写了一个tracker.py来实现追踪，具体代码如下\n\n    def step(self, results, public_det=None):\n    \"\"\"\n    逻辑是两个ct相减，固定i(表示当前识别到的任务)，取距离最小的j(表示已经被跟踪的人物)，从而实现跟踪。\n    Args:\n        results: 本次网络模型的结果图\n        public_det: 给定的结果\n\n    Returns:\n\n    \"\"\"\n        N = len(results)\n        M = len(self.tracks)\n        \n        # detection结果\n        dets = np.array([det['ct'] + det['tracking'] for det in results], np.float32)  # N x 2\n        track_size = np.array([((track['bbox'][2] - track['bbox'][0]) * (track['bbox'][3] - track['bbox'][1])) \\\n                               for track in self.tracks], np.float32)  # M\n        track_cat = np.array([track['class'] for track in self.tracks], np.int32)  # M\n        item_size = np.array([((item['bbox'][2] - item['bbox'][0]) * (item['bbox'][3] - item['bbox'][1])) \\\n                              for item in results], np.float32)  # N\n        item_cat = np.array([item['class'] for item in results], np.int32)  # N\n        tracks = np.array([pre_det['ct'] for pre_det in self.tracks], np.float32)  # M x 2\n        dist = (((tracks.reshape(1, -1, 2) - dets.reshape(-1, 1, 2)) ** 2).sum(axis=2))  # N x M\n\n        invalid = ((dist > track_size.reshape(1, M)) + \\\n                   (dist > item_size.reshape(N, 1)) + \\\n                   (item_cat.reshape(N, 1) != track_cat.reshape(1, M))) > 0\n        dist = dist + invalid * 1e18\n    \n        # 这里是同贪心配对进行跟踪\n        if self.opt.hungarian:\n            item_score = np.array([item['score'] for item in results], np.float32)  # N\n            dist[dist > 1e18] = 1e18\n            matched_indices = linear_assignment(dist)\n        else:\n            matched_indices = greedy_assignment(copy.deepcopy(dist))\n        unmatched_dets = [d for d in range(dets.shape[0]) \\\n                          if not (d in matched_indices[:, 0])]\n        unmatched_tracks = [d for d in range(tracks.shape[0]) \\\n                            if not (d in matched_indices[:, 1])]\n        \n        # 这里记录了没有跟踪到的结果\n        if self.opt.hungarian:\n            matches = []\n            for m in matched_indices:\n                if dist[m[0], m[1]] > 1e16:\n                    unmatched_dets.append(m[0])\n                    unmatched_tracks.append(m[1])\n                else:\n                    matches.append(m)\n            matches = np.array(matches).reshape(-1, 2)\n        else:\n            matches = matched_indices\n\n        # 记录跟踪到的对象的相应结果\n        ret = []\n        for m in matches:\n            track = results[m[0]]\n            track['tracking_id'] = self.tracks[m[1]]['tracking_id']\n            track['age'] = 1\n            track['active'] = self.tracks[m[1]]['active'] + 1\n            ret.append(track)\n\n        if self.opt.public_det and len(unmatched_dets) > 0:\n            # Public detection: only create tracks from provided detections\n            pub_dets = np.array([d['ct'] for d in public_det], np.float32)\n            dist3 = ((dets.reshape(-1, 1, 2) - pub_dets.reshape(1, -1, 2)) ** 2).sum(\n                axis=2)\n            matched_dets = [d for d in range(dets.shape[0]) \\\n                            if not (d in unmatched_dets)]\n            dist3[matched_dets] = 1e18\n            for j in range(len(pub_dets)):\n                i = dist3[:, j].argmin()\n                if dist3[i, j] < item_size[i]:\n                    dist3[i, :] = 1e18\n                    track = results[i]\n                    if track['score'] > self.opt.new_thresh:\n                        self.id_count += 1\n                        track['tracking_id'] = self.id_count\n                        track['age'] = 1\n                        track['active'] = 1\n                        ret.append(track)\n        else:\n            # Private detection: create tracks for all un-matched detections\n            for i in unmatched_dets:\n                track = results[i]\n                if track['score'] > self.opt.new_thresh:\n                    self.id_count += 1\n                    track['tracking_id'] = self.id_count\n                    track['age'] = 1\n                    track['active'] = 1\n                    ret.append(track)\n\n        for i in unmatched_tracks:\n            track = self.tracks[i]\n            if track['age'] < self.opt.max_age:\n                track['age'] += 1\n                track['active'] = 0\n                bbox = track['bbox']\n                ct = track['ct']\n                v = [0, 0]\n                track['bbox'] = [\n                    bbox[0] + v[0], bbox[1] + v[1],\n                    bbox[2] + v[0], bbox[3] + v[1]]\n                track['ct'] = [ct[0] + v[0], ct[1] + v[1]]\n                ret.append(track)\n        self.tracks = ret\n        return ret\n\n# 4.其他的相关细节\n\n## 4.1 评价指标\n\n1.MOTA：$1 - \\frac{\\sum_{t}(FP_{t}+FN_{t}+IDSW_{t})}{\\sum_{t}GT_{t}}$（正指标，越大越好，上限是1,即100%）\n\n其中，IDSW(Identity switches in frame)是多目标跟踪中的一种评价指标。它表示在跟踪过程中，同一个目标在不同帧之间的ID发生了切换的次数。具体来说，如果在相邻的两帧中，同一个目标的预测轨迹的ID发生了变化，那么这就算作一次ID切换。这个指标可以用来评估跟踪算法的稳定性和准确性。MOTA的主要问题是它考虑了跟踪器做出错误决定的次数，比如IDSW。而在某些情况下不想丢失跟踪对象的位置，我们更关心这个跟踪器跟踪某个对象的时间长短。\n\n2.MOTP:$\\frac{\\sum_{i,t}d^{i}_{t}}{\\sum_{t}c_{t}}$(越大越好，上限是1,即100%)\n\n其中，$c_{t}$表示第t帧目标$o_{i}$和假设$gt_{t}$匹配的个数，$d^{i}_{t}$表示第t帧$o_{i}$与其配对假设位置之间的距离，即匹配误差。作用：多目标跟踪的精确度，体现在确定目标位置上的精确度，用于衡量目标位置确定的精确程度。\n\n3.IDF1：$\\frac{2 \\times IDTP}{2 \\times IDTP+IDFP+IDFN}$\n\n其中，IDTP其实就是把一个ID当成一个类，看成二分类问题，和原来的TP的计算方式一样，其他的类似。\n\n4.MT：Mostly Tracked，大多数目标被跟踪的轨迹数量。目标被成功跟踪到的轨迹长度与轨迹总长度的比值大于等于80%的轨迹数量。\n\n5.ML：Mostly Lost，大多数目标被跟丢的轨迹数量。目标被成功跟踪到的轨迹长度与轨迹总长度的比值小于等于20%的轨迹数量。\n\n6.AP：平均精度，是目标检测中的一个常用指标。就是PR曲线的下的面积，PR曲线横坐标是查全率，纵坐标是查准率，当设置不同的阈值的时候，查全率和查准率是不同的。$AP_{50}$是指使用0.5作为IOU的阈值，这里要区分一下有两个阈值，一个是置信度的阈值，一个是IOU的阈值。","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2023-11-22T04:21:10.137Z","updatedAt":"2023-12-06T06:23:12.363Z","trashed":false,"_rev":"qHcd0rwst"}