{"_id":"note:tWyDhzz2Ue","title":"ViLD","content":"# ViLD 论文阅读笔记\n\n> Gu X, Lin T Y, Kuo W, et al. Open-vocabulary object detection via vision and language knowledge distillation[J]. arXiv preprint arXiv:2104.13921, 2021.\n\n## 1.Abstract\n\n开放世界目标检测是一个具有重要意义的课题，其能够检测由由文本所描述的对象。本研究最根本的挑战是训练数据的可用性。进一步扩大现有对象检测数据集中包含的类的数量的代价是十分昂贵的。为了克服这一挑战，本文提出了一种基于视觉和语言知识蒸馏的训练方法，将知识从预训练的开放世界图像分类模型(教师)提取到双阶段检测器(学生)中。具体而言，本文使用教师模型对对象提案的类别文本和图像区域进行编码。然后训练一个学生检测器，其检测框的区域嵌入与教师推断的文本和图像嵌入对齐。本文在LVIS进行基准测试，将所有罕见的类别作为训练期间未见的新类别。通过ResNet-50骨干网，ViLD获得了16.1的掩码APr，甚至比有监督的对手高出3.8。当使用更强的教师模型ALIGN进行训练时，ViLD达到26.3 apr，该模型可以直接转移到其他数据集，无需进行调整，在PASCAL VOC上达到72.2 AP50，在COCO上达到36.6 AP，在Objects365上达到11.8 AP。在COCO上，ViLD在新颖AP上比之前的核心状态(Zareian et al, 2021)高出4.8分，在整体AP上高出11.4分。\n\n## 2.Method\n\n本文采用了双阶段的检测器，第一个阶段是用来提取感兴趣区域的，该阶段的模型是与目标类无关的，只是用来提取前景和背景，获得共$N$个Region Proposal，之后将每一个Region Proposal送进类似于Image Encoder中提取Embedding，然后与Text Decoder计算相似度,得到类别。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/ViLD/pic1.jpg?raw=true)\n\n作者设计了三种Image Encoder的的模式：ViLD-Text，ViLD-Image，ViLD，第一个就是在不使用知识蒸馏（所谓知识蒸馏，可以看成两个模型，教师模型是一个特别牛逼的模型，但是他非常大，学到的数据分布很好，第二个模型较小，但是他要去拟合教师模型所学到的数据分布），第二个是仅使用知识蒸馏，第三个是全都使用。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/ViLD/pic2.jpg?raw=true)\n\n## 3.Code\n\nDataloader\n\n    class Parser(object):\n      \"\"\"Parser to parse an image and its annotations into a dictionary of tensors.\"\"\"\n\n      def __init__(self,\n                   output_size,\n                   min_level,\n                   max_level,\n                   num_scales,\n                   aspect_ratios,\n                   anchor_size,\n                   rpn_match_threshold=0.7,\n                   rpn_unmatched_threshold=0.3,\n                   rpn_batch_size_per_im=256,\n                   rpn_fg_fraction=0.5,\n                   aug_rand_hflip=False,\n                   aug_scale_min=1.0,\n                   aug_scale_max=1.0,\n                   skip_crowd_during_training=True,\n                   max_num_instances=100,\n                   include_mask=False,\n                   mask_crop_size=112,\n                   use_bfloat16=True,\n                   mode=None,\n                   # for vild\n                   visual_feature_distill=False,\n                   visual_feature_dim=512,\n                   max_num_rois=300,\n                   filter_distill_boxes_size=0):\n        \"\"\"Initializes parameters for parsing annotations in the dataset.\n\n        Args:\n          output_size: `Tensor` or `list` for [height, width] of output image. The\n            output_size should be divided by the largest feature stride 2^max_level.\n          min_level: `int` number of minimum level of the output feature pyramid.\n          max_level: `int` number of maximum level of the output feature pyramid.\n          num_scales: `int` number representing intermediate scales added\n            on each level. For instances, num_scales=2 adds one additional\n            intermediate anchor scales [2^0, 2^0.5] on each level.\n          aspect_ratios: `list` of float numbers representing the aspect raito\n            anchors added on each level. The number indicates the ratio of width to\n            height. For instances, aspect_ratios=[1.0, 2.0, 0.5] adds three anchors\n            on each scale level.\n          anchor_size: `float` number representing the scale of size of the base\n            anchor to the feature stride 2^level.\n          rpn_match_threshold:\n          rpn_unmatched_threshold:\n          rpn_batch_size_per_im:\n          rpn_fg_fraction:\n          aug_rand_hflip: `bool`, if True, augment training with random\n            horizontal flip.\n          aug_scale_min: `float`, the minimum scale applied to `output_size` for\n            data augmentation during training.\n          aug_scale_max: `float`, the maximum scale applied to `output_size` for\n            data augmentation during training.\n          skip_crowd_during_training: `bool`, if True, skip annotations labeled with\n            `is_crowd` equals to 1.\n          max_num_instances: `int` number of maximum number of instances in an\n            image. The groundtruth data will be padded to `max_num_instances`.\n          include_mask: a bool to indicate whether parse mask groundtruth.\n          mask_crop_size: the size which groundtruth mask is cropped to.\n          use_bfloat16: `bool`, if True, cast output image to tf.bfloat16.\n          mode: a ModeKeys. Specifies if this is training, evaluation, prediction\n            or prediction with groundtruths in the outputs.\n          visual_feature_distill: `bool`, whether to distill visual features.\n          visual_feature_dim: `int`, the distillation feature dimension.\n          max_num_rois: `int`, the number of precomputed rois to distill.\n          filter_distill_boxes_size: `int`, specify the size (height/width) of the\n           boxes such that the precomputed rois smaller than this size will be\n            filtered out.\n        \"\"\"\n        self._mode = mode\n        self._max_num_instances = max_num_instances\n        self._skip_crowd_during_training = skip_crowd_during_training\n        self._is_training = (mode == ModeKeys.TRAIN)\n\n        self._example_decoder = tf_example_decoder.TfExampleDecoder(\n            include_mask=include_mask,\n            visual_feature_distill=visual_feature_distill)\n\n        # Anchor.\n        self._output_size = output_size\n        self._min_level = min_level\n        self._max_level = max_level\n        self._num_scales = num_scales\n        self._aspect_ratios = aspect_ratios\n        self._anchor_size = anchor_size\n\n        # Target assigning.\n        self._rpn_match_threshold = rpn_match_threshold\n        self._rpn_unmatched_threshold = rpn_unmatched_threshold\n        self._rpn_batch_size_per_im = rpn_batch_size_per_im\n        self._rpn_fg_fraction = rpn_fg_fraction\n\n        # Data augmentation.\n        self._aug_rand_hflip = aug_rand_hflip\n        self._aug_scale_min = aug_scale_min\n        self._aug_scale_max = aug_scale_max\n\n        # Mask.\n        self._include_mask = include_mask\n        self._mask_crop_size = mask_crop_size\n\n        # Device.\n        self._use_bfloat16 = use_bfloat16\n\n        # Visual feature distill, only True during training\n        self._visual_feature_distill = visual_feature_distill\n        self._visual_feature_dim = visual_feature_dim\n\n        if self._visual_feature_distill:\n          self._max_num_rois = max_num_rois\n          self._filter_distill_boxes_size = filter_distill_boxes_size\n\n        # Data is parsed depending on the model Modekey.\n        if mode == ModeKeys.TRAIN:\n          self._parse_fn = self._parse_train_data\n        elif mode == ModeKeys.EVAL:\n          self._parse_fn = self._parse_eval_data\n        elif mode == ModeKeys.PREDICT or mode == ModeKeys.PREDICT_WITH_GT:\n          self._parse_fn = self._parse_predict_data\n        else:\n          raise ValueError('mode is not defined.')\n\n      def __call__(self, value):\n        \"\"\"Parses data to an image and associated training labels.\n\n        Args:\n          value: a string tensor holding a serialized tf.Example proto.\n\n        Returns:\n          image, labels: if mode == ModeKeys.TRAIN. see _parse_train_data.\n          {'images': image, 'labels': labels}: if mode == ModeKeys.PREDICT\n            or ModeKeys.PREDICT_WITH_GT.\n        \"\"\"\n        with tf.name_scope('parser'):\n          data = self._example_decoder.decode(\n              value,\n              visual_feat_dim=self._visual_feature_dim)\n          return self._parse_fn(data)\n\n      def _parse_train_data(self, data):\n        \"\"\"Parses data for training.\n\n        Args:\n          data: the decoded tensor dictionary from TfExampleDecoder.\n\n        Returns:\n          image: image tensor that is preproessed to have normalized value and\n            dimension [output_size[0], output_size[1], 3]\n          labels: a dictionary of tensors used for training. The following describes\n            {key: value} pairs in the dictionary.\n            image_info: a 2D `Tensor` that encodes the information of the image and\n              the applied preprocessing. It is in the format of\n              [[original_height, original_width], [scaled_height, scaled_width],\n            anchor_boxes: ordered dictionary with keys\n              [min_level, min_level+1, ..., max_level]. The values are tensor with\n              shape [height_l, width_l, 4] representing anchor boxes at each level.\n            rpn_score_targets: ordered dictionary with keys\n              [min_level, min_level+1, ..., max_level]. The values are tensor with\n              shape [height_l, width_l, anchors_per_location]. The height_l and\n              width_l represent the dimension of class logits at l-th level.\n            rpn_box_targets: ordered dictionary with keys\n              [min_level, min_level+1, ..., max_level]. The values are tensor with\n              shape [height_l, width_l, anchors_per_location * 4]. The height_l and\n              width_l represent the dimension of bounding box regression output at\n              l-th level.\n            gt_boxes: Groundtruth bounding box annotations. The box is represented\n               in [y1, x1, y2, x2] format. The coordinates are w.r.t the scaled\n               image that is fed to the network. The tennsor is padded with -1 to\n               the fixed dimension [self._max_num_instances, 4].\n            gt_classes: Groundtruth classes annotations. The tennsor is padded\n              with -1 to the fixed dimension [self._max_num_instances].\n            gt_masks: groundtrugh masks cropped by the bounding box and\n              resized to a fixed size determined by mask_crop_size.\n        \"\"\"\n        classes = data['groundtruth_classes']\n        boxes = data['groundtruth_boxes']\n        if self._include_mask:\n          masks = data['groundtruth_instance_masks']\n        if self._visual_feature_distill:\n          roi_boxes = data['roi_boxes']\n          distill_features = data['groundtruth_visual_features']\n\n        is_crowds = data['groundtruth_is_crowd']\n        # Skips annotations with `is_crowd` = True.\n        if self._skip_crowd_during_training and self._is_training:\n          num_groundtrtuhs = tf.shape(classes)[0]\n          with tf.control_dependencies([num_groundtrtuhs, is_crowds]):\n            indices = tf.cond(\n                tf.greater(tf.size(is_crowds), 0),\n                lambda: tf.where(tf.logical_not(is_crowds))[:, 0],\n                lambda: tf.cast(tf.range(num_groundtrtuhs), tf.int64))\n          classes = tf.gather(classes, indices)\n          boxes = tf.gather(boxes, indices)\n          if self._include_mask:\n            masks = tf.gather(masks, indices)\n\n        # Gets original image and its size.\n        image = data['image']\n        image_shape = tf.shape(image)[0:2]\n\n        # Normalizes image with mean and std pixel values.\n        image = input_utils.normalize_image(image)\n\n        # Flips image randomly during training.\n        if self._aug_rand_hflip:\n          if self._visual_feature_distill:\n            assert self._include_mask\n            image, boxes, masks, roi_boxes = input_utils.random_horizontal_flip(\n                image, boxes, masks, roi_boxes)\n          if self._include_mask:\n            image, boxes, masks = input_utils.random_horizontal_flip(\n                image, boxes, masks)\n          else:\n            image, boxes = input_utils.random_horizontal_flip(\n                image, boxes)\n\n        # Converts boxes from normalized coordinates to pixel coordinates.\n        # Now the coordinates of boxes are w.r.t. the original image.\n        boxes = box_utils.denormalize_boxes(boxes, image_shape)\n        if self._visual_feature_distill:\n          roi_boxes = box_utils.denormalize_boxes(roi_boxes, image_shape)\n\n          # filter out roi boxes smaller than given size\n          if self._filter_distill_boxes_size > 0:\n            roi_indices = box_utils.get_non_empty_box_indices(\n                roi_boxes,\n                self._filter_distill_boxes_size)\n            roi_boxes = tf.gather(roi_boxes, roi_indices)\n            distill_features = tf.gather(distill_features, roi_indices)\n\n        # Resizes and crops image.\n        image, image_info = input_utils.resize_and_crop_image(\n            image,\n            self._output_size,\n            padded_size=input_utils.compute_padded_size(\n                self._output_size, 2 ** self._max_level),\n            aug_scale_min=self._aug_scale_min,\n            aug_scale_max=self._aug_scale_max)\n        image_height, image_width, _ = image.get_shape().as_list()\n\n        # Resizes and crops boxes.\n        # Now the coordinates of boxes are w.r.t the scaled image.\n        image_scale = image_info[2, :]\n        offset = image_info[3, :]\n        boxes = input_utils.resize_and_crop_boxes(\n            boxes, image_scale, image_info[1, :], offset)\n        if self._visual_feature_distill:\n          roi_boxes = input_utils.resize_and_crop_boxes(\n              roi_boxes, image_scale, image_info[1, :], offset)\n\n        # Filters out ground truth boxes that are all zeros.\n        indices = box_utils.get_non_empty_box_indices(boxes)\n        boxes = tf.gather(boxes, indices)\n        classes = tf.gather(classes, indices)\n        if self._include_mask:\n          masks = tf.gather(masks, indices)\n          # Transfer boxes to the original image space and do normalization.\n          cropped_boxes = boxes + tf.tile(tf.expand_dims(offset, axis=0), [1, 2])\n          cropped_boxes /= tf.tile(tf.expand_dims(image_scale, axis=0), [1, 2])\n          cropped_boxes = box_utils.normalize_boxes(cropped_boxes, image_shape)\n          num_masks = tf.shape(masks)[0]\n          masks = tf.image.crop_and_resize(\n              tf.expand_dims(masks, axis=-1),\n              cropped_boxes,\n              box_indices=tf.range(num_masks, dtype=tf.int32),\n              crop_size=[self._mask_crop_size, self._mask_crop_size],\n              method='bilinear')\n          masks = tf.squeeze(masks, axis=-1)\n\n        # Assigns anchor targets.\n        # Note that after the target assignment, box targets are absolute pixel\n        # offsets w.r.t. the scaled image.\n        input_anchor = anchor.Anchor(\n            self._min_level,\n            self._max_level,\n            self._num_scales,\n            self._aspect_ratios,\n            self._anchor_size,\n            (image_height, image_width))\n        anchor_labeler = anchor.RpnAnchorLabeler(\n            input_anchor,\n            self._rpn_match_threshold,\n            self._rpn_unmatched_threshold,\n            self._rpn_batch_size_per_im,\n            self._rpn_fg_fraction)\n        rpn_score_targets, rpn_box_targets = anchor_labeler.label_anchors(\n            boxes, tf.cast(tf.expand_dims(classes, axis=-1), dtype=tf.float32))\n\n        # If bfloat16 is used, casts input image to tf.bfloat16.\n        if self._use_bfloat16:\n          image = tf.cast(image, dtype=tf.bfloat16)\n\n        # Packs labels for model_fn outputs.\n        labels = {\n            'anchor_boxes': input_anchor.multilevel_boxes,\n            'image_info': image_info,\n            'rpn_score_targets': rpn_score_targets,\n            'rpn_box_targets': rpn_box_targets,\n        }\n        labels['gt_boxes'] = input_utils.clip_or_pad_to_fixed_size(\n            boxes, self._max_num_instances, -1)\n        labels['gt_classes'] = input_utils.clip_or_pad_to_fixed_size(\n            classes, self._max_num_instances, -1)\n        if self._include_mask:\n          labels['gt_masks'] = input_utils.clip_or_pad_to_fixed_size(\n              masks, self._max_num_instances, -1)\n\n        if self._visual_feature_distill:\n          labels['roi_boxes'] = input_utils.clip_or_pad_to_fixed_size(\n              roi_boxes, self._max_num_rois, -1)\n          labels['gt_visual_feat'] = input_utils.clip_or_pad_to_fixed_size(\n              distill_features, self._max_num_rois, -1)\n        return image, labels\n\n      def _parse_eval_data(self, data):\n        \"\"\"Parses data for evaluation.\n\n        Args:\n          data: the decoded tensor dictionary from TfExampleDecoder.\n\n        Returns:\n          image: image tensor that is preproessed to have normalized value and\n            dimension [output_size[0], output_size[1], 3]\n          labels: a dictionary of tensors used for training. The following describes\n            {key: value} pairs in the dictionary.\n            image_info: a 2D `Tensor` that encodes the information of the image and\n              the applied preprocessing. It is in the format of\n              [[original_height, original_width], [scaled_height, scaled_width],\n            anchor_boxes: ordered dictionary with keys\n              [min_level, min_level+1, ..., max_level]. The values are tensor with\n              shape [height_l, width_l, 4] representing anchor boxes at each level.\n            groundtruths:\n              source_id: Groundtruth source id.\n              height: Original image height.\n              width: Original image width.\n              boxes: Groundtruth bounding box annotations. The box is represented\n                 in [y1, x1, y2, x2] format. The coordinates are w.r.t the scaled\n                 image that is fed to the network. The tennsor is padded with -1 to\n                 the fixed dimension [self._max_num_instances, 4].\n              classes: Groundtruth classes annotations. The tennsor is padded\n                with -1 to the fixed dimension [self._max_num_instances].\n              areas: Box area or mask area depend on whether mask is present.\n              is_crowds: Whether the ground truth label is a crowd label.\n              num_groundtruths: Number of ground truths in the image.\n        \"\"\"\n        # Gets original image and its size.\n        image = data['image']\n        image_shape = tf.shape(image)[0:2]\n\n        # Normalizes image with mean and std pixel values.\n        image = input_utils.normalize_image(image)\n\n        # Resizes and crops image.\n        image, image_info = input_utils.resize_and_crop_image(\n            image,\n            self._output_size,\n            padded_size=input_utils.compute_padded_size(\n                self._output_size, 2 ** self._max_level),\n            aug_scale_min=1.0,\n            aug_scale_max=1.0)\n        image_height, image_width, _ = image.get_shape().as_list()\n\n        # Assigns anchor targets.\n        input_anchor = anchor.Anchor(\n            self._min_level,\n            self._max_level,\n            self._num_scales,\n            self._aspect_ratios,\n            self._anchor_size,\n            (image_height, image_width))\n\n        # If bfloat16 is used, casts input image to tf.bfloat16.\n        if self._use_bfloat16:\n          image = tf.cast(image, dtype=tf.bfloat16)\n\n        # Sets up groundtruth data for evaluation.\n        groundtruths = {\n            'source_id':\n                data['source_id'],\n            'height':\n                data['height'],\n            'width':\n                data['width'],\n            'num_groundtruths':\n                tf.shape(data['groundtruth_classes']),\n            'boxes':\n                box_utils.denormalize_boxes(data['groundtruth_boxes'], image_shape),\n            'classes':\n                data['groundtruth_classes'],\n            'areas':\n                data['groundtruth_area'],\n            'is_crowds':\n                tf.cast(data['groundtruth_is_crowd'], tf.int32),\n        }\n        # TODO(b/143766089): Add ground truth masks for segmentation metrics.\n        groundtruths['source_id'] = dataloader_utils.process_source_id(\n            groundtruths['source_id'])\n        groundtruths = dataloader_utils.pad_groundtruths_to_fixed_size(\n            groundtruths, self._max_num_instances)\n\n        # Packs labels for model_fn outputs.\n        labels = {\n            'anchor_boxes': input_anchor.multilevel_boxes,\n            'image_info': image_info,\n            'groundtruths': groundtruths,\n        }\n\n        return image, labels\n\n      def _parse_predict_data(self, data):\n        \"\"\"Parses data for prediction.\n\n        Args:\n          data: the decoded tensor dictionary from TfExampleDecoder.\n\n        Returns:\n          A dictionary of {'images': image, 'labels': labels} where\n            images: image tensor that is preproessed to have normalized value and\n              dimension [output_size[0], output_size[1], 3]\n            labels: a dictionary of tensors used for training. The following\n              describes {key: value} pairs in the dictionary.\n              source_ids: Source image id. Default value -1 if the source id is\n                empty in the groundtruth annotation.\n              image_info: a 2D `Tensor` that encodes the information of the image\n                and the applied preprocessing. It is in the format of\n                [[original_height, original_width], [scaled_height, scaled_width],\n              anchor_boxes: ordered dictionary with keys\n                [min_level, min_level+1, ..., max_level]. The values are tensor with\n                shape [height_l, width_l, 4] representing anchor boxes at each\n                level.\n        \"\"\"\n        # Gets original image and its size.\n        image = data['image']\n        image_shape = tf.shape(image)[0:2]\n\n        # Normalizes image with mean and std pixel values.\n        image = input_utils.normalize_image(image)\n\n        # Resizes and crops image.\n        image, image_info = input_utils.resize_and_crop_image(\n            image,\n            self._output_size,\n            padded_size=input_utils.compute_padded_size(\n                self._output_size, 2 ** self._max_level),\n            aug_scale_min=1.0,\n            aug_scale_max=1.0)\n        image_height, image_width, _ = image.get_shape().as_list()\n\n        # If bfloat16 is used, casts input image to tf.bfloat16.\n        if self._use_bfloat16:\n          image = tf.cast(image, dtype=tf.bfloat16)\n\n        # Compute Anchor boxes.\n        input_anchor = anchor.Anchor(\n            self._min_level,\n            self._max_level,\n            self._num_scales,\n            self._aspect_ratios,\n            self._anchor_size,\n            (image_height, image_width))\n\n        labels = {\n            'source_id': dataloader_utils.process_source_id(data['source_id']),\n            'anchor_boxes': input_anchor.multilevel_boxes,\n            'image_info': image_info,\n        }\n\n        if self._mode == ModeKeys.PREDICT_WITH_GT:\n          # Converts boxes from normalized coordinates to pixel coordinates.\n          boxes = box_utils.denormalize_boxes(\n              data['groundtruth_boxes'], image_shape)\n          groundtruths = {\n              'source_id': data['source_id'],\n              'height': data['height'],\n              'width': data['width'],\n              'num_detections': tf.shape(data['groundtruth_classes']),\n              'boxes': boxes,\n              'classes': data['groundtruth_classes'],\n              'areas': data['groundtruth_area'],\n              'is_crowds': tf.cast(data['groundtruth_is_crowd'], tf.int32),\n          }\n          groundtruths['source_id'] = dataloader_utils.process_source_id(\n              groundtruths['source_id'])\n          groundtruths = dataloader_utils.pad_groundtruths_to_fixed_size(\n              groundtruths, self._max_num_instances)\n          labels['groundtruths'] = groundtruths\n\n        return {\n            'images': image,\n            'labels': labels,\n        }","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-21T08:37:17.075Z","updatedAt":"2024-04-24T02:01:28.488Z","trashed":false,"_rev":"TZU-hUnCH"}