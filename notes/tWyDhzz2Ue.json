{"_id":"note:tWyDhzz2Ue","title":"ViLD","content":"# ViLD 论文阅读笔记\n\n> Gu X, Lin T Y, Kuo W, et al. Open-vocabulary object detection via vision and language knowledge distillation[J]. arXiv preprint arXiv:2104.13921, 2021.\n\n## 1.Abstract\n\n开放世界目标检测是一个具有重要意义的课题，其能够检测由由文本所描述的对象。本研究最根本的挑战是训练数据的可用性。进一步扩大现有对象检测数据集中包含的类的数量的代价是十分昂贵的。为了克服这一挑战，本文提出了一种基于视觉和语言知识蒸馏的训练方法，将知识从预训练的开放世界图像分类模型(教师)提取到双阶段检测器(学生)中。具体而言，本文使用教师模型对对象提案的类别文本和图像区域进行编码。然后训练一个学生检测器，其检测框的区域嵌入与教师推断的文本和图像嵌入对齐。本文在LVIS进行基准测试，将所有罕见的类别作为训练期间未见的新类别。通过ResNet-50骨干网，ViLD获得了16.1的掩码APr，甚至比有监督的对手高出3.8。当使用更强的教师模型ALIGN进行训练时，ViLD达到26.3 apr，该模型可以直接转移到其他数据集，无需进行调整，在PASCAL VOC上达到72.2 AP50，在COCO上达到36.6 AP，在Objects365上达到11.8 AP。在COCO上，ViLD在新颖AP上比之前的核心状态(Zareian et al, 2021)高出4.8分，在整体AP上高出11.4分。\n\n## 2.Method\n\n本文采用了双阶段的检测器，第一个阶段是用来提取感兴趣区域的，该阶段的模型是与目标类无关的，只是用来提取前景和背景，获得共$N$个Region Proposal，之后将每一个Region Proposal送进类似于Image Encoder中提取Embedding，然后与Text Decoder计算相似度。\n\n","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-21T08:37:17.075Z","updatedAt":"2024-04-21T08:50:18.731Z","trashed":false,"_rev":"DX3MoFFKX"}