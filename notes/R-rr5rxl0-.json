{"_id":"note:R-rr5rxl0-","title":"Swin Transformer","content":"# Swin Transformer 论文阅读笔记\n\n>Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.\n\n## 1.Introduction\n\n这篇文章作者还是在探索如何将Transformer更好的运用到视觉中。在现有的Transformer领域内，tokens都是固定大小，不适合各种视觉任务的应用。此外，Transformer的计算量与图片大小是二次方的关系，然后作者就提出了Swin Transformer，他构造了层次化特征图，并且计算量与图片大小是线性关系。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic1.jpg?raw=true)\n\nSwin Transformer通过从小尺寸的补丁(用灰色表示)开始并逐渐合并更深Transformer层中的相邻补丁来构建分层表示。\n\nSwin Transformer的一个关键设计元素是它在连续的自关注层之间的窗口分区的移位,移位的窗口桥接前一层的窗口，提供它们之间的连接，显著增强建模能力.\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic2.jpg?raw=true)\n\n## 2.Method\n\n![pic3](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic3.jpg?raw=true)\n\n对于给定的RGB图，先进行patch split，这时候的patch size为$4\\times4$，然后一个线性层将他的通道数变成C，紧跟着几个Swin Transformer block，然后就是patch merging，他将附近的$4\\times4$进行concatenate操作，形成一个4C的feature，然后通过作者设计的block变成2C，之后都是类似的。\n\n接下来分析一下Attention的计算量，加入两个矩阵$A^{M\\times q},B^{q\\times N}$，则AB的计算量为$M\\times q\\times N$,根据下图，首先我们需要进行3次的参数转换，然后是两个bmm操作，然后再来一次参数转换，得到如下的公式,备注一下，这里的计算不考虑Batch这一个维度\n\n$$\n\\begin{aligned}\\Omega(\\mathbf{MSA})&=4hwC^2+2(hw)^2C,\\\\\\Omega(\\mathbf{W-MSA})&=4hwC^2+2M^2hwC,\\end{aligned}\n$$\n\n![pic4](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic4.webp?raw=true)\n\n对于W-MSA这个attention，我估计作者是将维度通道整理成了类似于$(Head\\times h\\times w/M)\\times M$这样的\n\n从上到下的第二张图，左边是均匀切割，右边是将左边的框进行了位移分别向右和向下位移了$M/2$个单位。\n\n$$\n\\begin{aligned}\n&\\hat{\\mathbf{z}}^l=\\text{W-MSA }\\left(\\mathrm{LN}\\left(\\mathbf{z}^{l-1}\\right)\\right)+\\mathbf{z}^{l-1}, \\\\\n&\\mathbf{z}^{l}=\\mathbf{MLP}\\left(\\mathbf{LN}\\left(\\hat{\\mathbf{z}}^{l}\\right)\\right)+\\hat{\\mathbf{z}}^{l}, \\\\\n&\\hat{\\mathbf{z}}^{l+1}=\\text{SW-MSA}\\left(\\mathrm{LN}\\left(\\mathbf{z}^l\\right)\\right)+\\mathbf{z}^l, \\\\\n&\\mathbf{z}^{l+1}=\\mathrm{MLP}\\left(\\mathrm{LN}\\left(\\hat{\\mathbf{z}}^{l+1}\\right)\\right)+\\hat{\\mathbf{z}}^{l+1},\n\\end{aligned}\n$$\n\n为了更好的进行注意力计算，作者提出了一种方式，如下图，他每次计算的时候首先进行cycle shift然后再对每一个子分区进行mask计算，可以在不用添加pad的情况下进行。\n\n\n![pic5](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic5.jpg?raw=true)\n\n![pic6](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic6.jpg?raw=true)\n","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2024-01-12T02:59:21.829Z","updatedAt":"2024-01-12T07:04:02.668Z","trashed":false,"_rev":"RvUOpP-zy"}