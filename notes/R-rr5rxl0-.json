{"_id":"note:R-rr5rxl0-","title":"Swin Transformer","content":"# Swin Transformer 论文阅读笔记\n\n>Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.\n\n## 1.Introduction\n\n这篇文章作者还是在探索如何将Transformer更好的运用到视觉中。在现有的Transformer领域内，tokens都是固定大小，不适合各种视觉任务的应用。此外，Transformer的计算量与图片大小是二次方的关系，然后作者就提出了Swin Transformer，他构造了层次化特征图，并且计算量与图片大小是线性关系。\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic1.jpg?raw=true)\n\nSwin Transformer通过从小尺寸的补丁(用灰色表示)开始并逐渐合并更深Transformer层中的相邻补丁来构建分层表示。\n\nSwin Transformer的一个关键设计元素是它在连续的自关注层之间的窗口分区的移位,移位的窗口桥接前一层的窗口，提供它们之间的连接，显著增强建模能力.\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic2.jpg?raw=true)\n\n## 2.Method\n\n![pic3](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Swin%20Transformer/pic3.jpg?raw=true)\n\n对于给定的RGB图，先进行patch split，这时候的patch size为$4\\times4$，然后一个线性层将他的通道数变成C，紧跟着几个Swin Transformer block，然后就是patch merging，他将附近的$4\\times4$进行concatenate操作，形成一个4C的feature，然后通过作者设计的block变成2C，之后都是类似的。\n\n接下来分析一下Attention的计算量","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2024-01-12T02:59:21.829Z","updatedAt":"2024-01-12T06:24:57.295Z","trashed":false,"_rev":"FeyWVY9yl"}