{"_id":"note:9nO_YmrfSA","title":"GLIP","content":"# GLIP 论文阅读笔记\n\n> Li L H, Zhang P, Zhang H, et al. Grounded language-image pre-training[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 10965-10975.\n\n## 1.abstract\n\n本文提出了一种基于语言-图像预训练(GLIP)的模型，用于学习对象级、语言感知和语义丰富的视觉表示。GLIP将目标检测和短语基础结合起来进行预训练。统一有两个好处:1)它允许GLIP从检测和接地数据中学习，以改进这两个任务并引导一个良好的接地模型;2) GLIP可以通过以自我训练的方式生成接地盒来利用大量的图像-文本对，使学习到的表示具有丰富的语义。在我们的实验中，我们在27M个基础数据上预训练GLIP，包括3M个人工注释和24M个网络抓取的图像-文本对。学习到的表征在各种对象级识别任务中表现出很强的零射击和少射击可转移性。1)当直接在COCO和LVIS上进行评估时(预训练时没有在COCO中看到任何图像)，GLIP分别达到49.8 AP和26.9 AP，超过了许多监督基线。在COCO上进行微调后，GLIP在val上达到60.8 AP，在测试开发上达到61.5 AP，超过了之前的SoTA。3)当转移到13个下游目标检测任务时，1次GLIP与完全监督的动态头部竞争。\n\n- 通过将对象检测重新表述为短语接地，统一检测与接地。\n- 使用大量图像-文本数据扩展视觉概念\n- 迁移学习与GLIP:一个模型为所有。\n\n\n## 2.Method\n\n对于grounding模型，其整个推理过程可以由下述三个公式表示：\n\n$$\nO=\\mathrm{Enc}_I(\\mathrm{Img}),P=\\mathrm{Enc}_L(\\mathrm{Prompt}),S_\\mathrm{ground}=OP^\\top \n$$\n\nN是区域/盒特征的数量，d是视觉特征隐藏维度，c是对象类别的数量，为了简单起见，我们忽略盒分类器中的偏差。训练时候的损失函数是：\n\n$$\n\\mathcal{L}=\\mathcal{L}_\\mathrm{cls}+\\mathcal{L}_\\mathrm{loc}.\n$$\n\n训练时候的标签生成是，假如我们输入的文本是M个单词，标签类别有C个，我们就将$T\\in\\{0,1\\}^{N\\times c}$拓展成$T\\in\\{0,1\\}^{N\\times M}$,其中训练的label生成是通过使所有子词正匹配，如果一个短语是正匹配，所有添加的tokens负匹配所有图像特征。\n\n> 相当于不考虑提示词，只考虑描述性文本，并且权重全是1，这显然不太合理，如何更好的生成label也是一个研究点\n\n本文的第二个创新点是进行早期融合，就是图中的Fusion\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/GLiD/pic1.jpg?raw=true)\n\n$$\n\\begin{aligned}&O_{\\mathrm{t2i}}^{i},P_{\\mathrm{i2t}}^{i}=\\mathrm{X-MHA}(O^{i},P^{i}),\\quad i\\in\\{0,1,..,L-1\\}&&\\text{(4)}\\\\&O^{i+1}=\\mathrm{DyHeadModule}(O^{i}+O_{\\mathrm{t2i}}^{i}),\\quad O=O^{L},&&\\text{(5)}\\\\&P^{i+1}=\\mathrm{BERTLayer}(P^{i}+P_{\\mathrm{i2t}}^{i}),\\quad P=P^{L},&&\\text{(6)}\\end{aligned}\n$$\n\n本质上来说就是添加了一个多头注意力机制进去\n\n$$\n\\begin{gathered}\nO(q) =OW^{(q,I)},P^{(q)}=PW^{(q,L)},Attn=O^{(q)}(P^{(q)})^{\\top}/\\sqrt{d}, \\\\\nP^{(v)} =PW^{(v,L)},O_{\\mathrm{t2i}}=\\mathrm{SoftMax}(Attn)P^{(v)}W^{(out,I)}, \\\\\nO^{(v)} =OW^{(v,I)},P_{\\mathrm{i2t}}=\\mathrm{SoftMax}(Attn^{\\top})O^{(v)}W^{(out,L)}, \n\\end{gathered}\n$$\n\n作者还提出了一种获得扩增数据的方式：此外，我们展示了一种有希望获得语义丰富数据的途径，而不是按比例放大检测数据:按比例放大接地数据。我们采用了一种受自我训练启发的简单方法。我们首先使用黄金(人类注释)检测和接地数据预训练教师GLIP。然后，我们使用这个教师模型来预测网络收集的图像文本数据的盒子，其中名词短语由NLP解析器检测[2]。最后，使用黄金数据和生成的伪接地数据训练学生模型。如图2所示，教师能够为语义丰富的实体生成准确的方框","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-22T08:35:02.312Z","updatedAt":"2024-04-23T03:10:24.529Z","trashed":false,"_rev":"q6T4MizDl"}