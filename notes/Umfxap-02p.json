{"_id":"note:Umfxap-02p","title":"CLIP","content":"# CLIP 阅读笔记\n\n>Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PMLR, 2021: 8748-8763.\n\n## 1.Abstract\n\n最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限制的监督形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从原始文本中学习图像是一种很有前途的选择，它利用了更广泛的监督来源。我们证明了预测哪个标题与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方法，可以在从互联网收集的4亿对(图像，文本)数据集上从头开始学习SOTA图像表示。在预训练之后，使用自然语言来参考学习到的视觉概念(或描述新的概念)，从而实现模型向下游任务的零射击转移。我们通过对30多个不同的现有计算机视觉数据集进行基准测试来研究这种方法的性能，这些数据集涵盖了OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务。\n该模型不平凡地转移到大多数任务，并且通常与完全监督的基线竞争，而不需要任何数据集特定的训练。例如，我们在ImageNet zero-shot上匹配原始ResNet-50的精度，而不需要使用它所训练的128万个训练样本中的任何一个。\n\n## 2.Method\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal","data":{},"createdAt":"2024-04-17T13:51:21.758Z","updatedAt":"2024-04-17T13:57:50.714Z","trashed":false,"_rev":"k3Yo-Uh5s"}