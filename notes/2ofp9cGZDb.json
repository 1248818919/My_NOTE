{"_id":"note:2ofp9cGZDb","title":"ViT","content":"# ViT网络模型笔记\n\n《An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale》\n\n## 1.OverView\n\n![img](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Classfication/ViT/model.png?raw=true)\n\n## 1.2 Inference\n\n按照上面的流程图，一个ViT block可以分为以下几个步骤\n\n(1) patch embedding：例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224/16x16=196个patch，即输入序列长度为196，每个patch维度16x16x3=768，线性投射层的维度为768xN (N=768)，因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是197x768。到目前为止，已经通过patch embedding将一个视觉问题转化为了一个seq2seq问题。\n\n(2) positional encoding（standard learnable 1D position embeddings）：ViT同样需要加入位置编码，位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列embedding的维度相同（768）。注意位置编码的操作是sum，而不是concat。加入位置编码信息之后，维度依然是197x768。\n\n(3) LN/multi-head attention/LN：LN输出维度依然是197x768。多头自注意力时，先将输入映射到q，k，v，如果只有一个头，qkv的维度都是197x768，如果有12个头（768/12=64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是197x768。\n\n(4) MLP：将维度放大再缩小回去，197x768放大为197x3072，再缩小变为197x768。\n\n一个block之后维度依然和输入相同，都是197x768，因此可以堆叠多个block。最后会将特殊字符cls对应的输出 Z0 作为encoder的最终输出 ，代表最终的image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下图公式(4)，后面接一个MLP进行图片分类\n\n![eq](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Classfication/ViT/%E5%85%AC%E5%BC%8F.png?raw=true)","tags":[],"folderPathname":"/Computer Vision/Classfication","data":{},"createdAt":"2023-12-05T02:57:45.534Z","updatedAt":"2023-12-09T03:28:37.496Z","trashed":false,"_rev":"7StooIOek"}