{"_id":"note:hqagTYSTea","title":"pytorch20231228","content":"# **可复现性**\n\n - **seed**\n\n为CPU中设置种子，生成随机数：torch.manual_seed(number)\n\n为特定GPU设置种子，生成随机数：torch.cuda.manual_seed(number)\n\n为所有GPU设置种子，生成随机数：torch.cuda.manual_seed_all(number)\n\n上面的操作是为了可复现结果，具体的代码是：\n\n    random.seed(seed)      #系统随机函数的种子\n    np.random.seed(seed)   #numpy随机函数的种子\n    torch.manual_seed(seed) #CPU生成随机数的种子\n    torch.cuda.manual_seed(seed) #为特定GPU设置种子，生成随机数，\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nHowever, as long as **torch.manual_seed()** is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.\n\n- **CUDA convolution benchmarking**\n\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. \n\nDisabling the benchmarking feature with **torch.backends.cudnn.benchmark = False** causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n\n- Avoiding nondeterministic algorithms\n\n**torch.use_deterministic_algorithms(True)** lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).\n\nWhen torch.bmm() is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used:\n\ntorch.backends.cudnn.deterministic = True和torch.use_deterministic_algorithms(True)区别:前者控制的范围小，后者控制的范围大\n\n- DataLoader\nDataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use worker_init_fn() and generator to preserve reproducibility:\n\n    def seed_worker(worker_id):\n        worker_seed = torch.initial_seed() % 2**32\n        numpy.random.seed(worker_seed)\n        random.seed(worker_seed)\n\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )","tags":[],"folderPathname":"/Pytorch","data":{},"createdAt":"2023-12-28T12:04:33.548Z","updatedAt":"2023-12-28T12:36:42.977Z","trashed":false,"_rev":"tDxH7a7Ja"}