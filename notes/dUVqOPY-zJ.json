{"_id":"note:dUVqOPY-zJ","title":"Video-ChatGPT","content":"# Video-Chatgpt 论文阅读笔记\n\n> Maaz M, Rasheed H, Khan S, et al. Video-chatgpt: Towards detailed video understanding via large vision and language models[J]. arXiv preprint arXiv:2306.05424, 2023.\n\n## 1.Introduction\n\n由大型语言模型(llm)推动的对话代理提供了一种与可视化数据交互的新方法。虽然已经有了基于图像的对话模型的初步尝试，但这项工作通过引入视频聊天技术(Video-ChatGPT)解决了基于视频的对话的未开发领域。它是一个多模态模型，将视频适应视觉编码器与LLM合并。由此产生的模型能够理解并生成关于视频的详细对话。我们引入了一个由100,000个视频指令对组成的新数据集，用于训练通过手动和半自动管道获得的视频聊天技术，该数据集易于扩展且对标记噪声具有鲁棒性。我们还开发了基于视频的对话模型的定量评估框架，以客观地分析基于视频的对话模型的优缺点。\n\n## 2.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/%E8%A7%86%E9%A2%91/Video-Chat/pic1.jpg?raw=true)\n\n首先使用ViT-L/14模型来当作visual encoder，在空间与时间上分别进行Pooling操作得到时间特征与空间特征，之后cat两个特征，送入Linear Layer来转换向量空间，用固定参数的大模型进行微调。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/multi-modal/CLIP%E7%9A%84%E8%A1%8D%E7%94%9F%E5%88%86%E6%94%AF/%E8%A7%86%E9%A2%91/Video-Chat/pic2.jpg?raw=true)\n\n作者使用了两种标注，人工标注和半自动注释框架，后者就是用一些模型去检测一些关键帧中的图片然后用模型去生成caption。\n\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Video","data":{},"createdAt":"2024-07-23T03:54:17.804Z","updatedAt":"2024-07-23T08:44:15.438Z","trashed":false,"_rev":"PPvUeRYYi"}