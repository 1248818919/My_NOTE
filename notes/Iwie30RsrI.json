{"_id":"note:Iwie30RsrI","title":"NetTrack","content":"# NetTrack 论文阅读笔记\n\n> Zheng, Guangze, et al. \"NetTrack: Tracking Highly Dynamic Objects with a Net.\" arXiv preprint arXiv:2403.11186 (2024).\n\n## 1.Abstract\n\n开放世界目标的复杂动态性对多目标跟踪(MOT)提出了不可忽视的挑战，其通常的表现有严重形变、快速运动和遮挡。大多数仅依赖于粗粒度对象线索的方法容易由于动态对象的内部结构关系的扭曲而导致失准。为了解决这个问题，本工作提出了一个高效、通用的跟踪框架NetTrack，其对动态目标的细粒度进行学习，从而提升模型的鲁棒性。具体来说，NetTrack利用关键点级别的视觉线索，构建了一个动态感知关联框架，其由细粒度采样器和对应的匹配方法组成。此外，NetTrack学习对象-文本对应关系实现细粒度级别的定位。为了评估动态开放世界场景下的MOT任务，本文构建了一个具有高动态多样性和开放世界场景的鸟群跟踪(BFT)数据集。对BFT的综合评价验证了细粒度学习对目标动态性的有效性，并在具有挑战性的开放世界基准(TAO、TAO- ow、AnimalTrack和GMOT-40)上进行了全面的迁移实验，验证了NetTrack即使不进行微调也具有强大的泛化能力。\n\n## 2.Introduction\n\n开放世界物体的高动态性，表现为剧烈的变形、快速的运动和频繁的遮挡，对现有方法提出了两个主要方面的挑战:\n\n- 对于大多数仅依赖粗粒度视觉表示的方法，由于对象的内部关系被扭曲，高动态性使得时间连续性在关联方面变得脆弱。这些方法通常将整个对象表示为粗粒度边界框或相应的特征，而动态性显著降低了这种表示在不同时间步长的相似性。\n- 高度的动态性也对建立准确的对象-文本对应关系提出了挑战。最先进的(SoTA)方法通常在预训练中学习整个图像和文本之间的粗粒度对应关系。对于严重变形或闭塞的物体，这些方法往往难以定位。\n\n本文所做的贡献如下：\n\n- 用于动态感知关联的细粒度网络不是将对象视为粗粒度实体，而是使用细粒度网络跟踪对象，该网络利用对象外观表面上的兴趣点(poi)。\n\n- 用于细粒度本地化的对象-文本对应,本工作采用预训练方法，通过短语接地来跟踪细粒度的物文对应\n\n- 本文引入了一个高度动态的开放世界MOT数据集——鸟群跟踪(BFT)，以评估跟踪方法在跟踪高动态目标方面的性能。\n\n## 3.Related Work\n\n- ** 开放世界多目标跟踪方法 ** 传统的MOT方法通常关注有限的场景和对象类别，例如公共场所的行人[4,59,65,70]或自动驾驶场景中的车辆[53,71]。相比之下，开放世界跟踪任务要求跟踪器具有跟踪复杂动态场景中任何物体的能力。基于CLIP的开放集目标检测的兴起促进了这一任务，激发了先进的开放世界跟踪基线利用CLIP风格的预训练，通过利用文本和图像之间的对应关系来实现泛化。然而，这些主流的跟踪方法通常将对象视为粗粒度的边界框，但开放世界对象的高动态性往往会破坏这种粗表示的时间相似性。此外，与在类似clip的预训练中使用的肤浅融合的视觉语言特征相比，动态对象的定位通常需要在对象和文本之间建立细粒度的对应关系，以抵消对象的外观失真或损伤。最近出现的物理点跟踪方法[11,12,22,28,63]启发了这项工作，以引入物体的细粒度视觉线索。这些方法旨在跟踪视频片段上的任意物理点，依赖于点级外观表示而不是粗略地传播整个对象，因此有望保持对动态对象的良好泛化。此外，基于短语基础的预训练方法[21]也已应用于开放集目标检测任务[32,41,68]，并且由于对象级、语言感知和语义丰富的视觉表示，预计其在动态目标跟踪方面的潜在优势。\n\n- ** 开放世界多目标跟踪基准 ** TAO, GMOT-40, AnimalTrack, TAO-OW \n\n> Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. TAO: A Large-Scale Bench-\nmark for Tracking Any Object. In ECCV, pages 436–454,2020. 3, 5, 7, 2\n\n> Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, and Haibin Ling. GMOT-40: A Benchmark for\nGeneric Multiple Object Tracking. In CVPR, pages 6719–6728, 2021. 3, 5, 7, 2\n\n> Yang Liu, Idil Esen Zulfikar, Jonathon Luiten, Achal Dave, Deva Ramanan, Bastian Leibe, Aljoˇsa Oˇsep, and Laura Leal-Taixé. Opening Up Open World Tracking. In CVPR, pages 19045–19055, 2022. 1, 3, 5, 6, 7, 8, 2\n\n> Libo Zhang, Junyuan Gao, Zhen Xiao, and Heng Fan. AnimalTrack: A Benchmark for Multi-Animal Tracking in the\nWild. IJCV, 131(2):496–513, 2023. 3, 5, 7, 2\n\n## 4.Method\n\n- **Fine-grained sampler**","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2024-04-16T07:53:07.435Z","updatedAt":"2024-04-16T09:29:49.710Z","trashed":false,"_rev":"sTwB3KP6M"}