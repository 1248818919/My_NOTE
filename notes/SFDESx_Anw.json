{"_id":"note:SFDESx_Anw","title":"Generative Adversarial Nets","content":"# Generative Adversarial Nets论文笔记\n\n>Goodfellow I , Pouget-Abadie J , Mirza M ,et al.Generative Adversarial Nets[C]//Neural Information Processing Systems.MIT Press, 2014.DOI:10.3156/JSOFT.29.5_177_2.\n\n## 1.Overview\n\n这篇文章的主要贡献是奠定了GAN的理论基础，它提出了 GAN 这个模型框架，讨论了非饱和的损失函数，然后对于最佳判别器(optimal discriminator)给出其导数，然后进行证明；最后是在 Mnist、TFD、CIFAR-10 数据集上进行了实验。\n\n我们提出了一个通过对抗过程来估计生成模型的新框架，其中我们同时训练两个模型:捕获数据分布的生成模型G和估计样本来自训练数据而不是G的概率的判别模型D。G的训练过程是最大化D犯错的概率。这个框架对应于一个极大极小的二人博弈。在任意函数G和D的空间中，存在一个唯一解，G恢复训练数据分布，D处处等于12。在G和D由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本过程中，不需要任何马尔可夫链或展开近似推理网络。通过对生成的样本进行定性和定量评估，实验证明了该框架的潜力。\n\n## 2.Method\n\n当模型都是多层感知器时，对抗性建模框架最容易应用。为了了解生成器在数据$\\boldsymbol{x}$上的分布$p_g$，我们定义了输入噪声变量$p_{\\boldsymbol{z}}(\\boldsymbol{z})$的先验，然后将到数据空间的映射表示为$G\\left(\\boldsymbol{z} ; \\theta_g\\right)$，其中$G$是由参数为$\\theta_g$的多层感知器表示的可微函数。我们还定义了第二个多层感知器$D(\\boldsymbol{x})$输出单个标量。$D(\\boldsymbol{x})$表示x来自数据而不是$p_g$的概率。我们训练$D$以最大化为训练样例和$G$的样本分配正确标签的概率。我们同时训练$G$以最小化$\\log (1-D(G(\\boldsymbol{z}))):$\n\n$$\n\\min _G \\max _D V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\n","tags":[],"folderPathname":"/GAN","data":{},"createdAt":"2023-12-21T06:13:00.143Z","updatedAt":"2023-12-21T08:53:42.475Z","trashed":false,"_rev":"mhWvxLrwp"}