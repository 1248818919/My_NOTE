{"_id":"note:SFDESx_Anw","title":"Generative Adversarial Nets","content":"# Generative Adversarial Nets论文笔记\n\n>Goodfellow I , Pouget-Abadie J , Mirza M ,et al.Generative Adversarial Nets[C]//Neural Information Processing Systems.MIT Press, 2014.DOI:10.3156/JSOFT.29.5_177_2.\n\n## 1.Overview\n\n这篇文章的主要贡献是奠定了GAN的理论基础，它提出了 GAN 这个模型框架，讨论了非饱和的损失函数，然后对于最佳判别器(optimal discriminator)给出其导数，然后进行证明；最后是在 Mnist、TFD、CIFAR-10 数据集上进行了实验。\n\n我们提出了一个通过对抗过程来估计生成模型的新框架，其中我们同时训练两个模型:捕获数据分布的生成模型G和估计样本来自训练数据而不是G的概率的判别模型D。G的训练过程是最大化D犯错的概率。这个框架对应于一个极大极小的二人博弈。在任意函数G和D的空间中，存在一个唯一解，G恢复训练数据分布，D处处等于12。在G和D由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本过程中，不需要任何马尔可夫链或展开近似推理网络。通过对生成的样本进行定性和定量评估，实验证明了该框架的潜力。\n\n![overview](https://github.com/1248818919/My_NOTE/blob/master/assets/GAN/Generative%20Adversarial%20Nets/overview.jpg?raw=true)\n\n## 2.Method\n\n当模型都是多层感知器时，对抗性建模框架最容易应用。为了了解生成器在数据$\\boldsymbol{x}$上的分布$p_g$，我们定义了输入噪声变量$p_{\\boldsymbol{z}}(\\boldsymbol{z})$的先验，然后将到数据空间的映射表示为$G\\left(\\boldsymbol{z} ; \\theta_g\\right)$，其中$G$是由参数为$\\theta_g$的多层感知器表示的可微函数。我们还定义了第二个多层感知器$D(\\boldsymbol{x})$输出单个标量。$D(\\boldsymbol{x})$表示x来自数据而不是$p_g$的概率。我们训练$D$以最大化为训练样例和$G$的样本分配正确标签的概率。我们同时训练$G$以最小化$\\log (1-D(G(\\boldsymbol{z}))):$\n\n$$\n\\min _G \\max _D V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\n![Algorithm.jpg](https://github.com/1248818919/My_NOTE/blob/master/assets/GAN/Generative%20Adversarial%20Nets/algorithm.jpg?raw=true)\n\n## 3.Theoretical Results\n\n这里作者进行了证明，在4.1节中展示这个极大极小对策对于$p_g = p_{data}$具有全局最优。然后，我们将在4.2节中说明，算法1优化了$Eq 1$，从而获得了期望的结果。\n\n### 证明在给定生成G，最优化判别器D取$p_g=p_{\\text {data}}$最优\n\nProposition 1. For $G$ fixed, the optimal discriminator $D$ is\n$$\nD_G^*(\\boldsymbol{x})=\\frac{p_{\\text {data }}(\\boldsymbol{x})}{p_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\n$$\n\n**证明：**\n$$\n\\begin{aligned}\nV(G, D) & =\\int_{\\boldsymbol{x}} p_{\\text {data }}(\\boldsymbol{x}) \\log (D(\\boldsymbol{x})) d x+\\int_z p_{\\boldsymbol{z}}(\\boldsymbol{z}) \\log (1-D(g(\\boldsymbol{z}))) d z \\\\\n& =\\int_{\\boldsymbol{x}} p_{\\text {data }}(\\boldsymbol{x}) \\log (D(\\boldsymbol{x}))+p_g(\\boldsymbol{x}) \\log (1-D(\\boldsymbol{x})) d x\n\\end{aligned}\n$$\n对于任意$a,b \\in \\mathbb{R}$,函数$y \\rightarrow a \\log (y)+b \\log (1-y)$的最小值在[0,1]上是$\\frac{a}{a+b}$,然后$V(G,D)$可以写成：\n$$\n\\begin{aligned}\nC(G) & =\\max _D V(G, D) \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log D_G^*(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}\\left[\\log \\left(1-D_G^*(G(\\boldsymbol{z}))\\right)\\right] \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log D_G^*(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[\\log \\left(1-D_G^*(\\boldsymbol{x})\\right)\\right] \\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log \\frac{p_{\\text {data }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_g}\\left[\\log \\frac{p_g(\\boldsymbol{x})}{p_{\\text {data }}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]\n\\end{aligned}\n$$\n\n### 全局最小值$C(G)$可以在$p_g=p_{\\text {data}}$的时候达到，并且最小值是$-\\log4$\n\n当我们固定了$D$，要优化$G$的时候，利用轮转不等式，可以很轻松得到$p_g=p_{\\text {data}}$的时候，上面去最小值，且为$-\\log4$，而等式可以进一步被写成：\n$$\nC(G)=-\\log (4)+K L\\left(p_{\\text {data }} \\| \\frac{p_{\\text {data }}+p_g}{2}\\right)+K L\\left(p_g \\| \\frac{p_{\\text {data }}+p_g}{2}\\right)\n$$\n如果用Jansen-Shannon散度表示的话为：\n$$\nC(G)=-\\log (4)+2 \\cdot J S D\\left(p_{\\text {data }} \\| p_g\\right)\n$$\n\n\n### 算法1的收敛性\n\n如果$G$和$D$有足够的容量，并且在算法1的每个步骤，允许鉴别器在给定$G$的情况下达到其最优值，并且更新$p_g$以改进准则。然后作者用凸优化对他进行了解释，（凸函数上确界的子导数）包括（函数在达到最大值的点上的导数），看不太懂，好好看看凸优化。\n\n## 4.优缺点\n\n缺点：主要是为𝑃𝑔(𝑥)没有明确的表示，可解释性差，且训练期间，生成器和判别器必须很好的同步（尤其是，不更新D时，G不必过度训练，为避免“Helvetica情景”）\n\n优点：根据实际结果。GAN的结果看上去可以比其他模型产生更好的样本；无需马尔可夫链，仅用反向传播就可以获得梯度，学习间无需推理，且模型中可融入多种函数","tags":[],"folderPathname":"/GAN","data":{},"createdAt":"2023-12-21T06:13:00.143Z","updatedAt":"2023-12-21T10:14:05.386Z","trashed":false,"_rev":"dimPL_gsY"}