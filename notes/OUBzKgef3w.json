{"_id":"note:OUBzKgef3w","title":"ARTrack","content":"# ARTrack 论文笔记\n\n《》\n## 1.OverView\n\n本文提出了ARTrack，一个用于视觉对象跟踪的自回归框架。ARTrack将跟踪视为一项坐标序列解释任务，逐步估计物体轨迹，其中当前估计是由先前的状态引起的，进而影响子序列。这种时间自回归方法对轨迹的顺序演化进行建模，以保持跨帧跟踪对象，使其优于现有的仅考虑每帧定位精度的基于模板匹配的跟踪器。ARTrack简单直接，省去了定制的本地化头和后期处理。尽管ARTrack简单，但它在主流基准数据集上实现了最先进的性能。\n\n![model.jpg](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/ARTrack/model.jpg?raw=true)\n\n## 2.Approach\n\n作者将track这个任务看成了序列坐标解释任务，用公式来表达就是：\n\n$P(Y^t|Y^{t-N:t-1},(C,Z,X^t))$ C表示command token，和Seqtrack中的start的start token有些类似。\n\n当N=0的时候， 当前的预测全靠当前图像，而不依赖于之前坐标的相关信息。其实，简单一点来看，就是把一连串的坐标丢进一个深度神经网络之后，然后用当前的图像来纠正只靠深度神经网络中的产生的相关坐标。\n\n**Tokenization** 作者受到了Pix2Seq框架的启发，将连续的坐标信息离散化，和Seq中使用的方法一样，这里坐标表示是$[x_{min},y_{min},x_{max},y_{max}]$,离散到$[1,n_{bins}]$这个空间中,当bin的数量大于或等于图像分辨率时，可以实现零量化误差。作者说，这种新的回归避免了从图像特征到坐标的直接非线性映射，这通常是困难的。在去字典化中，我们将输出令牌特性与共享词汇表相匹配，以找到最可能的位置。（不是很懂）\n\n**Trajectory coordinate mapping** 这个玩意，作者是说，由于裁剪的存在，模型产生的坐标是相对于裁剪后的图片的，所以作者说将box坐标映射到一个坐标系中是很有必要的，具体的可以见下图\n![mapping](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/ARTrack/mapping.jpg?raw=true)\n\n**Representation range of vocabulary** \n可以基于搜索区域的大小来设置词汇表的表示范围，但是由于对象的快速移动，先前的轨迹序列有时可能延伸到搜索区域的边界之外。为此，我们将表示范围扩展为搜索区域范围的倍数（例如，如果搜索区域范围为$[0.0，1.0]$，我们将其扩展为$[-0.5，1.5]$）。这使词汇表能够包括位于搜索区域之外的坐标，这又允许模型捕获更多先前的运动线索用于跟踪，并预测延伸到搜索区域之外的边界框。\n\n**Encoder** 使用VIT模型架构，和OSTrack采用一样的架构。\n\n**Decoder** 作者设计了两种解码器的架构，一个是合在一起，一个是解耦分开，设计解耦分开是因为为了提高跟踪效率，具体来说，自注意层和交叉注意层被解耦并单独堆叠，这样，可以并行地进行视觉特征的交叉注意，这是解码器中最耗时的计算。\n和SeqTrack一样，也是采用自回归的模式进行。\n\n**Traing and Inference** 训练的话，作者是采用了交叉熵损失函数搭配SIoU损失函数进行训练，SIoU函数需要等我查一下资料。\n\n## 3.Experiment\n\n![res](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/ARTrack/res.jpg?raw=true)\n\n## 4.Abliation experiment\n\n作者还比较了搜索区域的大小，序列长度的大小三个关系，结果如下。\n![abliation](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/ARTrack/performance.jpg?raw=true)\n\n作者还进行了可视化操作，这里就不放图了。\n\n作者还进行了bins的大小选择。\n\n作者还进行了损失函数两者是否必要的比较。\n","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2023-12-08T06:21:10.786Z","updatedAt":"2023-12-08T07:42:42.580Z","trashed":false,"_rev":"xAbRFWIqj"}