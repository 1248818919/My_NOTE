{"_id":"note:_1qFVBu9Dg","title":"transformer","content":"# transformer模型架构\n\n## 1.OverView\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/model.png?raw=true)\n\n### 1.1 input Embedding\n\n首先，对于每个单词进行编码，有word2vec，GloVe，one-hot编码等\n\n#### 1.Word2vec\n\n基本思想：通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。\n\n通常情况下，我们可以维护一个查询表。表中每一行都存储了一个特定词语的向量值，每一列的第一个元素都代表着这个词本身，以便于我们进行词和向量的映射（如“我”对应的向量值为 [0.3，0.5，0.7，0.9，-0.2，0.03] ）。给定任何一个或者一组单词，我们都可以通过查询这个excel，实现把单词转换为向量的目的，这个查询和替换过程称之为Embedding Lookup。\n\n![img1](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img1.png?raw=true)\n\n\n然而在进行神经网络计算的过程中，需要大量的算力，常常要借助特定硬件（如GPU）满足训练速度的需求。GPU上所支持的计算都是以张量（Tensor）为单位展开的，因此在实际场景中，我们需要把Embedding Lookup的过程转换为张量计算：\n\n![img2](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img2.png?raw=true)\n\n**如何让向量具有语义信息？**\n\n在自然语言处理研究中，科研人员通常有一个共识：使用一个单词的上下文来了解这个单词的语义，比如：\n\n    “苹果手机质量不错，就是价格有点贵。”\n    “这个苹果很好吃，非常脆。”\n    “菠萝质量也还行，但是不如苹果支持的APP多。”\n\n在上面的句子中，我们通过上下文可以推断出第一个“苹果”指的是苹果手机，第二个“苹果”指的是水果苹果，而第三个“菠萝”指的应该也是一个手机。在自然语言处理领域，使用上下文描述一个词语或者元素的语义是一个常见且有效的做法。我们可以使用同样的方式训练词向量，让这些词向量具备表示语义信息的能力。\n\n2013年，Mikolov提出的经典word2vec算法就是通过上下文来学习语义信息。word2vec包含两个经典模型：CBOW（Continuous Bag-of-Words）和Skip-gram。\n\n    CBOW：通过上下文的词向量推理中心词。\n    Skip-gram：根据中心词推理上下文。\n\n![img3]()","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2023-12-04T07:10:54.575Z","updatedAt":"2023-12-04T07:52:53.508Z","trashed":false,"_rev":"P_Xq4pHmU"}