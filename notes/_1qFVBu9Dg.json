{"_id":"note:_1qFVBu9Dg","title":"transformer","content":"# transformer模型架构\n\n## 1.OverView\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/model.png?raw=true)\n\n### 1.1 input Embedding\n\n首先，对于每个单词进行编码，有word2vec，GloVe，one-hot编码等,dmodel=512表示词向量的维度，然后进行position-encoding（可以通过学习得到，但是transformer中是固定的），然后相加，但是这里没有给出position-encoding公式推导，估计是公式有点问题，所以这里就先不记录了。\n\n#### 1.Word2vec\n\n基本思想：通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。\n\n通常情况下，我们可以维护一个查询表。表中每一行都存储了一个特定词语的向量值，每一列的第一个元素都代表着这个词本身，以便于我们进行词和向量的映射（如“我”对应的向量值为 [0.3，0.5，0.7，0.9，-0.2，0.03] ）。给定任何一个或者一组单词，我们都可以通过查询这个excel，实现把单词转换为向量的目的，这个查询和替换过程称之为Embedding Lookup。\n\n![img1](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img1.png?raw=true)\n\n\n然而在进行神经网络计算的过程中，需要大量的算力，常常要借助特定硬件（如GPU）满足训练速度的需求。GPU上所支持的计算都是以张量（Tensor）为单位展开的，因此在实际场景中，我们需要把Embedding Lookup的过程转换为张量计算：\n\n![img2](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img2.png?raw=true)\n\n**如何让向量具有语义信息？**\n\n在自然语言处理研究中，科研人员通常有一个共识：使用一个单词的上下文来了解这个单词的语义，比如：\n\n    “苹果手机质量不错，就是价格有点贵。”\n    “这个苹果很好吃，非常脆。”\n    “菠萝质量也还行，但是不如苹果支持的APP多。”\n\n在上面的句子中，我们通过上下文可以推断出第一个“苹果”指的是苹果手机，第二个“苹果”指的是水果苹果，而第三个“菠萝”指的应该也是一个手机。在自然语言处理领域，使用上下文描述一个词语或者元素的语义是一个常见且有效的做法。我们可以使用同样的方式训练词向量，让这些词向量具备表示语义信息的能力。\n\n2013年，Mikolov提出的经典word2vec算法就是通过上下文来学习语义信息。word2vec包含两个经典模型：CBOW（Continuous Bag-of-Words）和Skip-gram。\n\n    CBOW：通过上下文的词向量推理中心词。\n    Skip-gram：根据中心词推理上下文。\n\n![img3](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img3.png?raw=true)\n\n**CBOW模型**\n\n![img4](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img4.png?raw=true)\n\n__输入层__： 一个形状为C×V的one-hot张量，其中C代表上下文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示。\n__隐藏层__： 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。\n__输出层__： 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：\n\n**Skip-gram模型**\n\n![img5](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img5.png?raw=true)\n\n**Input Layer（输入层）**： 接收一个one-hot张量$V∈R^{1×vocab\\_size}$作为网络的输入，假设vocab_size为5000。<br>\n**Hidden Layer（隐藏层）**： 将张量V乘以一个word embedding张量$W1∈R^{vocab\\_size×embed\\_size}$，假设embed_size为128，并把结果作为隐藏层的输出，得到一个形状为$R^{1×embed\\_size}$的张量，里面存储着当前句子中心词的词向量。<br>\n**Output Layer（输出层）**： 将隐藏层的结果乘以另一个word embedding张量$W2∈R^{embed\\_size×vocab\\_size}$，得到一个形状为$R^{1×vocab\\_size}$的张量。这个张量经过softmax变换后，就得到了使用当前中心词对上下文的预测结果。根据这个softmax的结果，我们就可以去训练词向量模型。\n\nSkip-gram在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。\n\n### 1.2 Transformer的Encoder\n\n![img6](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img6.png?raw=true)\n\n假设词向量是512维，X矩阵的维度是(2,512)，$W^Q,W^K,W^V$均是(512,64)维，得到的Query，Keys，Values就都是(2,64)维。\n\n得到Q，K，V之后，接下来就是计算Attention值了。<br>\n**步骤1：** 输入序列中每个单词之间的相关性得分，上篇中说过计算相关性得分可以使用点积法，就是用Q中每一个向量与K中每一个向量计算点积，具体到矩阵的形式：$score=Q*K^T$,socre是一个(2,2)的矩阵.<br>\n**步骤2：** 对于输入序列中每个单词之间的相关性得分进行归一化，归一化的目的主要是为了训练时梯度能够稳定。$score=score/\\sqrt{d_k}$,$d_k$就是K的维度，以上面假设为例，$d_k=64$。<br>\n**步骤3：** 通过softmax函数，将每个单词之间的得分向量转换成[0,1]之间的概率分布，同时更加凸显单词之间的关系。经过softmax后，score转换成一个值分布在[0,1]之间的(2,2)α概率分布矩阵。<br>\n**步骤4：** 根据每个单词之间的概率分布，然后乘上对应的Values值，α与V进行点积，$Z=softmax(score)*V$，V的为维度是(2,64)，(2,2)x(2,64)最后得到的Z是(2,64)维的矩阵。\n\n然后进行了LayerNormalize，就是同一个样本而不是一个batch中进行的归一化。然后进行Feed-Forward Network：\n\n$FFN(x)=max(0,xW_1+b_1)W_2+b_2$\n\n这里的全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。\n\n\n### 1.3 Transformer的Decoder\n\n与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。为什么需要添加这两种mask码呢？\n\n1.padding mask\n什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！\n\n2.sequence mask\nsequence mask 是为了使得 decoder 不能看见未来的信息。对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。这在训练的时候有效，因为训练的时候每次我们是将target数据完整输入进decoder中地，预测时不需要，预测的时候我们只能得到前一时刻预测出的输出。\n那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n\n上面可能忘记说了，在Encoder中的Multi-Head Attention也是需要进行mask地，只不过Encoder中只需要padding mask即可，而Decoder中需要padding mask和sequence mask。OK除了这点mask不一样以外，其他的部分均与Encoder一样啦~\n\nAdd＆Normalize也与Encoder中一样，接下来就到了Decoder中第二个Multi-Head Attention，这个Multi-Head Attention又与Encoder中有一点点不一样。Encoder中的Multi-Head Attention是基于Self-Attention地，Decoder中的第二个Multi-Head Attention就只是基于Attention，它的输入Quer来自于Masked Multi-Head Attention的输出，Keys和Values来自于Encoder中最后一层的输出。\n\n补充（MSA代码实现）：\n\n    # 用于深度拷贝的copy工具包\n    import copy\n    # 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.\n    # 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.\n    def clones(module, N):\n        \"\"\"用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量\"\"\"\n        # 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,\n        # 然后将其放在nn.ModuleList类型的列表中存放.\n        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n    # 我们使用一个类来实现多头注意力机制的处理\n    class MultiHeadedAttention(nn.Module):\n        def __init__(self, head, embedding_dim, dropout=0.1):\n            \"\"\"在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度， \n               dropout代表进行dropout操作时置0比率，默认是0.1.\"\"\"\n            super(MultiHeadedAttention, self).__init__()\n\n            # 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，\n            # 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.\n            assert embedding_dim % head == 0\n\n            # 得到每个头获得的分割词向量维度d_k\n            self.d_k = embedding_dim // head\n\n            # 传入头数h\n            self.head = head\n\n            # 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，\n            # 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.\n            self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n\n            # self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.\n            self.attn = None\n\n            # 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.\n            self.dropout = nn.Dropout(p=dropout)\n\n            def forward(self, query, key, value, mask=None):\n                \"\"\"前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，\n                   最后一个是注意力机制中可能需要的mask掩码张量，默认是None. \"\"\"\n\n            # 如果存在掩码张量mask\n            if mask is not None:\n                # 使用unsqueeze拓展维度\n                mask = mask.unsqueeze(0)\n\n            # 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.\n            batch_size = query.size(0)\n\n            # 之后就进入多头处理环节\n            # 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，\n            # 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，\n            # 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，\n            # 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，\n            # 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，\n            # 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.\n            query, key, value = \\\n               [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n                for model, x in zip(self.linears, (query, key, value))]\n\n            # 得到每个头的输入后，接下来就是将他们传入到attention中，\n            # 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.\n            x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n\n            # 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，\n            # 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，\n            # 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，\n            # 所以，下一步就是使用view重塑形状，变成和输入形状相同.\n            x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)\n\n            # 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.\n            return self.linears[-1](x)\n\n### 1.4 Transformer的输出\n\nOutput如图中所示，首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2023-12-04T07:10:54.575Z","updatedAt":"2023-12-06T03:13:17.365Z","trashed":false,"_rev":"4K2UC439w"}