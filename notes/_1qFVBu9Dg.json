{"_id":"note:_1qFVBu9Dg","title":"transformer","content":"# transformer模型架构\n\n## 1.OverView\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/model.png?raw=true)\n\n### 1.1 input Embedding\n\n首先，对于每个单词进行编码，有word2vec，GloVe，one-hot编码等,dmodel=512表示词向量的维度，然后进行position-encoding（可以通过学习得到，但是transformer中是固定的），然后相加，但是这里没有给出position-encoding公式推导，估计是公式有点问题，所以这里就先不记录了。\n\n#### 1.Word2vec\n\n基本思想：通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。\n\n通常情况下，我们可以维护一个查询表。表中每一行都存储了一个特定词语的向量值，每一列的第一个元素都代表着这个词本身，以便于我们进行词和向量的映射（如“我”对应的向量值为 [0.3，0.5，0.7，0.9，-0.2，0.03] ）。给定任何一个或者一组单词，我们都可以通过查询这个excel，实现把单词转换为向量的目的，这个查询和替换过程称之为Embedding Lookup。\n\n![img1](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img1.png?raw=true)\n\n\n然而在进行神经网络计算的过程中，需要大量的算力，常常要借助特定硬件（如GPU）满足训练速度的需求。GPU上所支持的计算都是以张量（Tensor）为单位展开的，因此在实际场景中，我们需要把Embedding Lookup的过程转换为张量计算：\n\n![img2](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img2.png?raw=true)\n\n**如何让向量具有语义信息？**\n\n在自然语言处理研究中，科研人员通常有一个共识：使用一个单词的上下文来了解这个单词的语义，比如：\n\n    “苹果手机质量不错，就是价格有点贵。”\n    “这个苹果很好吃，非常脆。”\n    “菠萝质量也还行，但是不如苹果支持的APP多。”\n\n在上面的句子中，我们通过上下文可以推断出第一个“苹果”指的是苹果手机，第二个“苹果”指的是水果苹果，而第三个“菠萝”指的应该也是一个手机。在自然语言处理领域，使用上下文描述一个词语或者元素的语义是一个常见且有效的做法。我们可以使用同样的方式训练词向量，让这些词向量具备表示语义信息的能力。\n\n2013年，Mikolov提出的经典word2vec算法就是通过上下文来学习语义信息。word2vec包含两个经典模型：CBOW（Continuous Bag-of-Words）和Skip-gram。\n\n    CBOW：通过上下文的词向量推理中心词。\n    Skip-gram：根据中心词推理上下文。\n\n![img3](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img3.png?raw=true)\n\n**CBOW模型**\n\n![img4](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img4.png?raw=true)\n\n__输入层__： 一个形状为C×V的one-hot张量，其中C代表上下文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示。\n__隐藏层__： 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。\n__输出层__： 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：\n\n**Skip-gram模型**\n\n![img5](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img5.png?raw=true)\n\n**Input Layer（输入层）**： 接收一个one-hot张量$V∈R^{1×vocab\\_size}$作为网络的输入，假设vocab_size为5000。<br>\n**Hidden Layer（隐藏层）**： 将张量V乘以一个word embedding张量$W1∈R^{vocab\\_size×embed\\_size}$，假设embed_size为128，并把结果作为隐藏层的输出，得到一个形状为$R^{1×embed\\_size}$的张量，里面存储着当前句子中心词的词向量。<br>\n**Output Layer（输出层）**： 将隐藏层的结果乘以另一个word embedding张量$W2∈R^{embed\\_size×vocab\\_size}$，得到一个形状为$R^{1×vocab\\_size}$的张量。这个张量经过softmax变换后，就得到了使用当前中心词对上下文的预测结果。根据这个softmax的结果，我们就可以去训练词向量模型。\n\nSkip-gram在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。\n\n### 1.2 Transformer的Encoder\n\n![img6](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img6.png?raw=true)\n\n假设词向量是512维，X矩阵的维度是(2,512)，$W^Q,W^K,W^V$均是(512,64)维，得到的Query，Keys，Values就都是(2,64)维。\n\n得到Q，K，V之后，接下来就是计算Attention值了。<br>\n**步骤1：** 输入序列中每个单词之间的相关性得分，上篇中说过计算相关性得分可以使用点积法，就是用Q中每一个向量与K中每一个向量计算点积，具体到矩阵的形式：$score=Q*K^T$,socre是一个(2,2)的矩阵.<br>\n**步骤2：** 对于输入序列中每个单词之间的相关性得分进行归一化，归一化的目的主要是为了训练时梯度能够稳定。$score=score/\\sqrt{d_k}$,$d_k$就是K的维度，以上面假设为例，$d_k=64$。<br>\n**步骤3：** 通过softmax函数，将每个单词之间的得分向量转换成[0,1]之间的概率分布，同时更加凸显单词之间的关系。经过softmax后，score转换成一个值分布在[0,1]之间的(2,2)α概率分布矩阵。<br>\n**步骤4：** 根据每个单词之间的概率分布，然后乘上对应的Values值，α与V进行点积，$Z=softmax(score)*V$，V的为维度是(2,64)，(2,2)x(2,64)最后得到的Z是(2,64)维的矩阵。\n\n然后进行了LayerNormalize，就是同一个样本而不是一个batch中进行的归一化。然后进行Feed-Forward Network：\n\n$FFN(x)=max(0,xW_1+b_1)W_2+b_2$\n\n这里的全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。\n\n\n### 1.3 Transformer的Decoder\n\n与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。为什么需要添加这两种mask码呢？\n\n1.padding mask\n什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！\n\n2.sequence mask\nsequence mask 是为了使得 decoder 不能看见未来的信息。对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。这在训练的时候有效，因为训练的时候每次我们是将target数据完整输入进decoder中地，预测时不需要，预测的时候我们只能得到前一时刻预测出的输出。\n那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n\n上面可能忘记说了，在Encoder中的Multi-Head Attention也是需要进行mask地，只不过Encoder中只需要padding mask即可，而Decoder中需要padding mask和sequence mask。OK除了这点mask不一样以外，其他的部分均与Encoder一样啦~\n\nAdd＆Normalize也与Encoder中一样，接下来就到了Decoder中第二个Multi-Head Attention，这个Multi-Head Attention又与Encoder中有一点点不一样。Encoder中的Multi-Head Attention是基于Self-Attention地，Decoder中的第二个Multi-Head Attention就只是基于Attention，它的输入Quer来自于Masked Multi-Head Attention的输出，Keys和Values来自于Encoder中最后一层的输出。\n\n### 1.4 Transformer的输出\n\nOutput如图中所示，首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2023-12-04T07:10:54.575Z","updatedAt":"2023-12-04T12:45:24.258Z","trashed":false,"_rev":"T5bXkgrpz"}