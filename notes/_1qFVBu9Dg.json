{"_id":"note:_1qFVBu9Dg","title":"transformer","content":"# transformer模型架构\n\n## 1.OverView\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/model.png?raw=true)\n\n### 1.1 input Embedding\n\n首先，对于每个单词进行编码，有word2vec，GloVe，one-hot编码等\n\n#### 1.Word2vec\n\n基本思想：通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。\n\n通常情况下，我们可以维护一个查询表。表中每一行都存储了一个特定词语的向量值，每一列的第一个元素都代表着这个词本身，以便于我们进行词和向量的映射（如“我”对应的向量值为 [0.3，0.5，0.7，0.9，-0.2，0.03] ）。给定任何一个或者一组单词，我们都可以通过查询这个excel，实现把单词转换为向量的目的，这个查询和替换过程称之为Embedding Lookup。\n\n![img1]()\n\n\n然而在进行神经网络计算的过程中，需要大量的算力，常常要借助特定硬件（如GPU）满足训练速度的需求。GPU上所支持的计算都是以张量（Tensor）为单位展开的，因此在实际场景中，我们需要把Embedding Lookup的过程转换为张量计算：\n\n![img2]()","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2023-12-04T07:10:54.575Z","updatedAt":"2023-12-04T07:44:24.543Z","trashed":false,"_rev":"PaPZII4al"}