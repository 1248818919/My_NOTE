{"_id":"note:_1qFVBu9Dg","title":"transformer","content":"# transformer模型架构\n\n## 1.OverView\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/model.png?raw=true)\n\n### 1.1 input Embedding\n\n首先，对于每个单词进行编码，有word2vec，GloVe，one-hot编码等,dmodel=512表示词向量的维度，然后进行position-encoding（可以通过学习得到，但是transformer中是固定的），然后相加，但是这里没有给出position-encoding公式推导，估计是公式有点问题，所以这里就先不记录了。\n\n#### 1.Word2vec\n\n基本思想：通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。\n\n通常情况下，我们可以维护一个查询表。表中每一行都存储了一个特定词语的向量值，每一列的第一个元素都代表着这个词本身，以便于我们进行词和向量的映射（如“我”对应的向量值为 [0.3，0.5，0.7，0.9，-0.2，0.03] ）。给定任何一个或者一组单词，我们都可以通过查询这个excel，实现把单词转换为向量的目的，这个查询和替换过程称之为Embedding Lookup。\n\n![img1](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img1.png?raw=true)\n\n\n然而在进行神经网络计算的过程中，需要大量的算力，常常要借助特定硬件（如GPU）满足训练速度的需求。GPU上所支持的计算都是以张量（Tensor）为单位展开的，因此在实际场景中，我们需要把Embedding Lookup的过程转换为张量计算：\n\n![img2](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img2.png?raw=true)\n\n**如何让向量具有语义信息？**\n\n在自然语言处理研究中，科研人员通常有一个共识：使用一个单词的上下文来了解这个单词的语义，比如：\n\n    “苹果手机质量不错，就是价格有点贵。”\n    “这个苹果很好吃，非常脆。”\n    “菠萝质量也还行，但是不如苹果支持的APP多。”\n\n在上面的句子中，我们通过上下文可以推断出第一个“苹果”指的是苹果手机，第二个“苹果”指的是水果苹果，而第三个“菠萝”指的应该也是一个手机。在自然语言处理领域，使用上下文描述一个词语或者元素的语义是一个常见且有效的做法。我们可以使用同样的方式训练词向量，让这些词向量具备表示语义信息的能力。\n\n2013年，Mikolov提出的经典word2vec算法就是通过上下文来学习语义信息。word2vec包含两个经典模型：CBOW（Continuous Bag-of-Words）和Skip-gram。\n\n    CBOW：通过上下文的词向量推理中心词。\n    Skip-gram：根据中心词推理上下文。\n\n![img3](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img3.png?raw=true)\n\n**CBOW模型**\n\n![img4](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img4.png?raw=true)\n\n__输入层__： 一个形状为C×V的one-hot张量，其中C代表上下文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示。\n__隐藏层__： 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。\n__输出层__： 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：\n\n**Skip-gram模型**\n\n![img5](https://github.com/1248818919/My_NOTE/blob/master/assets/transformer/img5.png?raw=true)\n\n**Input Layer（输入层）**： 接收一个one-hot张量$V∈R^{1×vocab\\_size}$作为网络的输入，假设vocab_size为5000。<br>\n**Hidden Layer（隐藏层）**： 将张量V乘以一个word embedding张量$W1∈R^{vocab\\_size×embed\\_size}$，假设embed_size为128，并把结果作为隐藏层的输出，得到一个形状为$R^{1×embed\\_size}$的张量，里面存储着当前句子中心词的词向量。<br>\n**Output Layer（输出层）**： 将隐藏层的结果乘以另一个word embedding张量$W2∈R^{embed\\_size×vocab\\_size}$，得到一个形状为$R^{1×vocab\\_size}$的张量。这个张量经过softmax变换后，就得到了使用当前中心词对上下文的预测结果。根据这个softmax的结果，我们就可以去训练词向量模型。\n\nSkip-gram在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。\n\n### 1.2 Transformer的Encoder\n","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2023-12-04T07:10:54.575Z","updatedAt":"2023-12-04T12:05:52.794Z","trashed":false,"_rev":"vf0VqojSz"}