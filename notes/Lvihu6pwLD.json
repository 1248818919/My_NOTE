{"_id":"note:Lvihu6pwLD","title":"TransGeo","content":"# TransGeo论文笔记\n\n> Zhu S, Shah M, Chen C. Transgeo: Transformer is all you need for cross-view image geo-localization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 1162-1171.\n\n## 1.Overview\n\n![model.jpg](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Geo-localization/TransGeo/model.jpg?raw=true)\n\n本文的创新点：\n- 第一种纯基于变换器的跨视图图像地理定位方法（TransGeo），不依赖于极坐标变换或数据增强。\n- 一种新颖的注意力引导的非均匀裁剪策略，该策略删除了参考航空图像中的大量无信息补丁，以减少计算，性能下降可忽略不计。通过将保存的计算重新分配到信息区域的更高图像分辨率，进一步提高了性能。\n- 与基于CNN的方法相比，在城市和农村数据集上的最先进性能具有更低的计算成本、GPU内存消耗和推理时间。\n\n## 2. Related Work\n\n该研究注重于航拍图像的检索工作，即给定一张街景图，要求能够定位到他的航拍图；较早的办法是使用双流的CNN来进行特征提取，再在embedding层学习他们的特征，但这种方法无法对两个视图之间的显著外观差距进行建模，导致效果较差。最近的方法要么利用极坐标变换，要么添加额外的生成模型(GAN)，通过将图像从一个视图转换到另一个视图来减少域间隙。除此之外，上述的研究大多数是针对街景图和航拍图两者中心是对齐的，有些研究也针对两者中心不对齐的情况。\n\n## 3. Method\n\n损失函数定义：$\\mathcal{L}_{\\text {triplet }}=\\log \\left(1+e^{\\alpha\\left(d_{\\text {pos }}-d_{\\text {neg }}\\right)}\\right)$\n\nHere $d_{\\text {pos }}$ and $d_{\\text {neg }}$ denote the squared $l_2$ distance of the positive and negative pairs.\n\n模型的具体流程是：首先将两个视角的图像送到两个Encoder中进行编码，然后将鸟瞰图的图像进行crop和放缩，再送到Encoder中进行编码。\n\n裁剪的时候保留$\\beta$（e.g.64%）倍的patches，放大$\\gamma$倍。\n\n为了不使用数据增强，作者采用了正则/泛化技巧ASAM。\n\n>Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.arXiv preprint arXiv:2102.11600, 2021.\n\n对于给定的损失函数$\\mathcal{L}$和参数权重$\\mathcal{w}\\in \\mathbb{R}^k$,锐度损失定义为：\n$$\n\\max _{|\\epsilon|_2<\\rho} \\mathcal{L}(w+\\epsilon)-\\mathcal{L}(w)\n$$","tags":[],"folderPathname":"/Computer Vision/Geo-localization","data":{},"createdAt":"2023-12-19T11:22:50.538Z","updatedAt":"2023-12-20T03:20:58.021Z","trashed":false,"_rev":"UJUhfKy-t"}