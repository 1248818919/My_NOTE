{"_id":"note:ri3D3Q65TL","title":"Cascade-CLIP","content":"# Cascade-CLIP\n\n> Li Y, Li Z Y, Zeng Q, et al. Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation[J]. arxiv preprint arxiv:2406.00670, 2024.\n\n## 1.Introduction\n\n针对传统的将多个不同的尺度的特征图进行融合会导致zero-shot能力变差的问题，本文提出了一种新的多尺度特征图聚合方式，以实现语义分割的任务。\n\n> 这里进行评注一下，Zegclip:Towards adapting clip for zero-shot semantic segmentation.是一篇单阶段的语义分割论文，值得进行查阅。\n\n论文写法学习\n> 本文首先介绍了什么是语义分割（一句话），三句话介绍语义分割中zero-shot的问题，三句话介绍了CLIP模型对于zero-shot提供了新的解决方案，然后介绍双阶段，单阶段的基于CLIP模型的语义分割模型的优缺点，然后一段话总结自己本文的工作\n\n## 2.Analysis\n\n简单地聚合多层次视觉特征会降低分割性能，因此本文提出了一种新的聚合方式。首先，作者介绍说浅层特征图与深层特征图两者相似性很低，不应该直接进行融合，如图1所示。\n\n","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Seg","data":{},"createdAt":"2024-06-08T07:39:42.964Z","updatedAt":"2024-06-08T09:17:39.935Z","trashed":false,"_rev":"T4QaZEiSz"}