{"_id":"note:a4AG4lC6QR","title":"DIVOTrack","content":"# DIVOTrack论文笔记\n\n《DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes》\n\n## 1.OverView\n\n跨视图多目标跟踪旨在将帧和具有大量重叠的相机视图之间的对象连接起来。尽管近年来交叉视角多目标跟踪受到越来越多的关注，但现有数据集仍然存在一些问题，包括1)缺少真实场景，2)缺乏多样化的场景，3)包含有限数量的轨迹，4)仅包含静态摄像机，5)缺乏标准基准，这阻碍了交叉视角跟踪方法的研究和比较。为了解决上述问题，我们引入了DIVOTrack:一个新的跨视图多目标跟踪数据集，用于现实和非实验环境中具有密集跟踪行人的不同开放场景。我们的DIVOTrack有15个不同的场景和953个交叉视图轨道，超过了目前可用的所有交叉视图多目标跟踪数据集。此外，我们提出了一种新的基线交叉视图跟踪方法，该方法具有统一的联合检测和交叉视图跟踪框架CrossMOT，该方法通过一体化嵌入模型学习目标检测、单视图关联和交叉视图匹配。最后，我们通过我们的DIVOTrack对当前方法和一套标准基准进行了总结，以提供公平的比较，并对当前方法和我们提出的crosssmot进行全面分析。\n\n## 2.Data Collection\n\n收集了十五个地区的视频，分别是Circle, Shop, Moving, Park, Ground, Gate1, Floor, Side, Square, Gate2, Indoor1, Indoor2, Outdoor1, Outdoor2 and Park2， view1视角是$1920×1080$的，拍摄于$45°$角，view2是$3640×2048$，view3是$1920×1080$；其中Indoor1, Indoor2, Outdoor1, Outdoor2 and Park2在作者口中是困难的数据集，同时作者也对其进行了对齐并进行了降采样变成了30帧。\n\n标注介绍：标注分成了三个部分，分别是轨迹初始化、单视图校正和交叉视图匹配；使用了单视角的CenterTrack来初始化目标框和跟踪器，为了节省时间，作者手动更正了目标框和IDs；之后，在交叉视图匹配过程中，作者给同一个目标分配了相同的全局ID\n他的模型就是把各种东西都柔和在一起，然后针对ID冲突的问题，对不同分支分别设计了一个函数进行解决。\n\n![data](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/data.jpg?raw=true)\n\n## 3.Model\n\n首先展示一下模型图\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/model.jpg?raw=true)\n\n**1 Object Dection Embedding** 采用了CenterNet进行目标检测。\n\n**2.Cross-view Re-ID Embedding** 提取目标特征，采用交叉熵计算损失，使用的是独热编码对ID进行编码，使用的方式是MvMHAT方法，并不需要参数\n\n**3.single-view** 作者原本想用贡献embedding来表示人物，但发现效果不好，可能的原因是每一个embedding他们关注的内容不同，所以作者对其进行了解耦操作，变成了两个embedding。\n\n![single_view_loss](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/single_view_loss.jpg?raw=true)\n\n这个公式也非常好理解，就是只计算单个视图下的经过softmax cross-entropy loss。然后最终的公式如下：\n\n![final_loss](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/final_loss.jpg?raw=true)\n\n## 4.Result\n\n![single_view_res](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/single_view_result.jpg?raw=true)\n\n![cross_view_res1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/cross_view_result.jpg?raw=true)\n\n![cross_view_res2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/cross_view_result2.jpg?raw=true)\n\n## 5.Abliation \n\n![abliation1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/abliation1.jpg?raw=true)\n\n![abliation2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/ablation2.jpg?raw=true)\n\n![abliation3](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/DIVOTrack/abliation3.jpg?raw=true)\n\n## 6.CODE\n\n### 6.1 Track的逻辑\n\n1. main(opt,data_root,seqs,exp_name)\n    1. run_tracking\n        1. dataloader  \n        2. gather_seq_info_multi_view(opt, dataloader, seq, seq_length, use_cuda=True)\n            1.创建模型\n            2.加载模型参数 ","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2023-12-14T06:42:20.107Z","updatedAt":"2024-01-19T08:07:39.952Z","trashed":false,"_rev":"tg96jjt0J"}