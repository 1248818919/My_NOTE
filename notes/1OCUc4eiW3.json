{"_id":"note:1OCUc4eiW3","title":"Train a mdel","content":"## 1.添加依赖\n\n    import os\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    from torchvision import transforms\n    from torchvision.datasets import MNIST\n    from torch.utils.data import DataLoader\n    import lightning as L\n\n## 2.定义 PyTorch nn.Modules\n\n    class Encoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n\n        def forward(self, x):\n            return self.l1(x)\n\n\n    class Decoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n        def forward(self, x):\n            return self.l1(x)\n            \n## 3.定义 LightningModule\n\n这个LightningModule是一个用于指导你模型如何训练的模块\n\n- training_step 定义 nn.Modules 如何交互\n- configure_optimizers 定义模型的优化器\n\n\n    class LitAutoEncoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            super().__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n\n        def training_step(self, batch, batch_idx):\n            # training_step defines the train loop.\n            x, y = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            loss = F.mse_loss(x_hat, x)\n            return loss\n\n        def configure_optimizers(self):\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n            return optimizer\n        \n## 4.定义training dataset\n\n    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n    train_loader = DataLoader(dataset)\n\n## 5.训练模型\n\n使用Trainer训练模型，其可以处理所有的工程，摒弃其余复杂不重要的东西\n\n    # model\n    autoencoder = LitAutoEncoder(Encoder(), Decoder())\n\n    # train model\n    trainer = L.Trainer()\n    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\n上面的代码相当于\n\n    autoencoder = LitAutoEncoder(Encoder(), Decoder())\n    optimizer = autoencoder.configure_optimizers()\n\n    for batch_idx, batch in enumerate(train_loader):\n        loss = autoencoder.training_step(batch, batch_idx)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()","tags":[],"folderPathname":"/Tutorial/Pytorch_Lightning/Basic","data":{},"createdAt":"2024-02-22T05:16:07.178Z","updatedAt":"2024-02-22T05:23:38.323Z","trashed":false,"_rev":"1jzaxihRl"}