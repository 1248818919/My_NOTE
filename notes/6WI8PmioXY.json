{"_id":"note:6WI8PmioXY","title":"MvMHAT","content":"# MvMHAT论文笔记\n\n## 1.OverView\n\n在本文中，作者研究了一个相对较新的问题MvMHAT(Multi-view Multi-human association and tracking)，它不同于现有的MOT问题，有很多的应用。为了充分挖掘MvMHAT的特性，我们将该问题建模为一个自监督学习任务，并提出了一个端到端的框架，这个是目前唯一的一篇有交叉视角的模型。为了推进这一新课题的研究，我们还构建了一个新的MvMHAT基准来进行性能评估。\n\n![model_overView](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MvMHAT/model.jpg?raw=true)\n\n## 2.Related Work\n\n作者在这部分分成三个章节来进行撰写，分别是MOT，MTMCT，MvMHAT。\n\nMOT：作者建议阅读两个文献分别是\n>Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano,Roberto\nTagliaferri, and Francisco Herrera. 2020. Deep learning in video multi-object tracking: A survey. Neurocomputing 381 (2020), 61–88.\n\n>Arnold WM Smeulders, Dung M Chu, Rita Cucchiara, Simone Calderara, Afshin Dehghan, and Mubarak Shah. 2013. Visual tracking: An experimental survey.IEEE TPAMI 36, 7 (2013), 1442–1468.\n\nMTMCT(multi-target multi-camera tracking):旨在跟踪和重新识别大范围内的目标(主要是人类)，例如，校园，使用安装在许多地点的许多摄像机，很少或没有视野重叠。\n\nMVMHAT:与MTMCT的区别是1)它们对问题的定义不同。除了时间跟踪，MTMCT还旨在处理人的再识别，这是一个排序问题。不同的是，MvMHAT关注的是多人匹配，这是一个分类问题。2)他们使用不同的相机设置。MTMCT使用分布在大面积不同地点的多个摄像机，没有视场(FOV)重叠。不同的是，MvMHAT使用重叠FOV覆盖同一场景的多视图全能相机。\n\n## 3.Approach\n\n作者的想法是detection获得目标框$B^{v}_{t}$----->利用ResNet等网络提取目标框的相关信息,得到$E^{v}_{t} \\in R^{N^{v}_{t} \\times D}$,其中$N^{v}_{t}$表示视角$v$第$t$帧中所有目标,$D$表示骨干网络fc后的维度。----->Spatial-Temporal Association Network来进行关联，这一块在下一块将详细介绍。值得一提的是他们没有使用annotation相关信息，是一个自监督学习。\n\nSpatial association:关联空间视角下的方法,就是在视角v下的特征向量与视角u下特征向量相乘，具体公式为\n\n$S^{v,u}_{t}=E^{v}_{t} \\times (E^{u}_{t})^{T} \\in R^{N^{v}_{t} \\times N^{u}_{t}}$ cosine metric计算相似性\n\nTemporal association：和上面类似，具体公式为\n\n$S^{v}_{t,s}=E^{v}_{t} \\times (E^{v}_{s})^{T} \\in R^{N^{v}_{t} \\times N^{v}_{s}}$\n\n最后利用temperature-adaptive softmax计算概率值\n\n$X(r,c)=f_{r,c}(S)=\\frac{exp(\\tau S(r,c))}{\\sum^{C}_{c'=1}exp(\\tau S(r,c))}$,\n$\\tau = \\frac{1}{\\epsilon}log[\\frac{\\delta (C-1)+1}{1- \\delta}]$\n\n损失函数，这里就不进行推导了，大概讲一下思路，首先作者想出来了自相似性和传递性，即i-->j-->i=identity matrix以及i-->j-->k-->j-->i=identity matrix这两个性质，所以作者设计如下的损失函数\n\n$I_{S}=X_{ij} \\times X_{ji} \\in R^{I \\times I}$\n\n$I_{T}=X_{ik} \\times X_{ki} \\in R^{I \\times I}$\n\n$L(I)=\\sum^{|T|}_{r=1}relu(max_{c \\ne r}I(r,c)- I(r,r) + m)$\n\n$L_{SSIM}=L(I_{S})+L(I_{S}^{T})$\n\n$L_{TSIM}=L(I_{T})+L(I_{T}^{T})$\n\n伪代码展示如下\n\n![fake_code](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MvMHAT/fake_code.jpg?raw=true)\n\n## 4.benchmark\n\nFP false positive 指的是在第t帧中，跟踪器检测到了的bounding box但是在ground truth中却不存在bounding box的个数。\n\nFN false negative 指的是在第t帧中，跟踪器漏检了的bounding box但是在ground truth中存在bounding box的个数。\n\nTP true positive 指的是在第t帧中，跟踪器和ground truth同时都有的bounding box。\n\nIDP：对于某个ID对应的轨迹，预测轨迹上正确的ID值占该预测轨迹上帧数的数量和。\n\nIDR：对于某个ID对应的轨迹，预测轨迹上正确的ID值占该ground truth轨迹上帧数的总和。\n\n1.$AIDP$:代表多视角目标关联的准确性.\n\n2.$AIDR$：代表多视角目标关联性的召回率。\n\n以上两个的计算方式是给定所有视图中的主题id，我们每次取两个视图并计算成对主题匹配性能，其在所有视图对上的平均值用作多视图关联度量。对于AIDP指的是任意两个view中预测正确的目标数/所有预测的目标数，对于AIDR指的是任意两个view中预测正确的目标数/所有gt的目标数。\n\n3.$ADF_{1}$：$AIDF_{1}=\\frac{2 \\times AIDP \\times AIDR}{AIDP + AIDR}$代表多视角目标关联性的F1值，\n\n4.$MHAA$：$1-\\frac{\\sum (MS_{t}+FP_{t}+2MM_{t})}{\\sum_{t}N_{t}}$, where $MS_{t}$ , $FP_{t}$, $MM_{t}$ are the\nfalse negative, false positives, and mismatched pairs, respectively,\n\n5.$MHAT.F_{1}$:$mean(IDF_{1},AIDF_{1})$\n\n6.$MHAT.Acc$:$mean(MOTA,MHAA)$\n\n","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2023-11-27T13:25:39.091Z","updatedAt":"2023-12-06T06:21:45.979Z","trashed":false,"_rev":"OszkvEdRY"}