{"_id":"note:YW97B75sTY","title":"Seqtrack","content":"# Seqtrack论文笔记\n\n## 1.OverView\n\n这篇论文继承了Ostrack的思想，采用了one-stream one-stage的方法将track问题转换成seq2seq问题，提出了新的一种seq2seq的方法。注意，这里应该是单目标检测，而不是多目标检测。\n\n![model](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/pipeline.gif?raw=true)\n\n本文的贡献如下：\n\n1.提出了一种用于视觉跟踪的序列到序列学习方法。它将跟踪转换为生成任务，这为跟踪建模提供了一个新的视角。\n\n2.提出了一种新的序列跟踪模型，它在速度和精度之间取得了很好的平衡。实验验证了新模型的有效性。\n\n## 2. Related Work\n\n**Visual Tracking**\n现有的跟踪方法通常采用分而治之的策略，将跟踪分解为多个子任务。他们首先使用深度神经网络提取视频帧的视觉特征，然后设计多个特定任务的头部网络来预测目标物体的边界框。根据头部网络的不同，先验跟踪器可以分为两类:1)基于分类和回归的跟踪器和2)基于角点预测的跟踪器（STARK方法）。\n\n**Sequence Learning**\n序列到序列学习最初被提出用于自然语言建模，最近应用于计算机视觉。Pix2Seq是一个代表性的工作，它将对象检测作为一个令牌生成任务，以观察到的像素输入为条件。除了目标检测之外，这种序列学习方法也被成功地扩展到其他视觉任务中，如实例分割和关键点检测。此外，在跨模态领域，序列学习也越来越受欢迎。例如，DALL-E等文本到图像生成模型和Flamingo等视觉语言模型都采用序列到序列学习来统一多模态预训练。\n\n## 3.Method\n\n对于目标框，作者将目标框的表示采用了xywh型，意思是说先确定目标框的中心点，在确定目标框的scale，同时作者还加了两个token，分别是start和end，其中在训练的时候，start和end分别都有用到，取值分别是4001和4002，但是在推理的时候只用到了start没有用到end，这里取4001和4002是因为作者将离散的坐标点转换成整数空间中，取值是$[1,n_{bins}]$,这里的bins实验中使用的是4000，然后再采用了word_embedding。\n\n\n对于图像，作者是采用了VIT的类似处理方式，首先将图像分成了template和search两种图像，两个图像输入模型时候的size大小相同，其中template代表的是我们要配对的图像，search代表搜索的区域。然后就是将图像先进行卷积（16×16）得到14×14×768大小的图像，然后进行patch，得到了256×768的张量，再添加position_embedding就完成了。\n\n**Encoder** \n相对于原来的encoder来说，作者移除了CLASS token以及添加了一个线性层来对其特征大小。\n\n**Decoder**\n有一个Mask multi-head attention，multi-head attention和FFN结构。具体的，这个mask作用是保证坐标是一个个生成的，就如上面的动态图所示，每次只能看到这个位置之前的相关信息，而不能往后看。\n\n**Training and Inference** 在训练的时候，作者采用了交叉熵损失。\n![img1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/entroy1.jpg?raw=true)\n![img2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/entroy2.jpg?raw=true)\n![img3](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/entroy3.jpg?raw=true)\n![img4](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/entroy4.jpg?raw=true)\n![img5](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/SeqTrack/entroy5.jpg?raw=true)\n\n在推断期间，编码器感知后续视频帧中的模板图像和搜索区域。解码器的初始输入是开始令牌，它告诉模型开始生成。然后，模型逐个标记地“读出”目标序列$[x，y，w，h，end]$。对于每个令牌，模型根据最大似然性从词汇表V中对其进行采样。此外，我们还引入了在线模板更新和窗口惩罚，在推理过程中集成先验知识，进一步提高了模型的准确性和鲁棒性。\n\n对于Online Update和Window Penalty，我还没看，等后面看完后再更新。\n\n## 4.Experiment\n\n","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2023-12-06T06:11:20.760Z","updatedAt":"2023-12-06T11:37:35.407Z","trashed":false,"_rev":"9PrUPy1o3"}