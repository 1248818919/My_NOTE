{"_id":"note:KlJWWXLieB","title":"BERT","content":"# BERT论文笔记\n\n> Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n\n## 1.Overview\n\nBERT是一个模型，全名成为Bidirectional Encoder Representations from Transformers，类似于CV中的骨干网络，我们只需要添加一个output layer就可以对他进行微调从而实现各种任务。\n\n本文的贡献是：\n\n - 证明了双向预训练对语言表征的重要性。与Radford等人(2018)使用单向语言模型进行预训练不同，BERT使用屏蔽语言模型来实现预训练的深度双向表示。这也与Peters等人(2018a)形成对比，后者使用独立训练的从左到右和从右到左的LMs的浅连接。\n\n - 表明，pre-trained representations减少了对许多高度工程化的任务特定架构的需求。BERT是第一个基于fine-\ntuning的表示模型，它在大量句子级和记号级任务上实现了最先进的性能，优于许多特定于任务的架构。\n\n## 2.Introduce and Related Work\n\n有两种现有的策略可以将预训练的语言表示应用于下游任务：一个feature-based，一个是fine-tuning。前者使用使用特定于任务的架构，其中包括pre-trained representations作为额外的特征，后者是针对特定问题优化参数，并通过简单地微调所有预训练参数来对下游任务进行训练。\n\n## 3.BERT\n\n使用BERT有两个步骤，第一步是pre-train，第二步是fine-tunning。\n\n具体的，BERT基础模型的架构，由12个Transformer堆叠而成，隐藏层是768，自注意力头是12个，参数大概是110M；大模型是12个transformer，隐藏层是1024，自注意力头是16个，参数大概是340M。\n\n备注：bidirectional transformer指的是transformer encoder，left-context-only指的是transformer decoder。\n\n作者使用了WordPiece embeddings使用30000 token vocabulary，第一个token永远是cls，输入的seq可能是一个句子也可能是一对句子（<Question，Answer>），如果输入是一对句子，这用SEP token将他们分开，并且添加learning embedding来表示这是句子A还是句子B\n\n### 3.1 Pre-training BERT\n\n- **Masked LM** 针对单词\n\n为了训练deep bidirectional representation，我们只是随机屏蔽一定比例的输入令牌，然后预测这些被屏蔽的令牌。我们将这一过程称为“masked LM”(MLM)，尽管在文献中它通常被称为完形填空任务(Taylor, 1953)。在这种情况下，与掩码令牌相对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。在我们所有的实验中，我们随机屏蔽了每个序列中15%的WordPiece令牌。与去噪自编码器(Vincent et al .， 2008)相比，我们只预测被屏蔽的单词，而不是重建整个输入。\n\n虽然这允许我们获得deep bidirectional representation，但缺点是我们正在创建预训练和微调之间的不匹配，因为[MASK]令牌在微调期间没有出现。为了减轻这种情况，我们并不总是用实际的[MASK]令牌替换“被屏蔽”的单词。训练数据生成器随机选择15%的标记位置进行预测。如果选择了第i个令牌，我们用(1)80%的时间使用[MASK]令牌(2)10%的时间使用随机令牌(3)10%的时间使用未更改的第i个令牌替换第i个令牌。然后，利用$T_i$来预测具有交叉熵损失的原始token。我们在附录C.2中比较了这个过程的变化。\n\n- **Next Sentence Prediction (NSP)**\n\n许多重要的下游任务，如问答(QA)和自然语言推理(NLI)，都是基于理解两个句子之间的关系，这是语言建模无法直接捕获的。为了训练一个理解句子关系的模型，我们对一个二值化的下一个句子预测任务进行了预训练，该任务可以从任何单语语料库中轻松生成。具体来说，当为每个预训练示例选择句子A和B时，50%的时间B是A之后的下一个句子(标记为IsNext)， 50%的时间B是语料库中的随机句子(标记为NotNext)。其中C是用来预测下一个句子的（NSP）","tags":[],"folderPathname":"/NLP","data":{},"createdAt":"2023-12-27T05:53:47.502Z","updatedAt":"2023-12-27T07:58:41.561Z","trashed":false,"_rev":"jn0RT7opJ"}