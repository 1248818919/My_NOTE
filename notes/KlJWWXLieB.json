{"_id":"note:KlJWWXLieB","title":"BERT","content":"# BERT论文笔记\n\n> Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n\n## 1.Overview\n\nBERT是一个模型，全名成为Bidirectional Encoder Representations from Transformers，类似于CV中的骨干网络，我们只需要添加一个output layer就可以对他进行微调从而实现各种任务。\n\n本文的贡献是：\n\n - 证明了双向预训练对语言表征的重要性。与Radford等人(2018)使用单向语言模型进行预训练不同，BERT使用屏蔽语言模型来实现预训练的深度双向表示。这也与Peters等人(2018a)形成对比，后者使用独立训练的从左到右和从右到左的LMs的浅连接。\n\n - 表明，pre-trained representations减少了对许多高度工程化的任务特定架构的需求。BERT是第一个基于fine-\ntuning的表示模型，它在大量句子级和记号级任务上实现了最先进的性能，优于许多特定于任务的架构。\n\n## 2.Introduce and Related Work\n\n有两种现有的策略可以将预训练的语言表示应用于下游任务：一个feature-based，一个是fine-tuning。前者使用使用特定于任务的架构，其中包括pre-trained representations作为额外的特征，后者是针对特定问题优化参数，并通过简单地微调所有预训练参数来对下游任务进行训练。\n\n## 3.BERT\n\n使用BERT有两个步骤，第一步是pre-train，第二步是fine-tunning。\n\n具体的，BERT基础模型的架构，由12个Transformer堆叠而成，隐藏层是768，自注意力头是12个，参数大概是110M；大模型是12个transformer，隐藏层是1024，自注意力头是16个，参数大概是340M。\n\n备注：bidirectional transformer指的是transformer encoder，left-context-only指的是transformer decoder。\n\n作者使用了WordPiece embeddings使用30000 token vocabulary，第一个token永远是cls，输入的seq可能是一个句子也可能是一对句子（<Question，Answer>），如果输入是一对句子，这用SEP token将他们分开，并且添加learning embedding来表示这是句子A还是句子B\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/input_embedding.jpg?raw=true)\n\n### 3.1 Pre-training BERT\n\n- **Masked LM** 针对单词\n\n为了训练deep bidirectional representation，我们只是随机屏蔽一定比例的输入令牌，然后预测这些被屏蔽的令牌。我们将这一过程称为“masked LM”(MLM)，尽管在文献中它通常被称为完形填空任务(Taylor, 1953)。在这种情况下，与掩码令牌相对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。在我们所有的实验中，我们随机屏蔽了每个序列中15%的WordPiece令牌。与去噪自编码器(Vincent et al .， 2008)相比，我们只预测被屏蔽的单词，而不是重建整个输入。\n\n虽然这允许我们获得deep bidirectional representation，但缺点是我们正在创建预训练和微调之间的不匹配，因为[MASK]令牌在微调期间没有出现。为了减轻这种情况，我们并不总是用实际的[MASK]令牌替换“被屏蔽”的单词。训练数据生成器随机选择15%的标记位置进行预测。如果选择了第i个令牌，我们用(1)80%的时间使用[MASK]令牌(2)10%的时间使用随机令牌(3)10%的时间使用未更改的第i个令牌替换第i个令牌。然后，利用$T_i$来预测具有交叉熵损失的原始token。我们在附录C.2中比较了这个过程的变化。\n\n- **Next Sentence Prediction (NSP)**\n\n许多重要的下游任务，如问答(QA)和自然语言推理(NLI)，都是基于理解两个句子之间的关系，这是语言建模无法直接捕获的。为了训练一个理解句子关系的模型，我们对一个二值化的下一个句子预测任务进行了预训练，该任务可以从任何单语语料库中轻松生成。具体来说，当为每个预训练示例选择句子A和B时，50%的时间B是A之后的下一个句子(标记为IsNext)， 50%的时间B是语料库中的随机句子(标记为NotNext)。其中C是用来预测下一个句子的（NSP）\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/overview.jpg?raw=true)\n\n### 3.2 Fine-tuning BERT\n\n对于涉及文本对的应用程序，常见的模式是在应用双向交叉注意之前对文本对进行独立编码,BERT使用自注意机制来统一这两个阶段，因为用自注意编码一个连接的文本对有效地包括两个句子之间的双向交叉注意。\n\n对于每个任务，我们只需将特定于任务的输入和输出插入到BERT中，并对所有参数进行端到端的微调。在输入端，预训练的句子A和句子B类似于(1)释义中的句子对，(2)蕴涵中的假设-前提对，(3)问答中的问题-段落对，(4)文本分类或序列标注中的退化文本-∅对。在输出端，令牌表示被输入输出层用于令牌级任务，如序列标记或问题回答，而[CLS]表示被输入输出层用于分类，如蕴意或情感分析。\n\n## 4.Experiments\n\n### GLUE(The General Language Understanding Evaluation)\n\n模型上就多引入了一个参数$W \\in \\mathbb{R}^{K×H}$，然后使用了$log(softmax)$计算损失\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/GLUE.jpg?raw=true)\n\n### SQuAD v1.1 (The Stanford Question Answering Dataset)\n\n任务是给出一个问题和一段话（来自维基百科并且包含答案），目的是找到这个答案。\n\n单词$i$作为答案区间开始的概率计算为$T_i$和$S$之间的点积，然后是段落中所有单词的softmax。最终的得分是$S*T_i+E*T_i$,得分最高的区间用作预测，训练目标是正确起始位置和结束位置的对数似然之和。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/SQuAD.jpg?raw=true)\n\n### SQuAD v2.0 (The Stanford Question Answering Dataset)\n\nSQuAD 2.0任务扩展了SQuAD 1.1问题定义，允许在所提供的段落中不存在简短答案的可能性，从而使问题更加现实。我们将没有答案的问题视为具有以[CLS]令牌开始和结束的答案跨度的问题。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/SQuAD2.jpg?raw=true)\n\n### SWAG(The Situations With Adversarial Generations)\n\n任务是给定一个句子，任务是从四个选项中选出最合理的延续。当对SWAG数据集进行微调时，我们构建了四个输入序列，每个序列包含给定句子(句子A)和可能的延续(句子B)的连接。引入的唯一特定于任务的参数是一个向量，其与[CLS]标记表示C的点积表示每个选择的分数，该分数用softmax层进行规范化。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/NLP/BERT/SWAG.jpg?raw=true)\n\n## Ablation Studies\n\n略（比较了预训练的作用，模型的size，Feature-based Approach三个方面）","tags":[],"folderPathname":"/NLP","data":{},"createdAt":"2023-12-27T05:53:47.502Z","updatedAt":"2023-12-27T08:48:36.707Z","trashed":false,"_rev":"_S_JBpjps"}