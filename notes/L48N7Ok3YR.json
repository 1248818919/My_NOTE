{"_id":"note:L48N7Ok3YR","title":"Video-instruction","content":"# video-instruction\n\n> Liu H, Li C, Wu Q, et al. Visual instruction tuning[J]. Advances in neural information processing systems, 2024, 36.\n\n\n## 1.introduction\n\n使用机器生成的指令跟随数据对大型语言模型(llm)进行指令调优已被证明可以提高新任务的零射击能力，但该想法在多模态领域的探索较少。我们首次尝试使用仅语言的GPT-4来生成多模态语言图像指令跟随数据。通过对这些生成的数据进行指令调优，我们引入了LLaV A:大型语言和视觉助理，这是一个端到端的训练大型多模态模型，它将视觉编码器和LLM连接起来，用于通用的视觉和语言理解。为了促进视觉指令跟随的进一步研究，我们构建了两个具有多样化和挑战性的应用导向任务的评估基准。我们的实验表明，LLaV A展示了令人印象深刻的多模态聊天能力，有时在看不见的图像/指令上表现出多模态GPT-4的行为，与GPT-4相比，在合成的多模态指令遵循数据集上获得了85.1%的相对分数。当在Science QA上进行微调时，LLaVA和GPT-4的协同作用达到了92.53%的最新水平。我们让GPT-4生成的可视化指令调优数据、我们的模型和代码公开可用。","tags":[],"folderPathname":"/Computer Vision/multi-modal/CLIP衍生领域/Video","data":{},"createdAt":"2024-07-31T13:17:30.363Z","updatedAt":"2024-07-31T13:18:04.521Z","trashed":false,"_rev":"QpTF40wPO"}