{"_id":"note:jME8b8qK2r","title":"ReST","content":"# ReST 论文阅读笔记\n\n> Cheng C C, Qiu M X, Chiang C K, et al. Rest: A reconfigurable spatial-temporal graph model for multi-camera multi-object tracking[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 10051-10060.\n\n## 1.Introduction\n\n我们的贡献可以概括如下。1)多相机多目标跟踪问题在本文提出的图模型中分为空间关联和时间关联两个子任务。这使得利用时空一致性和更好的模型优化成为可能。2)提出图重构模块，利用两个阶段的跟踪结果。这使得目标跟踪易于适应动态场景变化和在线跟踪场景。3)实验结果表明，我们的模型在Wildtrack上取得了最先进的性能，在其他基准数据集上取得了具有竞争力的结果。\n\n## 2.Proposed Method\n\n 定义t时刻的graph model为$G_{t}=(V_{t},E_{t})$,其中$V_{t}$表示顶点集，$E_{t}$为边集，每个节点$v_{i} \\in V_t$包含了以下的信息：camera ID$c_{v_{i}}\\in\\mathbb{R}^{1}$,时间戳信息$t_{v_i}\\in\\mathbb{R}^1$,目标ID信息$o_{v_{i}}\\in\\mathbb{R}^{1}$，bounding box信息$b_{v_{i}}\\in\\mathbb{R^{4}}$，外貌特征$d_{v_{i}}\\in\\mathbb{R}^{512}$,位置信息$p_{v_{i}}\\in\\mathbb{R}^{2}$,速度信息$s_{v_{i}}\\in\\mathbb{R}^{2}$,外貌信息可以用以下函数获得：\n $$\n d_{v_i}=f_{ReID}(I_{v_i})\n $$\n 其中，f_{ReID}是现成的一个模型，而地理位置信息和速度信息用以下公式计算\n $$\n p_{v_i}=P_{c_{v_i}}(x+\\frac{w}{2},y+h),\\quad s_{v_i}=\\frac{p_{v_i}-p_{v_j}}{t_{v_i}-t_{v_j}}\n $$\n 任意一对节点vi和vj之间的几何位置、外观特征和速度的相对距离可以定义为：\n $$\n \\begin{aligned}\\Delta d_{ij}&=[\\|d_{v_i}-d_{v_j}\\|_1,1-cosine_{-}similarity(d_{v_i},d_{v_j})],\\\\\\Delta p_{ij}&=[\\|p_{v_i}-p_{v_j}\\|_1,\\|p_{v_i}-p_{v_j}\\|_2],\\\\\\\\\\Delta s_{ij}&=[\\|s_{v_i}-s_{v_j}\\|_1,\\|s_{v_i}-s_{v_j}\\|_2].\\end{aligned}\n $$\n \n - **Spatial Association**\n \n将某一时刻的所有视角下的所有目标用以下记号，$V_t^S=\\bigcup_{i=1}^CB_i^t$,初始化的时候图的边记为：\n $$\n a_{ij}^S=\\begin{cases}1,&\\text{if}c_{v_i}\\neq c_{v_j}\\\\0,&\\text{otherwise}\\end{cases}\n $$\n初始化的时候节点的特征和初始化边的特征用以下表示\n $$\n\\begin{aligned}h_{v_i}^0&=f_{FE}^v(d_{v_i}),\\\\\\\\h_{e_{ij}}^0&=f_{FE}^e([\\Delta p_{ij},\\Delta d_{ij}]),\\end{aligned}\n $$\n运算符[·，·]表示两个项的concatenation。提取最终的边缘特征后，进行链接预测，作为对象关联的结果，构建最终的空间图。通过后处理模块，进一步细化空间图形。\n\n- **Temporal Association**\n\n初始化的节点和边的信息用以下信息定义：\n$$\n\\begin{aligned}h_{v_i}^0&=f_{FE}^v([d_{v_i},p_{v_i}]),\\\\\\\\h_{e_{ij}}^0&=f_{FE}^e([\\Delta p_{ij},\\Delta d_{ij},\\Delta s_{ij}]).\\end{aligned}\n$$\n\n为了获得$G^T_t$,需要从$G^S_t,$G^T_{t-1}$中获得，前两者的作用是聚合信息得到当前的图，聚合方式如下：\n$$\nd_v=\\frac{\\sum_{v\\in H_i}d_v}{|H_i|}\\\\\np_v=\\frac{\\sum_{v\\in H_i}p_v}{|H_i|}.\n$$\n初始化的边如下,即不同的两个时间上有一个边：\n$$\na_{ij}^T=\\begin{cases}1,&\\text{if}t_{v_i}\\neq t_{v_j}\\\\0,&\\text{otherwise}\\end{cases}\n$$\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/ReST/pic2.jpg?raw=true)\n\n- **Post-Processing**\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/ReST/pic3.jpg?raw=true)\n\n- **Message Passing Network**\n\n这个MPN的最终目的是预测哪些边需要被裁剪，每次裁剪完后会更新边的值和节点的值\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/ReST/pic4.jpg?raw=true)\n\n- **Training Scheme**\n\n为了训练空间图，训练输入是同一时间段内不同视角的所有检测。对于时间图，训练输入只包含来自同一摄像机c的不同帧的检测，利用Focal Loss计算每次消息传递迭代l时，真值标签与预测标签之间的损失，为\n$$\n\\mathcal{L}=\\sum_{l=1}^{L}\\sum_{e_{ij}\\in E^S\\cup E^T}FL(\\hat{y}_{e_{ij}}^{l},y_{e_{ij}})\n$$\n\n## 3.Experiment\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/ReST/pic5.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/ReST/pic6.jpg?raw=true)\n","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-01-18T01:18:45.046Z","updatedAt":"2024-01-18T04:49:55.561Z","trashed":false,"_rev":"xCCY4F862"}