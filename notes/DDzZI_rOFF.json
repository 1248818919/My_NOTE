{"_id":"note:DDzZI_rOFF","title":"JDE","content":"# JDE 论文阅读笔记\n\n>Wang Z, Zheng L, Liu Y, et al. Towards real-time multi-object tracking[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 107-122.\n\n## 1.Abstract\n\n当前目标跟踪(MOT)系统通常采用检测-跟踪模式，即其由两部分组成：用于目标定位的目标检测模型和用于提取人物外貌特征以用于帧与帧关联的编码模型。然而，这种模式的一大缺点就是效率问题，因为运行时间是检测模型和外貌编码模型两者之和。本文提出了一种允许在共享模型同时学习目标检测和外观编码的MOT算法。具体的，通过将目标外貌编码模型融入到单视角检测器中，使得该模型可以同时输出检测结果和相应的外貌编码结果。除此之外，本文进一步提出了一种与联合模型相结合的简单快速的关联方法。与以往的MOT系统相比，这两个组件的计算成本都显著降低，为后续的实时MOT算法设计提供了简洁、快速的基线。根据输入图像分辨率的不同，运行速度为22到40 FPS。同时，其跟踪精度可与体现分离检测和嵌入(SDE)学习的最先进跟踪器相媲美(MOTA为64.4%，MOTA在MOT-16挑战中为66.1%)。\n\n## 2.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/SingleView/JDE/pic1.jpg?raw=true)\n\n前面提取特征是使用RPN方法，骨干网络使用的DarkNet-53.\n\n### 损失函数设计\n\n在外貌编码损失函数的时候，作者比较了三个损失函数：triplet loss，smooth upper bound of\nthe triplet loss以及交叉熵损失。\n\n$$\n\\mathcal{L}_{triplet}=\\sum_i\\max\\left(0,f^\\top f_i^--f^\\top f^+\\right)\n$$\n\n$$\n\\mathcal{L}_{upper}=-\\log\\frac{\\exp(f^\\top f^+)}{\\exp(f^\\top f^+)+\\sum_i\\exp(f^\\top f_i^-)}.\n$$\n\n$$\n\\mathcal{L}_{CE}=-\\log\\frac{\\exp(f^\\top g^+)}{\\exp(f^\\top g^+)+\\sum_i\\exp(f^\\top g_i^-)}\n$$\n\n其中，triplet loss使用的是一个mini-batch中hardest positive and negtive样本，这个等会看看代码，使用这种办法会导师损失不稳定，且收敛比较慢。然后第二个方法，是和采样的样本进行比较，计算损失使用的是特征向量之间的距离。第三个方法是直接使用定死的权重。作者认为第三种方法比较好，因为他和所有的东西都进行了比较。\n\n## 3.Code\n\n首先就是如何定义的，这里是直接使用了yolo的模型\n\n    class YOLOLayer(nn.Module):\n        def __init__(self, anchors, nC, nID, nE, img_size, yolo_layer):\n            super(YOLOLayer, self).__init__()\n            self.layer = yolo_layer\n            nA = len(anchors)\n            self.anchors = torch.FloatTensor(anchors)\n            self.nA = nA  # number of anchors (3)\n            self.nC = nC  # number of classes (80)\n            self.nID = nID # number of identities\n            self.img_size = 0\n            self.emb_dim = nE \n            self.shift = [1, 3, 5]\n\n            self.SmoothL1Loss  = nn.SmoothL1Loss()\n            self.SoftmaxLoss = nn.CrossEntropyLoss(ignore_index=-1)\n            self.CrossEntropyLoss = nn.CrossEntropyLoss()\n            self.IDLoss = nn.CrossEntropyLoss(ignore_index=-1)\n            self.s_c = nn.Parameter(-4.15*torch.ones(1))  # -4.15\n            self.s_r = nn.Parameter(-4.85*torch.ones(1))  # -4.85\n            self.s_id = nn.Parameter(-2.3*torch.ones(1))  # -2.3\n\n            self.emb_scale = math.sqrt(2) * math.log(self.nID-1) if self.nID>1 else 1\n\n然后就是如何推理和计算损失\n\n    def forward(self, p_cat,  img_size, targets=None, classifier=None, test_emb=False):\n        p, p_emb = p_cat[:, :24, ...], p_cat[:, 24:, ...]\n        nB, nGh, nGw = p.shape[0], p.shape[-2], p.shape[-1]\n\n        if self.img_size != img_size:\n            create_grids(self, img_size, nGh, nGw)\n\n            if p.is_cuda:\n                self.grid_xy = self.grid_xy.cuda()\n                self.anchor_wh = self.anchor_wh.cuda()\n        # (B, Anchor, Class + 5, H, W) -> (B, Anchor, H, W, class + 5)\n        p = p.view(nB, self.nA, self.nC + 5, nGh, nGw).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n        \n        p_emb = p_emb.permute(0,2,3,1).contiguous()\n        p_box = p[..., :4]\n        p_conf = p[..., 4:6].permute(0, 4, 1, 2, 3)  # Conf\n\n        # Training\n        if targets is not None:\n            if test_emb:\n                tconf, tbox, tids = build_targets_max(targets, self.anchor_vec.cuda(), self.nA, self.nC, nGh, nGw)\n            else:\n                tconf, tbox, tids = build_targets_thres(targets, self.anchor_vec.cuda(), self.nA, self.nC, nGh, nGw)\n            # tbox = torch.zeros(nB, nA, nGh, nGw, 4).cuda()  # batch size, anchors, grid size\n            # tconf = torch.LongTensor(nB, nA, nGh, nGw).fill_(0).cuda()\n            # tid = torch.LongTensor(nB, nA, nGh, nGw, 1).fill_(-1).cuda() \n            tconf, tbox, tids = tconf.cuda(), tbox.cuda(), tids.cuda()\n            mask = tconf > 0 # (nB, nA, nGh, nGw)\n\n            # Compute losses\n            nT = sum([len(x) for x in targets])  # number of targets\n            nM = mask.sum().float()  # number of anchors (assigned to targets)\n            nP = torch.ones_like(mask).sum().float()\n            if nM > 0:\n                lbox = self.SmoothL1Loss(p_box[mask], tbox[mask])\n            else:\n                FT = torch.cuda.FloatTensor if p_conf.is_cuda else torch.FloatTensor\n                lbox, lconf =  FT([0]), FT([0])\n            lconf =  self.SoftmaxLoss(p_conf, tconf)\n            lid = torch.Tensor(1).fill_(0).squeeze().cuda()\n            emb_mask,_ = mask.max(1) # (nB, nGh, nGw)\n            \n            # For convenience we use max(1) to decide the id, TODO: more reseanable strategy\n            tids,_ = tids.max(1)  # (nB, nGh, nGw, 1)\n            tids = tids[emb_mask]\n            embedding = p_emb[emb_mask].contiguous()\n            embedding = self.emb_scale * F.normalize(embedding)\n            nI = emb_mask.sum().float()\n            \n            if  test_emb:\n                if np.prod(embedding.shape)==0  or np.prod(tids.shape) == 0:\n                    return torch.zeros(0, self.emb_dim+1).cuda()\n                emb_and_gt = torch.cat([embedding, tids.float()], dim=1)\n                return emb_and_gt\n            \n            if len(embedding) > 1:\n                logits = classifier(embedding).contiguous()\n                lid =  self.IDLoss(logits, tids.squeeze())\n\n            # Sum loss components\n            loss = torch.exp(-self.s_r)*lbox + torch.exp(-self.s_c)*lconf + torch.exp(-self.s_id)*lid + \\\n                   (self.s_r + self.s_c + self.s_id)\n            loss *= 0.5\n\n            return loss, loss.item(), lbox.item(), lconf.item(), lid.item(), nT\n\n        else:\n            p_conf = torch.softmax(p_conf, dim=1)[:,1,...].unsqueeze(-1)\n            p_emb = F.normalize(p_emb.unsqueeze(1).repeat(1,self.nA,1,1,1).contiguous(), dim=-1)\n            #p_emb_up = F.normalize(shift_tensor_vertically(p_emb, -self.shift[self.layer]), dim=-1)\n            #p_emb_down = F.normalize(shift_tensor_vertically(p_emb, self.shift[self.layer]), dim=-1)\n            p_cls = torch.zeros(nB,self.nA,nGh,nGw,1).cuda()               # Temp\n            p = torch.cat([p_box, p_conf, p_cls, p_emb], dim=-1)\n            #p = torch.cat([p_box, p_conf, p_cls, p_emb, p_emb_up, p_emb_down], dim=-1)\n            p[..., :4] = decode_delta_map(p[..., :4], self.anchor_vec.to(p))\n            p[..., :4] *= self.stride\n\n            return p.view(nB, -1, p.shape[-1])","tags":[],"folderPathname":"/Computer Vision/Track/single_view","data":{},"createdAt":"2024-03-31T02:22:56.092Z","updatedAt":"2024-03-31T09:19:46.482Z","trashed":false,"_rev":"4uB8zlOkO"}