{"_id":"note:q_moQgltwy","title":"MTMMC","content":"# MTMMC 论文阅读笔记\n\n> Woo S, Park K, Shin I, et al. MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark[J]. arXiv preprint arXiv:2403.20225, 2024.\n\n## 1.Abstract\n\n多目标多相机跟踪是一项十分重要的任务，其使用来自多个相机机的视频流来识别和跟踪目标。该任务在视觉监控、人群行为分析和异常检测等各个领域都有实际应用。然而，由于收集和标记数据的难度和成本，该任务的现有数据集要么是在受限制的摄像机网络设置中合成生成的，要么是人工构建的，这限制了它们数据集模拟现实世界动态变化的和将其推广到不同摄像机的能力。为了解决这个问题，我们展示了MTMMC，一个现实世界大规模数据集，包括16台多模态摄像机在不同时间、天气和季节条件下在校园和工厂两种不同环境中捕获的长视频序列。该数据集为研究不同现实世界复杂性下的多相机跟踪提供了一个具有挑战性的测试平台，并包括空间对齐和时间同步的RGB和热像仪的额外输入方式，从而提高了多相机跟踪的准确性。MTMMC有利于不同的领域的研究，如人员检测、再识别和多目标跟踪。本文在这个数据集上提供基线和新的学习设置，并为未来的研究设置参考分数。\n\n## 2.Related Work\n\nBenchmarks：MTMC，MMPTRACK\n\n> Philipp Kohl, Andreas Specker, Arne Schumann, and Jurgen Beyerer. The mta dataset for multi-target multi-camera pedestrian tracking by weighted distance aggregation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 1042–1043, 2020.\n\n>Xiaotian Han, Quanzeng Y ou, Chunyu Wang, Zhizheng Zhang, Peng Chu, Houdong Hu, Jiang Wang, and Zicheng\nLiu. Mmptrack: Large-scale densely annotated multicamera multiple people tracking benchmark. arXiv preprint\narXiv:2111.15157, 2021.\n\nMulti Camera Association \n由于不同相机之间物体外观的明显变化、背景条件的变化以及需要匹配的目标数量的增加，跨相机关联提出了更大的挑战。为了促进这一过程，有各种约束条件包括时间冲突[86]、线性运动模式[55]、摄像机网络拓扑[32,59]、几何线索[6,10,78]和空间局部性[29]。跨摄像头关联既可以实时进行[8,52]，也可以离线进行[12,28,29,55,86]，后者因精度更高而更受青睐。值得注意的是，已经开发了几种离线全局关联技术，如层次聚类[86,91]、相关聚类[5,55]、矩阵分解[28]和自适应基于位置的关联[29]。\n\n## 3.Overview\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic1.jpg?raw=true)\n\nData Splits 我们将MTMMC数据集分成三个子集。训练集包括14个场景(7个来自工厂，7个来自校园)，验证集包括5个场景(3个来自工厂，2个来自校园)，测试集包括6个场景(2个来自工厂，4个来自校园)。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic2.jpg?raw=true)\n\n# 4.Experiment\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic3.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic4.jpg?raw=true)\n\n根据重新识别(Re-ID)数据构建的标准协议，如[88,90]所述，我们从更大的MTMMC数据集中导出了MTMMC- reid数据集。在我们的实验中，我们使用AGW模型[77]作为基准。\n\n重新识别任务需要在多个摄像机视图和不同时间识别个人。训练数据特征显著影响Re-ID系统的性能。特别是MTMMC-reID数据集，提供了一个具有挑战性的训练环境，与其他数据集相比，Rank-1精度和mAP分数分别为76.0和45.6，这证明了这一点(见表3，第2-4行)。这些数字突出了MTMMC-reID中跟踪场景的苛刻性质。\n\n然而，数据集的复杂性有利于模型的泛化。例如，当在MSMT17[69]数据集上训练的模型在Market-1501[88]上进行评估时，性能下降(降至64.3 Rank-1和34.2 mAP)，表明泛化能力的丧失。然而，如果同样的模型在MTMMC-reID上训练并在Market-1501上测试，与MSMT17训练相比，它表现出更好的鲁棒性，具有更高的Rank-1精度和mAP分数(分别为66.5和35.4)(参见表3，第5-6行)。这些结果表明，尽管MTMMCreID存在内在的挑战，但在其上训练的模型能够更好地处理新的、看不见的环境，这强调了严格的训练环境对于改进现实世界适用性的价值。\n\n","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-04-07T03:51:16.026Z","updatedAt":"2024-04-07T04:14:37.962Z","trashed":false,"_rev":"KbvKkLui-"}