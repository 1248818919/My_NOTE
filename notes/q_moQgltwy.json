{"_id":"note:q_moQgltwy","title":"MTMMC","content":"# MTMMC 论文阅读笔记\n\n> Woo S, Park K, Shin I, et al. MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark[J]. arXiv preprint arXiv:2403.20225, 2024.\n\n## 1.Abstract\n\n多目标多相机跟踪是一项十分重要的任务，其使用来自多个相机机的视频流来识别和跟踪目标。该任务在视觉监控、人群行为分析和异常检测等各个领域都有实际应用。然而，由于收集和标记数据的难度和成本，该任务的现有数据集要么是在受限制的摄像机网络设置中合成生成的，要么是人工构建的，这限制了它们数据集模拟现实世界动态变化的和将其推广到不同摄像机的能力。为了解决这个问题，我们展示了MTMMC，一个现实世界大规模数据集，包括16台多模态摄像机在不同时间、天气和季节条件下在校园和工厂两种不同环境中捕获的长视频序列。该数据集为研究不同现实世界复杂性下的多相机跟踪提供了一个具有挑战性的测试平台，并包括空间对齐和时间同步的RGB和热像仪的额外输入方式，从而提高了多相机跟踪的准确性。MTMMC有利于不同的领域的研究，如人员检测、再识别和多目标跟踪。本文在这个数据集上提供基线和新的学习设置，并为未来的研究设置参考分数。\n\n## 2.Related Work\n\nBenchmarks：MTMC，MMPTRACK\n\n> Philipp Kohl, Andreas Specker, Arne Schumann, and Jurgen Beyerer. The mta dataset for multi-target multi-camera pedestrian tracking by weighted distance aggregation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 1042–1043, 2020.\n\n>Xiaotian Han, Quanzeng Y ou, Chunyu Wang, Zhizheng Zhang, Peng Chu, Houdong Hu, Jiang Wang, and Zicheng\nLiu. Mmptrack: Large-scale densely annotated multicamera multiple people tracking benchmark. arXiv preprint\narXiv:2111.15157, 2021.\n\nMulti Camera Association \n由于不同相机之间物体外观的明显变化、背景条件的变化以及需要匹配的目标数量的增加，跨相机关联提出了更大的挑战。为了促进这一过程，有各种约束条件包括时间冲突[86]、线性运动模式[55]、摄像机网络拓扑[32,59]、几何线索[6,10,78]和空间局部性[29]。跨摄像头关联既可以实时进行[8,52]，也可以离线进行[12,28,29,55,86]，后者因精度更高而更受青睐。值得注意的是，已经开发了几种离线全局关联技术，如层次聚类[86,91]、相关聚类[5,55]、矩阵分解[28]和自适应基于位置的关联[29]。\n\n## 3.Overview\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic1.jpg?raw=true)\n\nData Splits 我们将MTMMC数据集分成三个子集。训练集包括14个场景(7个来自工厂，7个来自校园)，验证集包括5个场景(3个来自工厂，2个来自校园)，测试集包括6个场景(2个来自工厂，4个来自校园)。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic2.jpg?raw=true)\n\n# 4.Experiment\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic3.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic4.jpg?raw=true)\n\n根据重新识别(Re-ID)数据构建的标准协议，如[88,90]所述，我们从更大的MTMMC数据集中导出了MTMMC- reid数据集。在我们的实验中，我们使用AGW模型[77]作为基准。\n\n重新识别任务需要在多个摄像机视图和不同时间识别个人。训练数据特征显著影响Re-ID系统的性能。特别是MTMMC-reID数据集，提供了一个具有挑战性的训练环境，与其他数据集相比，Rank-1精度和mAP分数分别为76.0和45.6，这证明了这一点(见表3，第2-4行)。这些数字突出了MTMMC-reID中跟踪场景的苛刻性质。\n\n然而，数据集的复杂性有利于模型的泛化。例如，当在MSMT17[69]数据集上训练的模型在Market-1501[88]上进行评估时，性能下降(降至64.3 Rank-1和34.2 mAP)，表明泛化能力的丧失。然而，如果同样的模型在MTMMC-reID上训练并在Market-1501上测试，与MSMT17训练相比，它表现出更好的鲁棒性，具有更高的Rank-1精度和mAP分数(分别为66.5和35.4)(参见表3，第5-6行)。这些结果表明，尽管MTMMCreID存在内在的挑战，但在其上训练的模型能够更好地处理新的、看不见的环境，这强调了严格的训练环境对于改进现实世界适用性的价值。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic5.jpg?raw=true)\n\n在我们的实验中，我们使用了四种最先进的跟踪器:JDE[67]、QDTrack[49]、CenterTrack[94]和ByteTrack[85]，我们的分析主要集中在三个方面:\n\n1.同一数据集上的训练和评估:当模型在同一数据集上进行训练和评估时，与MOT17数据集相比，它们在MTMMC上表现出较低的性能。例如，JDE在MTMMC上的IDF1得分为42.4%，而相同的模型在MOT17上的IDF1得分提高了63.6%。这一趋势在所有测试模型中都是一致的，表明MTMMC提供了一个更具挑战性的测试平台。\n\n2.在不同数据集上的训练和评估:当训练和评估数据集不同时，我们观察到一种模式，即在MTMMC上训练的模型在备用数据集上评估时通常优于在MOT17上训练的模型。例如，在MTMMC上训练并在MOT17上测试后，ByteTrack的IDF1得分为69.1%，MOTA得分为55.9%，更接近在MOT17上训练和测试时观察到的实际上界(IDF1为76.8%，MOTA为75.0%)。相比之下，当ByteTrack在MOT17上进行训练并在MTMMC上进行评估时，它的IDF1为40.2%，MOTA为56.8%，远低于在MTMMC上的上限性能(IDF1为64.8%，MOTA为89.7%)。这表明，在MTMMC中发现的复杂和多样化的跟踪环境有助于开发更健壮和可泛化的模型特征。值得注意的是，多目标跟踪中的上述两种趋势反映了我们在Re-ID实验中观察到的趋势。这种一致性强化了一个概念，即在更复杂和多样化的环境中进行训练，可以有效地增强模型在引入新领域时的泛化和保持准确性的能力。\n\n3.在组合数据集上训练:当模型在MTMMC和MOT17数据集上混合训练时，观察到最令人信服的结果。这种联合训练方法在MTMMC和MOT17评估中都产生了最好的结果。这意味着MTMMC为MOT17提供了一个互补的训练信号。当组合时，MTMMC的多样性和复杂性补充了MOT17，导致一个鲁棒的跟踪模型.\n\n**Pre-Training: Real-world vs. Synthetic Data**\n\n在本研究中，我们利用我们的MTMMC数据集作为基础训练集，评估了现实世界数据在改进MOT模型方面的有效性。我们使用QDTrack[49]作为基础跟踪器，并对其在MOT17基准测试上的性能进行了实验测量。这些实验包括在MTMMC数据集上对模型进行预训练，然后在MOT17上对其进行微调。此外，我们与在MOTSynth数据集上预训练的模型进行了比较[20]，这是一个从游戏环境中广泛模拟得出的大规模合成数据集。\n\n如表5所示，我们的研究结果表明，尽管MTMMC数据集包含的注释数量是MOTSynth的一半(0.5M vs. M)，并且没有复杂的数据模拟技术的帮助，但仍然对跟踪精度有很大的贡献。值得注意的是，在MTMMC上预训练的模型在没有微调的情况下产生的MOTA分数为55.3(在MOTSynth上预训练时为54.1)，并且在微调后增加到68.6(在MOTSynth上预训练时为70.8)。虽然MOTSynth从更高的基线开始，但是我们的实际数据，当与MOTSynth结合使用时，显示出显著的协同作用，从而在微调后获得了72.0的出色IDF1分数。\n\n这些观察结果强调了现实世界数据集的持续相关性。虽然合成数据提供的可伸缩性和控制很吸引人，但现实世界数据中存在的固有复杂性和可变性对于模型有效学习至关重要。因此，MTMMC数据集仍然是实现高保真跟踪性能的宝贵资源，它与合成数据的集成进一步增强了这一进步。\n\n### Multi-modal Learning: Setups and Baselines\n\n多模态学习旨在通过利用来自不同传感器模态的互补信息来提高对场景的理解。在这种情况下，我们探索如何热数据，当与RGB数据配对时，可以增强目标跟踪。这个问题源于现有文献，这些文献证明了这种组合在其他领域的好处[3,63,93]。我们的研究将这些概念扩展到使用QDTrack[49]作为基础跟踪器的跟踪场景中。\n\n我们提出了两种新的学习设置，情态融合和下降，分别如图3-(a)和(b)所示。我们在附录中提供了更详细的设置规范和附加分析。在这里，我们简要介绍设置的高级概念，然后讨论关键结果。\n\n我们从模态融合开始，重点是将热数据显式集成到基于rgb的跟踪模型中。这包括将输入和特征级融合方法与RGB和仅热基线进行比较。我们评估了热数据合并的好处，当它可以直接用于训练和测试时。\n\n情态切换设置呈现了一个更具挑战性的场景。在这里，模型在RGB和热数据上进行训练，但仅在RGB数据上进行评估。\n\n其基本原理是，在训练期间，即使在测试期间缺少模态，模型也可以学习具有鲁棒性的广义特征表示。在训练过程中，我们引入了三种有效利用RGB-T数据的策略:知识蒸馏、多模态重构和多模态对比学习。\n\n一个实际应用是在单峰跟踪系统中使用多模态训练跟踪模型。例如，考虑CCTV监控系统，由于硬件或预算限制，主要依赖于RGB摄像机。我们的目标是使用MTMMC等同时包含RGB和热数据的数据集来训练模型，然后在仅提供RGB数据的环境中测试其有效性。从本质上讲，我们的目标是确定模型是否可以在训练期间从组合的RGB和热数据中学习通用特征，并在测试期间没有热数据的情况下保持其跟踪能力。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic6.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MTMMC/pic7.jpg?raw=true)","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-04-07T03:51:16.026Z","updatedAt":"2024-04-07T05:12:59.795Z","trashed":false,"_rev":"efavqy7Nb"}