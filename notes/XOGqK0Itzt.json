{"_id":"note:XOGqK0Itzt","title":"UniRepLKNet","content":"# UniRepLKNet 论文阅读笔记\n\nDing X, Zhang Y, Ge Y, et al. UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition[J]. arXiv preprint arXiv:2311.15599, 2023.\n\n## 1.Introduction\n\nRepLKNet开创了大核卷积网络的时代。目前，大核卷积仍然存在以下问题：他们仅仅follow了其他的模型，但不知道为什么有效。本文探索了大核卷积网络架构的原则，认为$3 \\times 3$的卷积之所以能够成功是因为\n\n- 使接受野变大\n- 增加空间模式的抽象层次，比如从角度和纹理过渡到目标的形状\n- 通过使模型更深入，引入更多可学习的参数和非线性来提高模型的一般表示能力。\n\n我们认为大内核架构中的这三种影响应该解耦，因为模型应该利用大核的强大功能：能够看的广但不深入。当目标是从低级别的局部空间模式中提取高级别的局部空间模式时，3×3卷积可能是比大核卷积更合适的选择。原因是后者需要更多的计算，并且可能导致模式不再局限于较小的局部区域，这在特定场景中可能是不希望的。\n\n具体的，作者提出了四个架构的原则：\n- 第一个是使用有效的模块比如SE Blocks来增加深度。\n- 第二是使用本文提出的Dilated Reparam Block来重新参数化大核卷积，以提高新能而不需要额外的推理成本。\n- 根据下游任务决定卷积核的大小，只在中或高层的时候使用大核卷积\n- 使用$3 \\times 3$的卷积来增加模型的深度。\n\n作者实验结果显示，本文提出的原则应用之后，在CNN统治的领域内依旧表现出现甚至超过SOTA，在其他领域也表现了不错的效果。\n\n## 2.Related Work\n\n### 2.1 Large kernels in early ConvNets\n\n经典的ConvNets如AlexNet和inception在底层使用7×7或11×11，但在VGG-Net之后，大核就不流行了。Global Convolution Network (GCN)使用非常大的卷积层(1×K和K×1)进行语义分割。Local Relation Networks (LR-Net)采用空间聚合算子(LRLayer)代替标准的conv层，可以看作是一个动态卷积。LR-Net从内核大小7×7中受益，但在9×9中有所下降。当内核大小与feature map一样大时，其top-1的准确率从75.7%显著降低到68.4%。\n\n### 2.2 Explorations with large kernels\n\n核的概念可以推广到空间卷积之外。Swin Transformer使用了shifted attention，窗口大小从7到12不等，可以看作是一个动态内核。Han等用静态或动态7×7 conv替代Swin中的注意层，仍然保持了可比较的结果。MetaFormer建议大核池化层可以替代自关注。另一个代表性的工作是Global Filter Network (GFNet)，它在傅里叶域中优化了空间连接权重。它等价于空间域中的圆形全局卷积。\n\n### 2.3 Modern ConvNets with very large kernels\n\nRepLKNet首先提出了简单地扩大现有卷积神经网络的内核大小可以带来改进，特别是在下游任务上。它提出了一些使用大核时的指导方针，这些指导方针侧重于微观结构设计(例如，在大核旁边使用快捷方式)和应用(大核卷积神经网络应该在下游任务上进行评估)。在架构方面，RepLKNet只是为了简单而遵循Swin Transformer。在过去的两年中，人们对大核卷积神经网络进行了深入的研究。一些工作成功地进一步扩大了核尺寸，将思想推广到3D场景和许多下游任务，例如图像去雾和超分辨率。然而，我们注意到具有非常大内核的卷积神经网络的架构设计仍然没有得到充分的探索。例如，SLaK遵循了ConvNeXt开发的架构，这是一个强大的中型(7×7)内核架构。\n\n## 3.Architectural Design of UniRepLKNet\n\n### 3.1 Dilated Reparam Block\n\n根据研究，大核卷积应该与并行的小核卷积一起使用，因为后者有助于在训练过程中捕获小尺度模式。以下给出了作者是如何将多个核进行融合的，图中融合了五个kernel，不同的kernel采用不同的空洞卷积的dilation，并且将空洞卷积转换为非空洞卷积。\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic2.jpg?raw=true)\n\n具体的，这个block作者将空洞卷积变成了非空洞卷积，这样等价的原因是通过插入0项可以让核的大小进行变化，，最后作者得到了下面的公式：\n\n$$\n\\mathrm{W}^{\\prime}=\\text{conv}\\_\\mathrm{transpose}2\\mathrm{d}(\\mathrm{W},\\mathrm{I},\\mathrm{stride}=r).\n$$\n\n引用其他人博客中关于转置卷积的定义,可以很容易理解这个公式显然成立：\n>同时，等价的标准卷积的输入矩阵 input 在卷积运算前，需先进行 padding'=$k-1$ 的填充；然后，相邻元素间的空洞数为 $s-1$ , 共有 $i^{\\prime}-1$ 组空洞需插入；从而，实际尺寸为 \n>$$\ni^{\\prime\\prime}=i^{\\prime}+2(k-1)+(i^{\\prime}-1)\\times(s-1)=s\\times(i^{\\prime}-1)+2k-1$$\n因此，实际上原计算公式为 (等价的标准卷积的步长 $s^{\\prime}=1$):\n>$$\no'=\\frac{i''-k+2p}{s'}+1=s(i'-1)+2k-1-k+1=s(i'-1)+k$$\n\n### 3.2 Architectural Guidelines for Large Kernels\n\n#### Vanilla architecture\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic1.jpg?raw=true)\n\n第一个下采样block使用了两个stride=2，$3\\times3$的卷积将图片变成通道数量为C的特征图，其他三个下采样blocks每个使用一个stride=2，$3\\times3$的卷积奖原来的通道数变成之前的两倍，也就是说四个stages的通道数分别为C,2C,4C,8C。本文中的stage类似于ConvNeXt使用了depthwise（DW）卷积层，配有GRU的Feed-Forward Network (FFN)，但是本文中不使用LayerNorm而使用BN，再FFN后面使用BN。本文中C=96，每个stage中block数的数量分别是是（3,3,9,3），第一个stage中使用DW layer后面三个都使用Dilated Reparam Block作为DW。\n\n#### Experimental settings and metrics. \n\n文献中已经强调，大核卷积神经网络应该在下游任务上进行评估，因为它们的全部潜力可能无法仅通过ImageNet的精度来准确反映。\n\n- Guideline1：使用高效的结构，同时进行通道间的通信和空间聚合，以增加深度。作者尝试了不同的方式，最终采用了SE block\n\n![pic3](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic3.jpg?raw=true)\n\n- Guideline2：使用扩展的小内核来重新参数化大内核。我们尝试了两种具有相同数量的由非扩展层组成的并行分支的变体，A)相同的核大小或B)相同的等效核大小。\n\n![pic4](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic4.jpg?raw=true)\n\n- Guideline3:根据下游任务决定内核大小，通常在中高层使用较大的内核。还是有问题，为什么13是最好的，更大一点的15为什么效果会变差？感觉本文还是没有解决根本的问题。这里作者解释说我们认为这种现象并不意味着更大的内核会导致更低的特征质量。由于在较低阶段有较大的核，因此低级特征不再局限于小的局部区域，因此将它们与高级特征相结合对UPerNet的好处较少。也就是说大核卷积会提升性能，但是在下游任务上性能的变化说明需要动态调整模型结构。\n\n![pic5](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic5.jpg?raw=true)\n\n- Guidelines4：当扩展深度时，添加的块应该使用较小的内核。作者认为大核卷积神经网络可能无法从更大的核中获益。在本次实验中作者将N3的数量从9变到27个，结果如下\n![pic5](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic6.jpg?raw=true)\n在保持大卷积不变的时候，增加小卷积可以明显提高性能这里好像说明了DW卷积在作者这个模型中是有用的。\n\n### 3.3 Architectural Specifications\n\n作者设计了不同的模型\n\n![pic7](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic7.jpg?raw=true)\n\n### 3.4 Generalizing UniRepLKNet beyond Image\n\n作者将不同模态的数据都变成$B\\times C^{\\prime}\\times H\\times W$，其中的C根据不同的模态进行调整\n\n![pic8](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic8.jpg?raw=true)\n\n![pic7](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/UniRepLKNet/pic7.jpg?raw=true)","tags":[],"folderPathname":"/Computer Vision","data":{},"createdAt":"2024-01-11T06:47:35.267Z","updatedAt":"2024-01-11T10:05:12.917Z","trashed":false,"_rev":"MpYYviP9t"}