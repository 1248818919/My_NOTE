{"_id":"note:ZbdKZA8dp-","title":"EarlyBird","content":"# EarlyBird 论文阅读笔记\n\n>Teepe T, Wolters P, Gilg J, et al. EarlyBird: Early-Fusion for Multi-View Tracking in the Bird's Eye View[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024: 102-111.\n\n## 1.introduction\n\n遮挡导致检测丢失，轨迹碎片化，从而限制了检测和跟踪质量。然而，像体育分析这样的实际情况需要在高度混乱或拥挤的场景中进行检测。\n\n在早期的方法中，多视图检测是通过后期融合方法解决的:首先，在单个视图中检测行人，然后将该检测投影到3D空间或主要是地平面，在那里它与其他视图的投影相关联。\n\n最近的方法利用早期融合策略，首先将所有视图的表示投影到公共地平面或鸟瞰图，然后执行检测。与之前的后期融合方法相比，这些早期融合检测器[21,22]显著提高了检测质量。\n\n## 2.Related Work\n\n**Multi-View Object Detection** 对象的概率建模是深度学习带来进步之前的主要焦点。平均场推理和条件随机场(conditional random field, CRF)等技术通常用于从多个视图聚合信息。MVDet提出了一种基于卷积的端到端可训练方法，该方法将编码后的图像特征从每个视图投影到公共地平面，产生了显著的改进，并使其成为所有后续方法的基础架构。其他的方法都是基于透视投影的改进办法。\n\n**One-Shot Tracking** 这些跟踪器一步完成检测和跟踪，从而减少了推理时间。\n\n## 3.EarlyBird\n\n作者将所有的部分都包裹在了MVDet模型中。\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/EarlyBird/pic1.jpg?raw=true)\n>在这个上下文中，resolution 参数表示模型中体素化（Voxelization）的相关设置。具体来说，resolution 是一个包含三个整数的元组 (Y, Z, X)，分别表示在三个维度上的体素数量。这里的三个维度通常分别对应着空间中的高度、深度和宽度。\n\n>具体地：\n\n>Y: 代表高度维度上的体素数量。\nZ: 代表深度维度上的体素数量。\nX: 代表宽度维度上的体素数量。\n在你提供的代码中，resolution=(200, 4, 200) 表示模型使用了一个体素网格，该网格在高度、深度和宽度上分别有 200、4 和 200 个体素。这些体素用于对空间进行离散化，以便在模型中处理和表示三维空间的信息\n\n    class MVDet(nn.Module):\n        def __init__(self, Y, Z, X,\n                     rand_flip=False,\n                     num_cameras=None,\n                     num_ids=None,\n                     latent_dim=512,\n                     encoder_type='res18',\n                     device=torch.device('cuda')):\n            super().__init__()\n            assert (encoder_type in ['res101', 'res50', 'res18', 'res34', 'effb0', 'effb4', 'swin_t'])\n            \n            \n            self.Y, self.Z, self.X = Y, Z, X \n            self.rand_flip = rand_flip\n            self.latent_dim = latent_dim\n            self.encoder_type = encoder_type\n            self.num_cameras = num_cameras   # 当为None的时候默认是单相机\n\n            self.mean = torch.as_tensor([0.485, 0.456, 0.406], device=device).reshape(1, 3, 1, 1)\n            self.std = torch.as_tensor([0.229, 0.224, 0.225], device=device).reshape(1, 3, 1, 1)\n\n            # Encoder，论文中的编码器，这里使用的有点多\n            self.feat2d_dim = 128\n            if encoder_type == 'res101':\n                self.encoder = Encoder_res101(self.feat2d_dim)\n            elif encoder_type == 'res50':\n                self.encoder = Encoder_res50(self.feat2d_dim)\n            elif encoder_type == 'effb0':\n                self.encoder = Encoder_eff(self.feat2d_dim, version='b0')\n            elif encoder_type == 'res18':\n                self.encoder = Encoder_res18(self.feat2d_dim)\n            elif encoder_type == 'res34':\n                self.encoder = Encoder_res34(self.feat2d_dim)\n            elif encoder_type == 'swin_t':\n                self.encoder = Encoder_swin_t(self.feat2d_dim)\n            else:\n                self.encoder = Encoder_eff(self.feat2d_dim, version='b4')\n            \n            #  当为None的时候默认是单相机\n            if self.num_cameras is None:\n                self.world_conv = nn.Sequential(\n                    nn.Conv3d(self.feat2d_dim, self.feat2d_dim, 3, padding=1),\n                    nn.InstanceNorm3d(latent_dim), nn.ReLU(),\n                    # nn.Conv3d(latent_dim, latent_dim, 3, padding=(1, 2, 2), dilation=(1, 2, 2)),\n                )\n            else:\n                self.world_feat = nn.Sequential(\n                    nn.Conv2d(self.feat2d_dim * self.num_cameras, latent_dim, kernel_size=3, padding=1),\n                    nn.InstanceNorm3d(latent_dim), nn.ReLU(),\n                    nn.Conv2d(latent_dim, latent_dim, kernel_size=1),\n                )\n\n            # self.cam_out = nn.Sequential(\n            #     nn.Conv2d(latent_dim, self.feat2d_dim, 3, padding=1),\n            #     nn.InstanceNorm2d(self.feat2d_dim), nn.ReLU(),\n            #     nn.Conv2d(self.feat2d_dim, self.feat2d_dim, 1, padding=0),\n            # )\n\n            self.decoder = Decoder(\n                in_channels=latent_dim,\n                n_classes=2,\n                n_ids=num_ids,\n            )\n\n            # Weights\n            self.center_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n            self.offset_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n            self.size_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n            self.rot_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n\n### 3.1 Encoder\n\n将RGB图片$3\\times H_{i}\\times W_{i}$编码，下采样$\\frac{1}{4}$,得到$C_{f}\\times H_{f}\\times W_{f}$，其中$H_{f}=H_{i}/4,W_{f}=W_{i}/4,C_f=128$，这里作者采用了很多编码器，比如ResNet，Swin Transformer\n\n    def forward(self, rgb_cams, pix_T_cams, cams_T_global, vox_util, ref_T_global):\n        \"\"\"\n        B = batch size, S = number of cameras, C = 3, H = img height, W = img width\n        rgb_cams: (B,S,C,H,W)\n        pix_T_cams: (B,S,4,4)\n        cams_T_global: (B,S,4,4)\n        vox_util: vox util object\n        ref_T_global: (B,4,4)\n        \"\"\"\n        B, S, C, H, W = rgb_cams.shape\n        # reshape tensors\n        # 这个是将B,S两个维度进行融合的函数\n        __p = lambda x: utils.basic.pack_seqdim(x, B)\n        # 这个是将BS分成B，S两个维度的函数\n        __u = lambda x: utils.basic.unpack_seqdim(x, B)\n        # 图片\n        rgb_cams_ = __p(rgb_cams)  # B*S,3,H,W\n        # 像素坐标系到相机坐标系\n        pix_T_cams_ = __p(pix_T_cams)  # B*S,4,4\n        # 相机坐标系到全局坐标系\n        cams_T_global_ = __p(cams_T_global)  # B*S,4,4\n        \n        # 全局坐标系到相机坐标系\n        global_T_cams_ = torch.inverse(cams_T_global_)  # B*S,4,4\n        # 参考坐标系到相机坐标系\n        ref_T_cams_ = torch.matmul(ref_T_global.repeat(S, 1, 1), global_T_cams_)  # B*S,4,4\n        # 相机坐标系到参考坐标系\n        cams_T_ref_ = torch.inverse(ref_T_cams_)  # B*S,4,4\n\n        # rgb encoder\n        device = rgb_cams_.device\n        # 对图片进行标准化处理\n        rgb_cams_ = (rgb_cams_ - self.mean.to(device)) / self.std.to(device)  # B*S,3,H,W\n        # 利用对应的encoder进行编码\n        feat_cams_ = self.encoder(rgb_cams_)  # B*S,latent_dim,H/8,W/8\n        _, C, Hf, Wf = feat_cams_.shape\n        # 计算图片的放缩比例\n        sy = Hf / float(H)\n        sx = Wf / float(W)\n        # 作用好像是放缩内参，没仔细看\n        featpix_T_cams_ = utils.geom.scale_intrinsics(pix_T_cams_, sx, sy)  # B*S,4,4\n        \n### 3.2 Projection\n\n作者将其从特征图坐标系转换到参考坐标系，不过作者忽略了Z坐标轴，转换公式如下：\n$$\ns\\begin{pmatrix}u\\\\v\\\\1\\end{pmatrix}=\\boldsymbol{P_0}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix}=\\begin{bmatrix}p_{11}&p_{12}&p_{14}\\\\p_{21}&p_{22}&p_{24}\\\\p_{31}&p_{32}&p_{34}\\end{bmatrix}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix}\n$$\n    \n    # 只取其中三列，忽略z轴坐标\n    featpix_T_ref_ = torch.matmul(featpix_T_cams_[:, :3, :3], cams_T_ref_[:, :3, [0, 1, 3]])  # B*S,3,3\n    ref_T_mem = vox_util.get_ref_T_mem(B, self.Y, self.Z, self.X)  # B,4,4\n    ref_T_mem = ref_T_mem[0, [0, 1, 3]][:, [0, 1, 3]]  # 3,3\n    featpix_T_mem_ = torch.matmul(featpix_T_ref_, ref_T_mem)  # B*S,3,3\n    mem_T_featpix = torch.inverse(featpix_T_mem_)  # B*S,3,3\n    proj_mats = mem_T_featpix  # B*S,3,3\n\n    # B*S,latent_dim,Y,X\n    world_features_ = warp_perspective(feat_cams_, proj_mats, (self.Y, self.X), align_corners=False)\n    # 将BS变成B,S\n    world_features = __u(world_features_)  # B,S,latent_dim,Y,X\n\n### 3.3 Aggregation & Decoder\n\n将高维度的BEV Feature map转换成我们想要大小的通道的特征图，使用的办法是一个2D conv。之后利用解码器，将提取到的特征BEV再次进行变换，聚合地平面上的位置和识别特征，然后，我们使用金字塔网络架构将每层的输出上采样到前一个更大的输出的大小。然后，将两个特征在通道维度上进行连接，并应用二维卷积。特征金字塔产生的解码输出与$C_g × H_g × W_g$的输入形状相同，但每个网格位置的接受场要高得多。\n\n    # 不是很清楚num_camera的作用\n    if self.num_cameras is None:\n        world_features = self.world_conv(world_features.permute(0, 2, 1, 3, 4))\n        world_features = self.world_feat(world_features.sum(2, keepdim=True))\n    else:\n        world_features = self.world_feat(world_features.view(B, S * self.feat2d_dim, self.Y, self.X))\n\n    # back_proj_mats = torch.inverse(proj_mats)  # B*S,3,3\n    # world_features_cam = __p(world_features.unsqueeze(1).repeat(1, S, 1, 1, 1))\n    # feat_cams_back_ = warp_perspective(world_features_cam, back_proj_mats, (Hf, Wf), align_corners=False)\n    # feat_cams_ = feat_cams_ + self.cam_out(feat_cams_back_)\n    out_dict = self.decoder(world_features, feat_cams_,\n                                (self.bev_flip1_index, self.bev_flip2_index) if self.rand_flip else None)\n\n        return out_dict\n其中的decoder的代码如下：\n\n    class Decoder(nn.Module):\n        def __init__(self, in_channels, n_classes, n_ids):\n            super().__init__()\n            backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n            freeze_bn(backbone)\n            self.first_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.bn1 = backbone.bn1\n            self.relu = backbone.relu\n            \n            # ResNet-18的三层网络\n            self.layer1 = backbone.layer1\n            self.layer2 = backbone.layer2\n            self.layer3 = backbone.layer3\n            \n            # 特征维度\n            self.reid_feat = 64\n            self.feat2d = 128\n            \n            # 上采样模块\n            shared_out_channels = in_channels\n            self.up3_skip = UpsamplingConcat(256 + 128, 256)\n            self.up2_skip = UpsamplingConcat(256 + 64, 256)\n            self.up1_skip = UpsamplingConcat(256 + 512, shared_out_channels)\n\n            # bev\n            # 不同的head\n            self.instance_offset_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, 2, kernel_size=1, padding=0),\n            )\n            self.instance_center_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, 1, kernel_size=1, padding=0),\n            )\n            self.instance_center_head[-1].bias.data.fill_(-2.19)\n\n            self.instance_size_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, 3, kernel_size=1, padding=0),\n            )\n            self.instance_rot_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, 8, kernel_size=1, padding=0),\n            )\n\n            # img\n            self.img_center_head = nn.Sequential(\n                nn.Conv2d(self.feat2d, self.feat2d, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(self.feat2d),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(self.feat2d, n_classes, kernel_size=1, padding=0),\n            )\n            self.img_offset_head = nn.Sequential(\n                nn.Conv2d(self.feat2d, self.feat2d, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(self.feat2d),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(self.feat2d, 2, kernel_size=1, padding=0),\n            )\n            self.img_size_head = nn.Sequential(\n                nn.Conv2d(self.feat2d, self.feat2d, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(self.feat2d),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(self.feat2d, 2, kernel_size=1, padding=0),\n            )\n\n            # re_id\n            self.id_feat_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, self.reid_feat, kernel_size=1, padding=0),\n            )\n            self.img_id_feat_head = nn.Sequential(\n                nn.Conv2d(self.feat2d, self.feat2d, kernel_size=3, padding=1, bias=False),\n                nn.InstanceNorm2d(self.feat2d),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(self.feat2d, self.reid_feat, kernel_size=1, padding=0),\n            )\n            self.emb_scale = math.sqrt(2) * math.log(n_ids - 1)\n\n        def forward(self, x, feat_cams, bev_flip_indices=None):\n            b, c, h, w = x.shape\n\n            # pad input\n            m = 8\n            ph, pw = math.ceil(h / m) * m - h, math.ceil(w / m) * m - w\n            x = torch.nn.functional.pad(x, (ph, pw))\n\n            # (H, W)\n            skip_x = {'1': x}\n            x = self.first_conv(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n\n            # (H/4, W/4)\n            x = self.layer1(x)\n            skip_x['2'] = x\n            x = self.layer2(x)\n            skip_x['3'] = x\n\n            # (H/8, W/8)\n            x = self.layer3(x)\n\n            # First upsample to (H/4, W/4)\n            x = self.up3_skip(x, skip_x['3'])\n\n            # Second upsample to (H/2, W/2)\n            x = self.up2_skip(x, skip_x['2'])\n\n            # Third upsample to (H, W)\n            x = self.up1_skip(x, skip_x['1'])\n\n            # Unpad\n            x = x[..., ph // 2:h + ph // 2, pw // 2:w + pw // 2]\n\n            # Extra upsample to (2xH, 2xW)\n            # x = self.up_sample_2x(x)\n\n            if bev_flip_indices is not None:\n                bev_flip1_index, bev_flip2_index = bev_flip_indices\n                x[bev_flip2_index] = torch.flip(x[bev_flip2_index], [-2])  # note [-2] instead of [-3], since Y is gone now\n                x[bev_flip1_index] = torch.flip(x[bev_flip1_index], [-1])\n\n            # bev\n            instance_center_output = self.instance_center_head(x)\n            instance_offset_output = self.instance_offset_head(x)\n            instance_size_output = self.instance_size_head(x)\n            instance_rot_output = self.instance_rot_head(x)\n            instance_id_feat_output = self.emb_scale * F.normalize(self.id_feat_head(x), dim=1)\n\n            # img\n            img_center_output = self.img_center_head(feat_cams)  # B*S,1,H/8,W/8\n            img_offset_output = self.img_offset_head(feat_cams)  # B*S,2,H/8,W/8\n            img_size_output = self.img_size_head(feat_cams)  # B*S,2,H/8,W/8\n            img_id_feat_output = self.emb_scale * F.normalize(self.img_id_feat_head(feat_cams), dim=1)  # B*S,C,H/8,W/8\n\n            return {\n                # bev\n                'raw_feat': x,\n                'instance_center': instance_center_output.view(b, *instance_center_output.shape[1:]),\n                'instance_offset': instance_offset_output.view(b, *instance_offset_output.shape[1:]),\n                'instance_size': instance_size_output.view(b, *instance_size_output.shape[1:]),\n                'instance_rot': instance_rot_output.view(b, *instance_rot_output.shape[1:]),\n                'instance_id_feat': instance_id_feat_output.view(b, *instance_id_feat_output.shape[1:]),\n                # img\n                'img_center': img_center_output,\n                'img_offset': img_offset_output,\n                'img_size': img_size_output,\n                'img_id_feat': img_id_feat_output,\n            }\n### 3.4 损失计算\n\n对于HeatMap，作者是使用focal loss进行损失计算的，方法和CenterNet中使用的一样。\n$$\nL_k=\\frac{1}{N}\\sum_{xyc}\\begin{cases}(1-\\hat{Y}_{xyc})^\\alpha\\log(\\hat{Y}_{xyc})&\\text{if}Y_{xyc}=1\\\\[2ex](1-Y_{xyc})^\\beta(\\hat{Y}_{xyc})^\\alpha\\log(1-\\hat{Y}_{xyc})&\\text{otherwise}\\end{cases}\n$$\n\n    class FocalLoss(torch.nn.Module):\n        '''nn.Module warpper for focal loss'''\n\n        def __init__(self, use_distance_weight=False):\n            super(FocalLoss, self).__init__()\n            self.use_distance_weight = use_distance_weight\n\n        def forward(self, pred, gt):\n            \"\"\" Modified focal loss. Exactly the same as CornerNet.\n                Runs faster and costs a little bit more memory\n                Arguments:\n                    pred (batch x c x h x w)\n                    gt_regr (batch x c x h x w)\n            \"\"\"\n            # find pos indices and neg indices\n            pos_inds = gt.eq(1).float()\n            neg_inds = gt.lt(1).float()\n\n            distance_weight = torch.ones_like(gt)\n            这段代码比较简单，就是以图片中心点(h/2,w/2)为源点，计算其他点与其的距离作为权重\n            if self.use_distance_weight:\n                w, h = gt.shape[-2:]\n                xs = torch.linspace(-1, 1, steps=h, device=gt.device)\n                ys = torch.linspace(-1, 1, steps=w, device=gt.device)\n                x, y = torch.meshgrid(xs, ys, indexing='xy')\n                distance_weight = 9 * torch.sin(torch.sqrt(x * x + y * y)) + 1\n\n            # following paper alpha 2, beta 4\n            neg_weights = torch.pow(1 - gt, 4)\n\n            loss = 0\n\n            pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds * distance_weight\n            neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds * distance_weight\n\n            num_pos = pos_inds.sum()\n            pos_loss = pos_loss.sum()\n            neg_loss = neg_loss.sum()\n\n            if num_pos == 0:\n                loss = loss - neg_loss\n            else:\n                loss = loss - (pos_loss + neg_loss) / num_pos\n            return loss\n\n至于如何计算损失的，以下是全部代码\n\n    def loss(self, target, output):\n        center_e = output['instance_center']\n        offset_e = output['instance_offset']\n        size_e = output['instance_size']\n        rot_e = output['instance_rot']\n        feat_bev_e = output['instance_id_feat']  # B,nids,Y,X\n        center_img_e = output['img_center']\n        offset_img_e = output['img_offset']\n        size_img_e = output['img_size']\n        feat_img_e = output['img_id_feat']\n\n        valid_g = target['valid_bev']\n        center_g = target['center_bev']\n        offset_g = target['offset_bev']\n        size_g = target['size_bev']\n        rotbin_g = target['rotbin_bev']\n        rotres_g = target['rotres_bev']\n        # depth_g = target['depth']\n        id_g = target['pid_bev']  # B,1,Y,X\n\n        B, S = target['center_img'].shape[:2]\n        center_img_g = basic.pack_seqdim(target['center_img'], B)\n        offset_img_g = basic.pack_seqdim(target['offset_img'], B)\n        size_img_g = basic.pack_seqdim(target['size_img'], B)\n        valid_img_g = basic.pack_seqdim(target['valid_img'], B)\n        id_img_g = basic.pack_seqdim(target['pid_img'], B)  # B*S,1,H/8,W/8\n\n        center_loss = self.center_loss_fn(basic._sigmoid(center_e), center_g)\n        offset_loss = torch.abs(offset_e - offset_g).sum(dim=1, keepdim=True)\n        # 这里计算偏差损失，只针对valid_g进行计算，至于valid_g怎么计算出来，我会在下一节进行讲解\n        offset_loss = 10 * basic.reduce_masked_mean(offset_loss, valid_g)\n\n        if size_g.any():\n            size_loss = torch.abs(size_e - size_g).sum(dim=1, keepdim=True)\n            size_loss = basic.reduce_masked_mean(size_loss, valid_g)\n            rot_loss = compute_rot_loss(rot_e, rotbin_g, rotres_g, valid_g)\n        else:\n            size_loss = torch.tensor(0.)\n            rot_loss = torch.tensor(0.)\n        \n        # 这里作者是遵照了FairMot，添加不确定项\n        center_factor = 1 / (2 * torch.exp(self.model.center_weight))\n        center_loss = 10.0 * center_factor * center_loss\n        center_uncertainty_loss = 0.5 * self.model.center_weight\n\n        offset_factor = 1 / (2 * torch.exp(self.model.offset_weight))\n        offset_loss = offset_factor * offset_loss\n        offset_uncertainty_loss = 0.5 * self.model.offset_weight\n\n        size_factor = 1 / (2 * torch.exp(self.model.size_weight))\n        size_loss = size_factor * size_loss\n        size_uncertainty_loss = 0.5 * self.model.size_weight\n        \n        rot_factor = 1 / (2 * torch.exp(self.model.rot_weight))\n        rot_loss = rot_factor * rot_loss\n        rot_uncertainty_loss = 0.5 * self.model.rot_weight\n\n        # img loss\n        center_img_loss = self.center_loss_fn(basic._sigmoid(center_img_e), center_img_g) / S\n        offset_img_loss = basic.reduce_masked_mean(\n            torch.abs(offset_img_e - offset_img_g).sum(dim=1, keepdim=True), valid_img_g) / S\n        size_img_loss = basic.reduce_masked_mean(\n            torch.abs(size_img_e - size_img_g).sum(dim=1, keepdim=True), valid_img_g) / (10 * S)\n\n        # re_id loss\n        valid_g = valid_g.flatten(1)\n        valid_img_g = valid_img_g.flatten(1)\n        targets = torch.cat([\n            id_g.flatten(2).transpose(1, 2)[valid_g],\n            id_img_g.flatten(2).transpose(1, 2)[valid_img_g]\n        ]).squeeze(-1)\n        feats = torch.cat([\n            feat_bev_e.flatten(2).transpose(1, 2)[valid_g],\n            feat_img_e.flatten(2).transpose(1, 2)[valid_img_g]\n        ])\n        ids = self.id_head(feats)\n        reid_class_loss = self.classification_loss(ids, targets)\n        reid_contras_loss = self.contrastive_loss(feats, targets)\n\n        loss_dict = {\n            'center_loss': center_loss,\n            'offset_loss': offset_loss,\n            'size_loss': size_loss,\n            'rot_loss': rot_loss,\n\n            'center_img': center_img_loss,\n            'offset_img': offset_img_loss,\n            'size_img': size_img_loss,\n\n            'reid_class_loss': reid_class_loss,\n            'reid_contras_loss': reid_contras_loss,\n        }\n        stats_dict = {\n            'center_uncertainty_loss': center_uncertainty_loss,\n            'offset_uncertainty_loss': offset_uncertainty_loss,\n            'size_uncertainty_loss': size_uncertainty_loss,\n            'rot_uncertainty_loss': rot_uncertainty_loss,\n        }\n        total_loss = sum(loss_dict.values()) + sum(stats_dict.values())\n\n        return total_loss, loss_dict, stats_dict\n\n推理时候的跟踪代码：\n\n    from collections import deque\n    from typing import List\n    from collections import OrderedDict\n    import numpy as np\n\n    from tracking.kalman_filter import KalmanFilter\n    from tracking import matching\n\n\n    class TrackState(object):\n        New = 0\n        Tracked = 1\n        Lost = 2\n        Removed = 3\n\n\n    class BaseTrack(object):\n        _count = 0\n\n        track_id = 0\n        is_activated = False\n        state = TrackState.New\n\n        history = OrderedDict()\n        features = []\n        curr_feature = None\n        score = 0\n        start_frame = 0\n        frame_id = 0\n        time_since_update = 0\n\n        # multi-camera\n        location = (np.inf, np.inf)\n\n        @property\n        def end_frame(self):\n            return self.frame_id\n\n        @staticmethod\n        def next_id():\n            BaseTrack._count += 1\n            return BaseTrack._count\n\n        def activate(self, *args):\n            raise NotImplementedError\n\n        def predict(self):\n            raise NotImplementedError\n\n        def update(self, *args, **kwargs):\n            raise NotImplementedError\n\n        def mark_lost(self):\n            self.state = TrackState.Lost\n\n        def mark_removed(self):\n            self.state = TrackState.Removed\n\n\n    class STrack(BaseTrack):\n        shared_kalman = KalmanFilter()\n\n        def __init__(self, tlwh, score, temp_feat, buffer_size=30):\n\n            # wait activate\n            self._tlwh = np.asarray(tlwh, dtype=np.float32)\n            self.kalman_filter = None\n            self.mean, self.covariance = None, None\n            self.is_activated = False\n\n            self.score = score\n            self.tracklet_len = 0\n\n            self.smooth_feat = None\n            self.update_features(temp_feat)\n            self.features = deque([], maxlen=buffer_size)\n            self.alpha = 0.9\n\n        def update_features(self, feat):\n            feat /= np.linalg.norm(feat)\n            self.curr_feat = feat\n            if self.smooth_feat is None:\n                self.smooth_feat = feat\n            else:\n                self.smooth_feat = self.alpha * self.smooth_feat + (1 - self.alpha) * feat\n            self.features.append(feat)\n            self.smooth_feat /= np.linalg.norm(self.smooth_feat)\n\n        def predict(self):\n            mean_state = self.mean.copy()\n            if self.state != TrackState.Tracked:\n                mean_state[7] = 0\n            self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n        @staticmethod\n        def multi_predict(stracks):\n            if len(stracks) > 0:\n                multi_mean = np.asarray([st.mean.copy() for st in stracks])\n                multi_covariance = np.asarray([st.covariance for st in stracks])\n                for i, st in enumerate(stracks):\n                    if st.state != TrackState.Tracked:\n                        multi_mean[i][7] = 0\n                multi_mean, multi_covariance = STrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\n                for i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\n                    stracks[i].mean = mean\n                    stracks[i].covariance = cov\n\n        def activate(self, kalman_filter, frame_id):\n            \"\"\"Start a new tracklet\"\"\"\n            self.kalman_filter = kalman_filter\n            self.track_id = self.next_id()\n            self.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n\n            self.tracklet_len = 0\n            self.state = TrackState.Tracked\n            if frame_id == 1:\n                self.is_activated = True\n            # self.is_activated = True\n            self.frame_id = frame_id\n            self.start_frame = frame_id\n\n        def re_activate(self, new_track, frame_id, new_id=False):\n            self.mean, self.covariance = self.kalman_filter.update(\n                self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n            )\n\n            self.update_features(new_track.curr_feat)\n            self.tracklet_len = 0\n            self.state = TrackState.Tracked\n            self.is_activated = True\n            self.frame_id = frame_id\n            if new_id:\n                self.track_id = self.next_id()\n\n        def update(self, new_track, frame_id, update_feature=True):\n            \"\"\"\n            Update a matched track\n            :type new_track: STrack\n            :type frame_id: int\n            :type update_feature: bool\n            :return:\n            \"\"\"\n            self.frame_id = frame_id\n            self.tracklet_len += 1\n\n            new_tlwh = new_track.tlwh\n            self.mean, self.covariance = self.kalman_filter.update(\n                self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\n            self.state = TrackState.Tracked\n            self.is_activated = True\n\n            self.score = new_track.score\n            if update_feature:\n                self.update_features(new_track.curr_feat)\n\n        @property\n        def tlwh(self):\n            \"\"\"Get current position in bounding box format `(top left x, top left y,\n                    width, height)`.\n            \"\"\"\n            if self.mean is None:\n                return self._tlwh.copy()\n            ret = self.mean[:4].copy()\n            ret[2] *= ret[3]\n            ret[:2] -= ret[2:] / 2\n            return ret\n\n        @property\n        def tlbr(self):\n            \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n            `(top left, bottom right)`.\n            \"\"\"\n            ret = self.tlwh.copy()\n            ret[2:] += ret[:2]\n            return ret\n\n        @staticmethod\n        def tlwh_to_xyah(tlwh):\n            \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n            height)`, where the aspect ratio is `width / height`.\n            \"\"\"\n            ret = np.asarray(tlwh).copy()\n            ret[:2] += ret[2:] / 2\n            ret[2] /= ret[3]\n            return ret\n\n        def to_xyah(self):\n            return self.tlwh_to_xyah(self.tlwh)\n\n        @staticmethod\n        def tlbr_to_tlwh(tlbr):\n            ret = np.asarray(tlbr).copy()\n            ret[2:] -= ret[:2]\n            return ret\n\n        @staticmethod\n        def tlwh_to_tlbr(tlwh):\n            ret = np.asarray(tlwh).copy()\n            ret[2:] += ret[:2]\n            return ret\n\n        def __repr__(self):\n            return 'OT_{}_({}-{})'.format(self.track_id, self.start_frame, self.end_frame)\n\n\n    class JDETracker(object):\n        def __init__(self, conf_thres=0.1, track_buffer=10, gating_threshold=1000):\n            self.tracked_stracks: List[STrack] = []\n            self.lost_stracks: List[STrack] = []\n            self.removed_stracks: List[STrack] = []\n\n            self.frame_id = 0\n            self.det_thresh = conf_thres\n            self.gating_threshold = gating_threshold\n            self.max_time_lost = track_buffer\n\n            self.kalman_filter = KalmanFilter()\n\n        def merge_outputs(self, detections):\n            results = {}\n            for j in range(1, self.opt.num_classes + 1):\n                results[j] = np.concatenate(\n                    [detection[j] for detection in detections], axis=0).astype(np.float32)\n\n            scores = np.hstack(\n                [results[j][:, 4] for j in range(1, self.opt.num_classes + 1)])\n            if len(scores) > self.max_per_image:\n                kth = len(scores) - self.max_per_image\n                thresh = np.partition(scores, kth)[kth]\n                for j in range(1, self.opt.num_classes + 1):\n                    keep_inds = (results[j][:, 4] >= thresh)\n                    results[j] = results[j][keep_inds]\n            return results\n\n        def update(self, dets, id_feature):\n            self.frame_id += 1\n            activated_starcks = [] # 包含了tracked和uncomfirmed\n            refind_stracks = []    # 重新再次跟踪\n            lost_stracks = []      # 丢失的\n            removed_stracks = []   # 不要了的\n\n            remain_inds = dets[:, 4] > self.det_thresh\n            dets = dets[remain_inds]\n            id_feature = id_feature[remain_inds]\n\n            if len(dets) > 0:\n                \"\"\"Detections\"\"\"\n                detections = [STrack(tlwhs[:4], tlwhs[4], f, self.max_time_lost) for\n                              (tlwhs, f) in zip(dets[:, :5], id_feature)]\n            else:\n                detections = []\n\n            \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n            unconfirmed = []\n            tracked_stracks: List[STrack] = []\n            for track in self.tracked_stracks:\n                if not track.is_activated:\n                    unconfirmed.append(track)\n                else:\n                    tracked_stracks.append(track)\n\n            ''' Step 2: First association, with embedding'''\n            strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n            # Predict the current location with KF\n            STrack.multi_predict(strack_pool)\n            dists = matching.embedding_distance(strack_pool, detections)\n            dists = matching.fuse_motion(self.kalman_filter, dists, strack_pool, detections,\n                                         gating_threshold=self.gating_threshold)\n            matches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.5)\n\n            for itracked, idet in matches:\n                track = strack_pool[itracked]\n                det = detections[idet]\n                if track.state == TrackState.Tracked:\n                    track.update(detections[idet], self.frame_id)\n                    activated_starcks.append(track)\n                else:\n                    track.re_activate(det, self.frame_id, new_id=False)\n                    refind_stracks.append(track)\n\n            ''' Step 3: Second association, with IOU'''\n            detections = [detections[i] for i in u_detection]\n            r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n            # dists = matching.iou_distance(r_tracked_stracks, detections)\n            dists = matching.center_distance(r_tracked_stracks, detections)\n            matches, u_track, u_detection = matching.linear_assignment(dists, thresh=100)\n\n            for itracked, idet in matches:\n                track = r_tracked_stracks[itracked]\n                det = detections[idet]\n                if track.state == TrackState.Tracked:\n                    track.update(det, self.frame_id)\n                    activated_starcks.append(track)\n                else:\n                    track.re_activate(det, self.frame_id, new_id=False)\n                    refind_stracks.append(track)\n\n            for it in u_track:\n                track = r_tracked_stracks[it]\n                if not track.state == TrackState.Lost:\n                    track.mark_lost()\n                    lost_stracks.append(track)\n\n            '''Deal with unconfirmed tracks, usually tracks with only one beginning frame'''\n            detections = [detections[i] for i in u_detection]\n            # dists = matching.iou_distance(unconfirmed, detections)\n            dists = matching.center_distance(unconfirmed, detections)\n            matches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=50)\n            for itracked, idet in matches:\n                unconfirmed[itracked].update(detections[idet], self.frame_id)\n                activated_starcks.append(unconfirmed[itracked])\n            for it in u_unconfirmed:\n                track = unconfirmed[it]\n                track.mark_removed()\n                removed_stracks.append(track)\n\n            \"\"\" Step 4: Init new stracks\"\"\"\n            for inew in u_detection:\n                track = detections[inew]\n                if track.score < self.det_thresh:\n                    continue\n                track.activate(self.kalman_filter, self.frame_id)\n                activated_starcks.append(track)\n            \"\"\" Step 5: Update state\"\"\"\n            for track in self.lost_stracks:\n                if self.frame_id - track.end_frame > self.max_time_lost:\n                    track.mark_removed()\n                    removed_stracks.append(track)\n\n            # print('Ramained match {} s'.format(t4-t3))\n\n            self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\n            self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\n            self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n            self.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\n            self.lost_stracks.extend(lost_stracks)\n            self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n            self.removed_stracks.extend(removed_stracks)\n            self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n            # get scores of lost tracks\n            output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n\n            print('===========Frame {}=========='.format(self.frame_id))\n            print('Activated: {}'.format([track.track_id for track in activated_starcks]))\n            print('Refind: {}'.format([track.track_id for track in refind_stracks]))\n            print('Lost: {}'.format([track.track_id for track in lost_stracks]))\n            print('Removed: {}'.format([track.track_id for track in removed_stracks]))\n\n            return output_stracks\n\n\n    def joint_stracks(tlista, tlistb):\n        exists = {}\n        res = []\n        for t in tlista:\n            exists[t.track_id] = 1\n            res.append(t)\n        for t in tlistb:\n            tid = t.track_id\n            if not exists.get(tid, 0):\n                exists[tid] = 1\n                res.append(t)\n        return res\n\n\n    def sub_stracks(tlista, tlistb):\n        stracks = {}\n        for t in tlista:\n            stracks[t.track_id] = t\n        for t in tlistb:\n            tid = t.track_id\n            if stracks.get(tid, 0):\n                del stracks[tid]\n        return list(stracks.values())\n\n\n    def remove_duplicate_stracks(stracksa, stracksb):\n        # pdist = matching.iou_distance(stracksa, stracksb)\n        pdist = matching.center_distance(stracksa, stracksb)\n        pairs = np.where(pdist < 6)\n        dupa, dupb = list(), list()\n        for p, q in zip(*pairs):\n            timep = stracksa[p].frame_id - stracksa[p].start_frame\n            timeq = stracksb[q].frame_id - stracksb[q].start_frame\n            if timep > timeq:\n                dupb.append(q)\n            else:\n                dupa.append(p)\n        resa = [t for i, t in enumerate(stracksa) if not i in dupa]\n        resb = [t for i, t in enumerate(stracksb) if not i in dupb]\n        return resa, resb\n\n### 4.Result\n\n对于3D Detection来说，他评估的是projected ground plane occupancy map而不是bounding box\n\n对于跟踪就是常用的那几个指标。\n\n","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-02-02T03:29:48.218Z","updatedAt":"2024-02-04T03:20:40.029Z","trashed":false,"_rev":"mseKtXv3n"}