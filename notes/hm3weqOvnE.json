{"_id":"note:hm3weqOvnE","title":"MVFlow","content":"# MVFlow 论文阅读笔记\n\n>Engilberge M, Liu W, Fua P. Multi-view tracking using weakly supervised human motion prediction[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023: 1582-1592.\n\n## Introduction\n\n在拥挤的场景中，与单视图相比，多视图跟踪人员的方法有可能更好地处理闭塞。他们通常依赖于检测跟踪模式，即首先检测到人，然后将检测到的人联系起来。在本文中，我们认为更有效的方法是预测人们随时间的运动，并从中推断人们在各个帧中的存在。这使得可以在时间和单一时间框架的视图之间强制一致性。我们在PETS2009和WILDTRACK数据集上验证了我们的方法，并证明它优于最先进的方法。\n\n## Method\n\n大多数最新的方法依赖于tracking-by-dection。在最简单的形式中，检测步骤与关联步骤是断开的。在本节中，我们提出一种新颖的方法来拉近这两个步骤。首先，我们引入了一个检测网络，以弱监督的方式预测人流。然后，我们展示了如何修改现有的关联算法来利用预测流来生成明确的轨道\n\n首先定义human flow，对于给定的位置$i$，the flow$f_{i,j}^{t,t+1}$表示一个人从位置i移动到了位置j，作者采用网格表示法，如图，用一个9维向量表示。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic1.jpg?raw=true)\n\n本文所提出的模型由五个步骤组成，输入是一组不同视角的图片，每一个图片通过ResNet进行特征提取，所得到的特征图投影到ground plane，然后，空间聚合模块将不同角度的特征组合成人流，从两个时间步长的流中重建检测预测。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic1.5.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic2.jpg?raw=true)\n\nWhere: $g_{\\boldsymbol{\\theta}_0}(\\mathbf{I}_v^t)\\:\\in\\:\\mathbb{R}^{W/8\\times H/8\\times D}$ is the output of the ResNet parametrized by weights $\\theta_0.\\quad P(\\mathbf{F}_v^t,\\mathbf{C}_v)$ project features onto the groundplane. Temporal aggregation is achieved with a convolution parametrized by weights $\\theta_{1}$ and output $c_{\\boldsymbol{\\theta}_1}(\\mathbf{G}_v^t,\\mathbf{G}_v^{t+1})\\in\\mathbb{R}^{w\\times h\\times D^{\\prime}}.$ The spatial aggregation layer parametrized by weights $\\theta_{2}$ generates the human flow $s_{\\boldsymbol{\\theta}_2}(\\mathbf{G}^{t,t+1})\\in\\mathbb{R}^{w\\times h\\times9}$ .The detection heatmaps $\\mathbf{x}\\in\\mathbb{R}^{w\\times h}$ are reconstructed from the flow as follows,\n\n将所有特征根据通道维度进行拼接，由于相机参数并非完全准确，所以使用一个大核卷积$5\\times 5$,允许相邻特征之间的重新排列。之后为了处理遮挡，隐藏在某些视图中的对象仍然可以从其他视图中预测出来。为了简化这一过程，我们提出了一个多尺度模块:在粗水平上更容易检测遮挡，而精确定位需要精细水平特征。特征被pool到4个不同的大小，并通过四组卷积、批处理归一化和relu进行处理，每个尺度一个。然后将4个尺度表示升级到相同的维度，并与卷积层结合。最后，一个卷积层和一个s型激活函数将聚合特征转换成人流\n$f^{t-1,t}$\n\n首先，作者根据坐标位置生成一个检测图$y^t \\in (0,1)^{w\\times t}$,损失函数定义：\n$$\n\\begin{aligned}L=L_{\\det}(\\mathbf{x}^t,\\mathbf{y}^t)+L_{\\det}(\\mathbf{x}^{t+1},\\mathbf{y}^{t+1})+L_{\\text{cycle}}(f^{t,t+1},f^{t+1,t})\\end{aligned}\\\\L_{\\det}\\left(\\mathbf{x}^t,\\mathbf{y}^t\\right)=\\left(\\mathbf{x}^t-\\mathbf{y}^t\\right)^2+\\left(\\mathbf{\\bar{x}}^t-\\mathbf{y}^t\\right)^2\\\\L_\\text{cycle }(f^{t,t+1},f^{t+1,t})=\\sum_{j\\in G^t}\\sum_{k\\in\\mathcal{N}(j)}\\left(f_{j,k}^{t,t+1}-f_{k,j}^{t+1,t}\\right)^2\n$$\n\n由于帧率很高，人物基本不会移动，所以作者提出了一个新的函数,大概意思是如果目标在grid间移动，你还保持静止，就会给你施加很大的惩罚。\n$$\nL_{\\det}(\\mathbf{x}^t,\\mathbf{y}^t)=(\\left(\\mathbf{x}^t-\\mathbf{y}^t\\right)^2+(\\bar{\\mathbf{x}}^t-\\mathbf{y}^t)^2)\\times(1+\\lambda_r|\\mathbf{y}^t-\\mathbf{y}^{t+1}|)\n$$\n\n作者改进了KSP算法，他是一个图优化模型，每条边的代价表示为$c_{KSPFlow}\\left(e_{i,j}^t\\right)=-\\log\\left(\\frac{f_{i,j}^{t,t+1}}{1-f_{i,j}^{t,t+1}}\\right)$\n\nMuSSP是一个用最小成本流求解的方法，作者也是更新了他的代价函数，\n$$\n\\begin{aligned}c_{muSSPFlow}\\left(e_{i,j}^t\\right)&=-e^{-(\\delta_t-1)*\\sigma_t}*e^{-d(\\mathbf{x_i},\\mathbf{x_j})*\\sigma_d}\\\\&*e^{-d(\\mathbf{x_i}+\\delta_t*f_{i,j}^{t,t+1},\\mathbf{x_j})*\\sigma_f}\\end{aligned}\n$$\nwhere $\\delta_t$ is the temporal distance between detection $i$ and detection $j$ and $d(\\mathbf{x_i},\\mathbf{x_j})$ is the physical distance between them. We propose to add the term $d(\\mathbf{x_i}+\\delta_t*f_{i,j}^{t,t+1},\\mathbf{x_j})$ which model the distance between detection $j$ and the estimated position of detection $i$ in the future based on the flow prediction. $\\sigma_t,\\sigma_d$ and $\\sigma_f$ are the hyperparameters that respectively control the contribution of the temporal, spatial and motion-based distances between the detections.\n\n","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-01-16T13:43:01.426Z","updatedAt":"2024-01-21T02:39:30.612Z","trashed":false,"_rev":"emSPZHAvz"}