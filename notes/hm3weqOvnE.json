{"_id":"note:hm3weqOvnE","title":"MVFlow","content":"# MVFlow 论文阅读笔记\n\n>Engilberge M, Liu W, Fua P. Multi-view tracking using weakly supervised human motion prediction[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023: 1582-1592.\n\n## Introduction\n\n在拥挤的场景中，与单视图相比，多视图跟踪人员的方法有可能更好地处理遮挡。他们通常依赖于检测跟踪模式，即首先检测到人，然后将检测到的人联系起来。在本文中，我们认为更有效的方法是预测人们随时间的运动，并从中推断人们在各个帧中的存在。这使得可以在时间和单一时间框架的视图之间强制一致性。我们在PETS2009和WILDTRACK数据集上验证了我们的方法，并证明它优于最先进的方法。\n\n## Method\n\n大多数最新的方法依赖于tracking-by-dection。在最简单的形式中，检测步骤与关联步骤是断开的。在本节中，我们提出一种新颖的方法来拉近这两个步骤。首先，我们引入了一个检测网络，以弱监督的方式预测人流。然后，我们展示了如何修改现有的关联算法来利用预测流来生成明确的轨道\n\n首先定义human flow，对于给定的位置$i$，the flow$f_{i,j}^{t,t+1}$表示一个人从位置i移动到了位置j，作者采用网格表示法，如图，用一个9维向量表示。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic1.jpg?raw=true)\n\n本文所提出的模型由五个步骤组成，输入是一组不同视角的图片，每一个图片通过ResNet进行特征提取，所得到的特征图投影到ground plane，然后，空间聚合模块将不同角度的特征组合成人流，从两个时间步长的流中重建检测预测。\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic1.5.jpg?raw=true)\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pic2.jpg?raw=true)\n\nWhere: $g_{\\boldsymbol{\\theta}_0}(\\mathbf{I}_v^t)\\:\\in\\:\\mathbb{R}^{W/8\\times H/8\\times D}$ is the output of the ResNet parametrized by weights $\\theta_0.\\quad P(\\mathbf{F}_v^t,\\mathbf{C}_v)$ project features onto the groundplane. Temporal aggregation is achieved with a convolution parametrized by weights $\\theta_{1}$ and output $c_{\\boldsymbol{\\theta}_1}(\\mathbf{G}_v^t,\\mathbf{G}_v^{t+1})\\in\\mathbb{R}^{w\\times h\\times D^{\\prime}}.$ The spatial aggregation layer parametrized by weights $\\theta_{2}$ generates the human flow $s_{\\boldsymbol{\\theta}_2}(\\mathbf{G}^{t,t+1})\\in\\mathbb{R}^{w\\times h\\times9}$ .The detection heatmaps $\\mathbf{x}\\in\\mathbb{R}^{w\\times h}$ are reconstructed from the flow as follows,\n\n将所有特征根据通道维度进行拼接，由于相机参数并非完全准确，所以使用一个大核卷积$5\\times 5$,允许相邻特征之间的重新排列。之后为了处理遮挡，隐藏在某些视图中的对象仍然可以从其他视图中预测出来。为了简化这一过程，我们提出了一个多尺度模块:在粗水平上更容易检测遮挡，而精确定位需要精细水平特征。特征被pool到4个不同的大小，并通过四组卷积、批处理归一化和relu进行处理，每个尺度一个。然后将4个尺度表示升级到相同的维度，并与卷积层结合。最后，一个卷积层和一个s型激活函数将聚合特征转换成人流\n$f^{t-1,t}$\n\n首先，作者根据坐标位置生成一个检测图$y^t \\in (0,1)^{w\\times t}$,损失函数定义：\n$$\n\\begin{aligned}L=L_{\\det}(\\mathbf{x}^t,\\mathbf{y}^t)+L_{\\det}(\\mathbf{x}^{t+1},\\mathbf{y}^{t+1})+L_{\\text{cycle}}(f^{t,t+1},f^{t+1,t})\\end{aligned}\\\\L_{\\det}\\left(\\mathbf{x}^t,\\mathbf{y}^t\\right)=\\left(\\mathbf{x}^t-\\mathbf{y}^t\\right)^2+\\left(\\mathbf{\\bar{x}}^t-\\mathbf{y}^t\\right)^2\\\\L_\\text{cycle }(f^{t,t+1},f^{t+1,t})=\\sum_{j\\in G^t}\\sum_{k\\in\\mathcal{N}(j)}\\left(f_{j,k}^{t,t+1}-f_{k,j}^{t+1,t}\\right)^2\n$$\n\n由于帧率很高，人物基本不会移动，所以作者提出了一个新的函数,大概意思是如果目标在grid间移动，你还保持静止，就会给你施加很大的惩罚。\n$$\nL_{\\det}(\\mathbf{x}^t,\\mathbf{y}^t)=(\\left(\\mathbf{x}^t-\\mathbf{y}^t\\right)^2+(\\bar{\\mathbf{x}}^t-\\mathbf{y}^t)^2)\\times(1+\\lambda_r|\\mathbf{y}^t-\\mathbf{y}^{t+1}|)\n$$\n\n作者改进了KSP算法，他是一个图优化模型，每条边的代价表示为$c_{KSPFlow}\\left(e_{i,j}^t\\right)=-\\log\\left(\\frac{f_{i,j}^{t,t+1}}{1-f_{i,j}^{t,t+1}}\\right)$\n\nMuSSP是一个用最小成本流求解的方法，作者也是更新了他的代价函数，\n$$\n\\begin{aligned}c_{muSSPFlow}\\left(e_{i,j}^t\\right)&=-e^{-(\\delta_t-1)*\\sigma_t}*e^{-d(\\mathbf{x_i},\\mathbf{x_j})*\\sigma_d}\\\\&*e^{-d(\\mathbf{x_i}+\\delta_t*f_{i,j}^{t,t+1},\\mathbf{x_j})*\\sigma_f}\\end{aligned}\n$$\nwhere $\\delta_t$ is the temporal distance between detection $i$ and detection $j$ and $d(\\mathbf{x_i},\\mathbf{x_j})$ is the physical distance between them. We propose to add the term $d(\\mathbf{x_i}+\\delta_t*f_{i,j}^{t,t+1},\\mathbf{x_j})$ which model the distance between detection $j$ and the estimated position of detection $i$ in the future based on the flow prediction. $\\sigma_t,\\sigma_d$ and $\\sigma_f$ are the hyperparameters that respectively control the contribution of the temporal, spatial and motion-based distances between the detections.\n\n## Code\n\n### 1.数据加载环节\n\n最重要的就是dataset的构造，下面是dataloader的构造方法\n\n    def get_dataloader(data_arg):\n        # 这个没啥好说的\n        log.info(f\"Building Datasets\")\n        log.debug(f\"Data spec: {dict_to_string(data_arg)}\")\n        \n        # 获取数据集 这部分详细见1.1\n        train_datasets = get_datasets(data_arg, \"dataset\", data_arg[\"aug_train\"])\n        train_datasets_no_aug = get_datasets(data_arg, \"dataset\", False)\n        \n        val_datasets = get_datasets(data_arg, \"eval-dataset\", False)\n\n        # 分割数据集 这部分详见1.2\n        train_val_splits = [get_train_val_split_index(dataset, data_arg[\"split_proportion\"]) for dataset in train_datasets]\n\n        train_dataloaders = list()\n        val_dataloaders = list()\n\n        for dataset, dataset_no_aug, train_val_split in zip(train_datasets, train_datasets_no_aug, train_val_splits):\n            #Add train dataset (possibly subset) to train dataloaders\n            train_dataloaders.append(DataLoader(\n                Subset(dataset, train_val_split[0]),\n                shuffle=data_arg[\"shuffle_train\"],\n                batch_size=data_arg[\"batch_size\"],\n                collate_fn=dataset.collate_fn,\n                pin_memory=True,\n                num_workers=data_arg[\"num_workers\"]\n                )\n            )\n\n            #Add split part of the train dataset to validation dataloaders\n            if data_arg[\"split_proportion\"] < 1:\n                val_dataloaders.append(DataLoader(\n                    Subset(dataset_no_aug, train_val_split[1]),\n                    shuffle=False,\n                    batch_size=data_arg[\"batch_size\"],\n                    collate_fn=dataset.collate_fn,\n                    pin_memory=True,\n                    num_workers=data_arg[\"num_workers\"]\n                    )\n                )\n\n        #add validation only dataset to val dataloaders\n        for dataset in val_datasets:\n            val_dataloaders.append(DataLoader(\n                    Subset(dataset, list(range(len(dataset)))),\n                    shuffle=False,\n                    batch_size=data_arg[\"batch_size\"],\n                    collate_fn=dataset.collate_fn,\n                    pin_memory=True,\n                    num_workers=data_arg[\"num_workers\"]\n                )\n            )\n\n\n        return train_dataloaders, val_dataloaders\n    \n#### 1.1 dataset的构造\n\n在下面这一段代码中，有三个比较比较重要的点，分别是get_scene_set，heatmapbuilder....heatmap，FlowSceneSet这三个东西\n\n    def get_datasets(data_arg, data_arg_dict_key, use_aug):\n\n        sceneset_list = get_scene_set(data_arg, data_arg_dict_key)\n\n        if data_arg[\"hm_type\"] == \"density\":\n            data_arg[\"hm_builder\"] = heatmapbuilder.gaussian_density_heatmap\n        elif data_arg[\"hm_type\"] == \"center\":\n            data_arg[\"hm_builder\"] = heatmapbuilder.gausian_center_heatmap\n        elif data_arg[\"hm_type\"] == \"constant\":\n            data_arg[\"hm_builder\"] = heatmapbuilder.constant_center_heatmap\n        else:\n            log.error(f\"Unknown heatmap type {data_arg['hm_type']}\")\n\n        datasets = [FlowSceneSet(sceneset, data_arg, use_aug) for sceneset in sceneset_list] \n\n        return datasets\n        \n\n针对get_scene_set这一个东西，我们查看他的源码，可惜还是看不出什么。下面以wildtrack这一个类为例，看看里面是啥内容。\n\n    def get_scene_set(data_arg, data_arg_dict_key = \"dataset\"):\n\n\n        sceneset_list = []\n\n        if \"PETS\" in data_arg[data_arg_dict_key]:\n            for S, L, T in PET_SCENE_SET:\n                log.debug(f\"Adding PETS sequences {S}-{L}-{T} to the dataset\")\n                sceneset_list.append(petsdataset.PETSSceneSet(data_arg, pets_config_file, S, L, T))\n        if \"PETSeval\" in data_arg[data_arg_dict_key]:\n            log.debug(\"Adding PETS evaluation sequences (2-1-12_34) to the dataset\")\n            sceneset_list.append(petsdataset.PETSSceneSet(data_arg, pets_config_file, 2, 1, (12,34)))\n        if \"wild\" in data_arg[data_arg_dict_key]:\n            log.debug(\"Adding Wildtrack sequences to the dataset\")\n            sceneset_list.append(wildtrack.WildtrackSet(data_arg, wildtrack_config_file))\n        if \"wildext\" in data_arg[data_arg_dict_key]:\n            log.debug(\"Adding Wildtrack extended sequences to the dataset list\")\n            sceneset_list.append(wildtrack.WildtrackExtendedSet(data_arg, wildtrack_extended_config_file))\n\n        return sceneset_list\n\n首先，先查看wildtrack的继承的类SceneBaseSet\n\n    class SceneBaseSet():\n        \"\"\"\n        Parent class containing all the information regarding a single scene:\n            - frames\n            - groundtruth\n            - Homography to groundplane\n            - Region of interest\n            - Occlusion mask\n\n        The scene can contains multiple view. This class provide generic implementation and helper function.\n        Specific function can be overwritten by child class: for example homography can be computed from camera calibration or point in the image depending on the type of scene.\n        \"\"\"\n\n        def __init__(self, data_conf, scene_config_file):\n            super(SceneBaseSet, self).__init__()\n            # 一些通用的数据表示\n            self.data_conf = data_conf\n            # 世界的描述\n            self.scene_config = read_json_file(scene_config_file)\n\n            self.frame_original_size = self.scene_config[\"frame_size\"]\n            self.fps = self.scene_config[\"fps\"]\n\n            self.frame_input_size = data_conf[\"frame_input_size\"]\n            self.homography_input_size = data_conf[\"homography_input_size\"]\n            self.homography_output_size = data_conf[\"homography_output_size\"]\n            self.hm_size = data_conf[\"hm_size\"]\n            self.hm_image_size = data_conf[\"hm_image_size\"]\n            \n            # Id of the view from the same scene to process at the same time start a 0\n            self.view_IDs = data_conf[\"view_ids\"]\n\n            self.desired_fps = data_conf[\"desired_fps\"]\n\n            if \"scene_id\" in self.scene_config:\n                self.scene_id = self.scene_config[\"scene_id\"]\n            else:\n                self.scene_id = uuid.uuid4()\n\n            # extract region of interest and occluded area for view_IDs from data_conf\n\n            self.ROIs = [np.array(self.scene_config[\"roi_corner_points_per_view\"][view_id]) for view_id in self.view_IDs]\n            self.occluded_areas = [np.array(self.scene_config[\"occluded_area_corner_points_per_view\"][view_id]) for view_id in self.view_IDs]\n\n        def generate_scene_elements(self):\n            self.homographies = [self._get_homography(view_id) for view_id in self.view_IDs]\n\n            #by default we use the original fps\n            if self.desired_fps == -1:\n                self.desired_fps = self.fps\n\n            #it is only possible to reduce the framerate by skiping intermediate frame\n            assert self.desired_fps <= self.fps\n\n            #the desired fps rate must be a factor of the original fps\n            assert (self.fps /self.desired_fps) == int((self.fps /self.desired_fps))\n\n            self.reducing_factor = int(self.fps / self.desired_fps)        \n            \n            '''\n            对图片中的point进行resize操作，然后将这些点通过之前的变换矩阵投影到地平面上，\n            然后提取，提取图像中的ROI区域部分，就是有用的区域\n            '''\n            self.scene_ROI, self.scene_ROI_boundary = generate_scene_roi_from_view_rois(self.ROIs, self.homographies, self.frame_original_size, self.homography_input_size, self.homography_output_size, self.hm_size)\n\n        def log_init_completed(self):\n            log.debug(f\"Dataset from directory {self.scene_config['data_root']} containing {len(self)} frames loaded\")\n            log.debug(f\"Dataset contains {self.get_nb_view()} view, the following are used: {self.view_IDs}\")\n\n            if self.reducing_factor != 1:\n                log.debug(f\"Desired fps smaller than original one ({self.fps}). Set fps to {self.desired_fps} which corespond to a reducing factor of {self.reducing_factor}\")\n\n        def get(self, index, view_id):\n            # correct index to match desired fps\n            index = index * self.reducing_factor\n\n            frame = self.get_frame(index, view_id)\n            homography = self.get_homography(view_id)\n\n            return frame, homography\n\n        def get_frame(self, index, view_id):\n            return []\n\n        def get_gt(self, index):\n            # correct index to match desired fps\n            index = index * self.reducing_factor\n\n            gts = [self._get_gt(index, view_id) for view_id in self.view_IDs] \n            aggregated_gt = aggregate_multi_view_gt(gts, self.homographies, self.frame_original_size, self.homography_input_size, self.homography_output_size, self.hm_size)\n\n            return aggregated_gt\n\n        def get_gt_image(self, index, view_id):\n            index = index * self.reducing_factor\n\n            gt = self._get_gt(index, view_id)\n            gt_points, person_id = extract_points_from_gt(gt, self.hm_image_size, gt_original_size=self.frame_original_size)\n\n            return gt_points, person_id\n\n        def get_homography(self, view_id):\n            return self.homographies[view_id]\n\n        def get_ROI(self, view_id):\n            return self.ROIs[view_id]\n\n        def get_scene_ROI(self):\n            return self.scene_ROI, self.scene_ROI_boundary\n\n        def get_occluded_area(self, view_id):\n            return self.occluded_areas[view_id]\n\n        def get_nb_used_view(self):\n            return len(self.view_IDs)\n\n        def get_length(self):\n            log.error(\"Abstract class get_lentgth as be called, it should be overwriten it in child class\")\n            return -1\n\n        def __len__(self):\n            length = int(self.get_length() // self.reducing_factor)\n            return length\n\n\n\n然后就是wildtrackSet，他包含了wildtrack数据集中的所有信息\n\n    class WildtrackSet(SceneBaseSet):\n        def __init__(self, data_conf, scene_config_file):\n            super().__init__(data_conf, scene_config_file)\n\n            self.root_path = Path(self.scene_config[\"data_root\"])    \n\n            self.frame_dir_path = self.root_path / \"Image_subsets/\"\n            self.gt_dir_path = self.root_path / \"annotations_positions/\"\n\n            self.nb_frames = len([frame_path for frame_path in (self.gt_dir_path).iterdir() if frame_path.suffix == \".json\"])\n\n            self.calibs = load_calibrations(self.root_path) # （view, Calibaration类）\n\n            self.world_origin_shift = self.scene_config[\"world_origin_shift\"]\n            self.groundplane_scale = self.scene_config[\"grounplane_scale\"]\n\n            log.debug(f\"Dataset Wildtrack containing {self.nb_frames} frames from {self.get_nb_view()} views \")\n\n            #use parent class to generate ROI and occluded area maps\n            self.generate_scene_elements()\n            self.log_init_completed()\n\n        def get_frame(self, index, view_id):\n                \"\"\"\n                Read and return undistoreted frame coresponding to index and view_id.\n                The frame is return at the original resolution\n                \"\"\"\n                index = index * 5 \n\n                frame_path = self.frame_dir_path / \"C{:d}/{:08d}.png\".format(view_id + 1, index)\n\n                # log.debug(f\"pomelo dataset get frame {index} {view_id}\")\n                frame = get_frame_from_file(frame_path)\n\n                return frame\n\n        def _get_gt(self, index, view_id):\n\n            index = index * 5 \n\n            gt_path = self.gt_dir_path / \"{:08d}.json\".format(index)\n            gt = read_json(gt_path, self.calibs)[view_id]\n\n            return gt\n\n        def _get_homography(self, view_id):\n            \"\"\"\n            return the homography projecting the image to the groundplane.\n            It takes into account potential resizing of the image and\n            uses class variables:\n            frame_original_size, homography_input_size, homography_output_size,\n            as parameters\n            \"\"\"\n\n            # update camera parameter to take into account resizing from the image before homography is applied\n            K = geometry.update_K_after_resize(self.calibs[view_id].K, self.frame_original_size, self.homography_input_size)\n            H = geometry.get_ground_plane_homography(K, self.calibs[view_id].R, self.calibs[view_id].T, self.world_origin_shift, self.homography_output_size, self.groundplane_scale, grounplane_size_for_scale=[128,128])\n\n            return H\n\n        def get_nb_view(self):\n            return len(self.calibs)\n\n        def get_length(self):\n            return self.nb_frames\n\n这里补充一下作者获取homography matrix的推导流程，假设已经熟悉了补充资料中的内容，接下来进行详细推导。\n\n- 首先，需要将X，Y，Z的坐标进行转换，这里仅考虑X,Y两个维度的坐标。所以有Ki\n- 其次，需要将world_origin_shift当成默认的变换，所以需要放进T中\n- 之后，由图像坐标系变成像素坐标系，还需要进行变换，所以有update k\n- 貌似，这里没有考虑畸变之类的东西\n\n其中第二个heatmap感觉看的必要不大，可以参考CenterNet，直接看第三个FlowSceneSet\n\n\n    class FlowSceneSet(torch.utils.data.Dataset):\n    \n    def __init__(self, scene_set, data_conf, use_augmentation, compute_flow_stat=False):\n        self.scene_set = scene_set\n        \n        self.nb_view = len(data_conf[\"view_ids\"])\n        self.nb_frames = data_conf[\"nb_frames\"]\n        self.frame_interval = data_conf[\"frame_interval\"]\n        self.generate_flow_hm = False\n\n        log.debug(f\"Flow scene set containing {self.nb_view} view and {len(self.scene_set)} frames, will process {self.nb_frames} frame at a time\")\n\n        #original image and gt dimension needed for further rescaling\n        self.frame_original_size = self.scene_set.frame_original_size\n        self.frame_input_size = data_conf[\"frame_input_size\"]\n\n        #homography information are needed to generate hm\n        self.homography_input_size = data_conf[\"homography_input_size\"]\n        self.homography_output_size = data_conf[\"homography_output_size\"]\n\n        self.hm_builder = data_conf[\"hm_builder\"]\n        self.hm_radius = data_conf[\"hm_radius\"]\n        self.hm_size = data_conf[\"hm_size\"]\n        self.hm_image_size = data_conf[\"hm_image_size\"]\n        \n        #Reduce length by two to be able to return triplet of frames\n        self.total_number_of_frame = len(self.scene_set) - (self.nb_frames-1)*self.frame_interval\n        \n        self.img_transform = transforms.Compose(\n            [transforms.ToPILImage(),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n             transforms.Resize(self.frame_input_size)\n            ]\n        )\n\n        self.use_augmentation = use_augmentation\n\n        #list of augmentation and their probability\n        self.view_based_augs = [\n            (HomographyDataAugmentation(torchvision.transforms.RandomResizedCrop(self.frame_input_size)), 0.5),\n            (None, 0.5)\n            ]\n        self.scene_base_augs = [\n            (HomographyDataAugmentation(torchvision.transforms.RandomAffine(degrees = 45, translate = (0.2, 0.2), scale = (0.8,1.2), shear = 10)), 0.5),\n            (None, 0.5)\n            ]\n\n        if sum([x[1] for x in self.scene_base_augs]) != 1:\n            log.warning(f\"Scene based augmentation probability should sum up to one but is {sum([x[1] for x in self.scene_base_augs])}\")\n        if sum([x[1] for x in self.view_based_augs]) != 1:\n            log.warning(f\"View based augmentation probability should sum up to one but is {sum([x[1] for x in self.view_based_augs])}\")\n\n        if compute_flow_stat:\n            self.compute_flow_stat()\n\n        \n    def __getitem__(self, index):\n        multi_view_data = list()\n        \n        scene_base_aug = self.select_augmentation(*zip(*self.scene_base_augs))\n        ROIs_view = list()\n\n        for view_id in range(self.nb_view):\n            frame_dict = {}\n            frame_dict[\"view_id\"] = view_id\n\n            view_based_aug = self.select_augmentation(*zip(*self.view_based_augs))\n\n            for frame_id in range(self.nb_frames):\n                true_frame_id = index+frame_id*self.frame_interval\n                frame, homography = self.scene_set.get(true_frame_id, view_id)\n\n                #normalize and rescale frame\n                frame = self.img_transform(frame)\n\n                #TODO: get individual ROI and occluded area for each frame\n                homography = torch.from_numpy(homography).float()\n\n                gt_points_image, person_id_image = self.scene_set.get_gt_image(true_frame_id, view_id)\n\n                frame, homography, gt_points_image, person_id_image = self.apply_view_based_augmentation(frame, homography, gt_points_image, person_id_image, view_based_aug)\n                \n                hm_image  = self.build_heatmap(gt_points_image, self.hm_image_size, self.hm_radius).squeeze(0)\n\n                frame_dict[f\"frame_{frame_id}\"] = frame\n                frame_dict[f\"frame_{frame_id}_true_id\"] = true_frame_id\n                frame_dict[f\"hm_image_{frame_id}\"] = hm_image\n                frame_dict[f\"gt_points_image_{frame_id}\"] = gt_points_image\n                frame_dict[f\"person_id_image_{frame_id}\"] = person_id_image\n\n\n            homography = self.apply_scene_based_aug(homography, scene_base_aug)\n\n            frame_dict[\"homography\"] = homography\n            multi_view_data.append(frame_dict)\n\n            if view_based_aug is not None:\n                ROIs_view.append(view_based_aug.augment_gt_point_view_based(self.scene_set.get_ROI(view_id), None, filter_out_of_frame=False)[0])\n            else:\n                ROIs_view.append(self.scene_set.get_ROI(view_id))\n\n        # log.debug(multi_view_data)\n        multi_view_data = listdict_to_dictlist(multi_view_data)\n        multi_view_data = stack_tensors(multi_view_data)\n\n        #adding groundtuth shared between the view\n        for frame_id in range(self.nb_frames):\n            true_frame_id = index+frame_id*self.frame_interval\n\n            gt_points, person_id = aggregate_multi_view_gt_points(multi_view_data[f\"gt_points_image_{frame_id}\"], multi_view_data[f\"person_id_image_{frame_id}\"], multi_view_data[\"homography\"], self.hm_image_size, self.homography_input_size, self.homography_output_size, self.hm_size)\n            gt_points = np.rint(gt_points)\n\n            hm  = self.build_heatmap(gt_points, self.hm_size, self.hm_radius)\n\n            multi_view_data[f\"hm_{frame_id}\"] = hm\n            multi_view_data[f\"gt_points_{frame_id}\"] = gt_points\n            multi_view_data[f\"person_id_{frame_id}\"] = person_id\n\n        # ROI_mask, ROI_boundary = self.scene_set.get_scene_ROI()\n        ROI_mask, ROI_boundary = generate_scene_roi_from_view_rois(ROIs_view, multi_view_data[\"homography\"], self.frame_original_size, self.homography_input_size, self.homography_output_size, self.hm_size)\n        ROI_mask = torch.from_numpy(ROI_mask).float().unsqueeze(0)\n        boundary_mask = torch.from_numpy(ROI_boundary).float().unsqueeze(0)  \n\n        #adding additional scene data (roi, and boundary)\n        multi_view_data[\"ROI_mask\"] = ROI_mask\n        multi_view_data[\"ROI_boundary_mask\"] = boundary_mask\n        multi_view_data[\"scene_id\"] = self.scene_set.scene_id\n\n        return multi_view_data\n\n    def select_augmentation(self, aug_list, aug_prob):\n        if self.use_augmentation:\n            aug = random.choices(aug_list, weights=aug_prob)[0]\n        else:\n            aug = None\n\n        if aug is not None:\n            aug.reset()\n\n        return aug\n\n    def apply_view_based_augmentation(self, frame, homography, gt_points_image, person_id_image, view_based_aug):\n        if view_based_aug is None:\n            return frame, homography, gt_points_image, person_id_image\n\n        frame = view_based_aug(frame)\n        homography = view_based_aug.augment_homography_view_based(homography, self.homography_input_size)\n        gt_points_image, person_id_image = view_based_aug.augment_gt_point_view_based(gt_points_image, person_id_image)\n\n        return frame, homography, gt_points_image, person_id_image\n\n    def apply_scene_based_aug(self, homography, scene_base_aug):\n        if scene_base_aug is None:\n            return homography\n        \n        homography = scene_base_aug.augment_homography_scene_based(homography, self.homography_output_size)\n\n        return homography\n    \n    def build_heatmap(self, gt_points, hm_size, hm_radius):\n        \n        if len(gt_points) != 0:\n            gt_points = np.rint(gt_points).astype(int)\n        hm = self.hm_builder(hm_size, gt_points, hm_radius)\n        \n        return hm.unsqueeze(0)\n\n    def build_static_mask(self, points_flow, prob_masking):\n        \n        filtered_points = [point[0] for point in points_flow if point[1] == 4 and random.uniform(0, 1) > prob_masking]\n\n        return constant_center_heatmap(self.hm_size, filtered_points, self.hm_radius+1, value=0, background=\"one\")\n            \n\n    def augment_input_data(self, multi_view_data, aug_transform):\n\n        if aug_transform is not None:\n            for input_data in multi_view_data:\n                input_data[\"frame\"] = aug_transform(input_data[\"frame\"]).contiguous()\n                input_data[\"pre_frame\"] = aug_transform(input_data[\"pre_frame\"]).contiguous()\n                input_data[\"post_frame\"] = aug_transform(input_data[\"post_frame\"]).contiguous()\n\n                # input_data[\"hm\"] = aug_transform.transforms[0](input_data[\"hm\"]).contiguous()\n\n\n        return multi_view_data\n    \n    def disturb_points(self, gt_points):\n        \n        disturbed_points = list()\n\n        #Leave pre hm empty sometime\n        if np.random.random() > 0.1:\n            for point in gt_points:\n\n                #Drop gt point to generate False negative\n                if np.random.random() > self.pre_hm_fp_rate:\n                    disturbed_points.append(point)\n\n                #Add False positive near existing point\n                if np.random.random() < self.pre_hm_fn_rate:\n                    point_fn = point.copy()\n                    point_fn[0] = point_fn[0] + np.random.randn() * 0.05 * self.hm_size[0]\n                    point_fn[1] = point_fn[1] + np.random.randn() * 0.05 * self.hm_size[1]\n                    \n                    disturbed_points.append(point_fn)\n\n        disturbed_points = np.array(disturbed_points)\n\n        return disturbed_points\n\n    def build_tracking_gt(self, gt, pre_gt, pre_gt_points, pre_gt_point_mask, gt_points, gt_point_mask):\n\n        #Find all corespondence between pre point and current point\n        person_id = np.array([ann.id for ann in gt])\n        pre_person_id = np.array([ann.id for ann in pre_gt])\n\n        intersect_id, ind, pre_ind = np.intersect1d(person_id, pre_person_id, return_indices=True)\n\n        #Initialize map and mask\n        tracking_map = np.zeros((2, self.hm_size[0], self.hm_size[1]))\n        tracking_mask = np.zeros((1, self.hm_size[0], self.hm_size[1]))\n\n        for i in range(intersect_id.shape[0]):\n            frame_id = ind[i]\n            pre_id = pre_ind[i]\n\n            #filter out out of frame gt points\n            if gt_point_mask[frame_id] and pre_gt_point_mask[pre_id]:\n                frame_id = np.sum(gt_point_mask[:frame_id].astype(int))\n                pre_id = np.sum(pre_gt_point_mask[:pre_id].astype(int))\n            else:\n                #if person hidden in frame or previous frame we skip it\n                continue\n            \n            point = gt_points[frame_id].astype(int)\n            tracking_map[:, point[1], point[0]] = gt_points[frame_id] - pre_gt_points[pre_id]\n            tracking_mask[:, point[1], point[0]] = 1\n\n\n        tracking_map = torch.from_numpy(tracking_map).to(torch.float32)\n        tracking_mask = torch.from_numpy(tracking_mask).to(torch.float32)\n\n        return tracking_map, tracking_mask\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n    \n    def __len__(self):\n        return self.total_number_of_frame\n\n    @staticmethod\n    def collate_fn(batch):\n        \n        #Merge dictionnary\n        batch = listdict_to_dictlist(batch)\n        batch = stack_tensors(batch)\n\n        collate_dict = PinnableDict(batch)\n\n        log.spam(f\"collate_dict {type(collate_dict)}\")\n\n        return collate_dict\n\n\n    def compute_flow_stat(self):\n        ROI_mask, ROI_boundary = self.scene_set.get_scene_ROI()\n\n        flow_count_per_channel = defaultdict(int) \n        total_move_length = list()\n        nb_overlaping_gt = 0\n        for index in range(self.total_number_of_frame):\n            index_next = index + self.frame_interval\n\n            gt_points, person_id = extract_points_from_gt(self.scene_set.get_gt(index), self.hm_size)\n            next_gt_points, next_person_id = extract_points_from_gt(self.scene_set.get_gt(index_next), self.hm_size)\n\n            gt_points = np.rint(gt_points).astype(int)\n            next_gt_points = np.rint(next_gt_points).astype(int)\n\n            nb_overlaping_gt += gt_points.shape[0]-np.unique(gt_points, axis=0).shape[0]\n            _, flow_gt_list, move_length = generate_flow(gt_points, person_id, next_gt_points, next_person_id, ROI_mask, hm_radius=-1, generate_hm=False)\n            \n            total_move_length.extend(move_length)\n                       \n            for (pos, flow_channel) in flow_gt_list:\n                flow_count_per_channel[flow_channel] += 1\n\n        \n        for k, v in flow_count_per_channel.items():\n            if v != 0:\n                log.info(f\"flow stats channel {k} : {v}\")\n\n        log.info(f\"Motion of len 0: {len([x for x in total_move_length if x < 1])}\")\n        log.info(f\"Motion of len 1: {len([x for x in total_move_length if x < 2 and x >= 1])}\")\n        log.info(f\"Motion of len 2: {len([x for x in total_move_length if x < 3 and x >= 2])}\")\n        log.info(f\"Motion of len 3: {len([x for x in total_move_length if x < 4 and x >= 3])}\")\n        log.info(f\"Motion of len 4: {len([x for x in total_move_length if x < 5 and x >= 4])}\")\n        log.info(f\"Motion of len >5: {len([x for x in total_move_length if x >= 5])}\")\n\n        log.info(f\"Number of person in the gt overlapping {nb_overlaping_gt}\")\n\n#### 1.2 数据增强的构造\n\n\n## 2. 模型搭建\n\n    class MultiNet(torch.nn.Module):\n    def __init__(self, hm_size, homography_input_size, homography_output_size, nb_ch_out=10, nb_view=3):\n        super(MultiNet, self).__init__()\n        \n        self.nb_view = nb_view\n\n        self.homography_input_size = homography_input_size\n        self.homography_output_size = homography_output_size\n\n        self.hm_size = hm_size\n\n        resnet = models.resnet34(pretrained=True)\n        self.frontend = torch.nn.Sequential(*list(resnet.children())[:-4])\n        frontend_feature_outsize = 128\n        backend_feat = [128,128,64,32]\n        multiview_features = 128\n        output_layer = 32\n\n        self.multiview_layer_front = torch.nn.Conv2d(frontend_feature_outsize*2, multiview_features, kernel_size=1)\n\n        self.multiview_layer_back = torch.nn.Sequential(*[torch.nn.Conv2d(multiview_features*self.nb_view, multiview_features*2, kernel_size=5, padding=2),#torch.nn.Dropout(p=0.5),\n        torch.nn.BatchNorm2d(multiview_features*2),\n        torch.nn.ReLU(inplace=True),\n        MultiScaleBackend(multiview_features*2, backend_feat, out_features=output_layer) #torch.nn.Conv2d(1024, 64, kernel_size=1)#None#dla34up(64)\n        ])\n\n        self.output_layer = torch.nn.Conv2d(output_layer, nb_ch_out, kernel_size=3 ,padding=1)\n        self.logit_f = torch.nn.Sigmoid()\n\n\n\n    def forward(self, x_prev, x, h_groundplane, roi):\n        B, V, C, H, W = x_prev.shape\n        x_prev = x_prev.view(B*V,C,H,W)\n        x = x.view(B*V,C,H,W)\n\n        x_prev = self.frontend(x_prev) # 就是ResNet34  # B*V,C',H/8,W/8\n        x = self.frontend(x) # B*V,C',H/8,W/8\n\n        x_forw = torch.cat((x_prev,x),1)  # B*V,2C',H/8,W/8\n        x_inv = torch.cat((x, x_prev),1)  \n\n        #Reshape to have batchdimension before grounplane projection\n        # B,V,2C',H/8,W/8\n        x_forw = x_forw.view(B,V,x_forw.shape[-3],x_forw.shape[-2],x_forw.shape[-1]) \n        x_inv = x_inv.view(B,V,x_inv.shape[-3],x_inv.shape[-2],x_inv.shape[-1]) \n        \n        # project\n        # B,V,2c',H/8,W/8\n        x_forw = project_to_ground_plane_pytorch(x_forw, h_groundplane,  self.homography_input_size, self.homography_output_size, self.hm_size)\n        x_inv = project_to_ground_plane_pytorch(x_inv, h_groundplane,  self.homography_input_size, self.homography_output_size, self.hm_size)\n        \n        # merge batch and view dimension for fronted of multiview aggreagation\n        x_forw = x_forw.view(B*V,x_forw.shape[-3],x_forw.shape[-2],x_forw.shape[-1]) \n        x_inv = x_inv.view(B*V,x_inv.shape[-3],x_inv.shape[-2],x_inv.shape[-1]) \n    \n        # 时间维度信息提取\n        x_inv = self.multiview_layer_front(x_inv)\n        x_forw = self.multiview_layer_front(x_forw)\n\n        x_inv = x_inv.view(B,V*x_inv.shape[1],*x_inv.shape[2:])\n        x_forw = x_forw.view(B,V*x_forw.shape[1],*x_forw.shape[2:])\n        \n        # 空间维度信息提取\n        x_inv = self.multiview_layer_back(x_inv)\n        x_forw = self.multiview_layer_back(x_forw)\n\n        \n        x_forw = self.output_layer(x_forw)\n        x_forw = self.logit_f(x_forw)\n        \n        x_inv = self.output_layer(x_inv)\n        x_inv = self.logit_f(x_inv)\n\n        return x_forw, x_inv\n\n\n    def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n        if dilation:\n            d_rate = 2\n        else:\n            d_rate = 1\n        layers = []\n        for v in cfg:\n            if v == 'M':\n                layers += [torch.nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = torch.nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n                if batch_norm:\n                    layers += [conv2d, torch.nn.BatchNorm2d(v), torch.nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, torch.nn.ReLU(inplace=True)]\n                in_channels = v\n        return layers\n\n\n    class MultiScaleBackend(torch.nn.Module):\n        def __init__(self, features, backend_feat, out_features=64, nb_sizes=4):\n            super(MultiScaleBackend, self).__init__()\n            self.sizes = [2**i for i in list(range(8))[::-1][:nb_sizes]]\n            self.backend_feat  = backend_feat #[512,256,128,64]\n\n            self.scales = torch.nn.ModuleList([self._make_scale(features, self.backend_feat, size) for size in self.sizes])\n            self.bottleneck = torch.nn.Conv2d(self.backend_feat[-1] * nb_sizes, out_features, kernel_size=1)\n            self.relu = torch.nn.ReLU()\n\n        def __make_weight(self,feature,scale_feature):\n            weight_feature = feature - scale_feature\n            return F.sigmoid(self.weight_net(weight_feature))\n\n        def _make_scale(self, features, backend_feat, size):\n            layers = []\n\n            layers += [torch.nn.AdaptiveAvgPool2d(output_size=(size, size))]\n            layers += make_layers(backend_feat, in_channels=features, batch_norm=True, dilation=True)\n            #torch.nn.Conv2d(features, features, kernel_size=1, bias=False)\n            return torch.nn.Sequential(*layers)\n\n        def forward(self, feats):\n            h, w = feats.size(2), feats.size(3)\n            multi_scales = [F.interpolate(stage(feats), size=(h, w), mode='bilinear') for stage in self.scales]\n            bottle = self.bottleneck(torch.cat(multi_scales, 1))\n\n            return self.relu(bottle)\n\n## 3.损失计算\n\n    import numpy as np\n    import torch\n\n    class MSEwithROILoss(torch.nn.Module):\n        def __init__(self, reweighting_factor):\n            super(MSEwithROILoss, self).__init__()\n            self.mse = torch.nn.MSELoss(reduction=\"none\")\n            self.reweighting_factor = reweighting_factor\n\n        def forward(self, pred, target, roi_mask, reweighting=None):\n            loss = self.mse(pred, target) * roi_mask\n\n            if reweighting is not None:\n                loss = loss + self.reweighting_factor*loss*reweighting\n\n            return loss.sum()\n\n    class ConsistencywithROILoss(torch.nn.Module):\n        def __init__(self, reweighting_factor):\n            super(ConsistencywithROILoss, self).__init__()\n\n            self.reweighting_factor = reweighting_factor\n            self.loss = torch.nn.MSELoss(reduction=\"none\")\n\n        def forward(self, pred1, pred2, roi_mask, reweighting=None):\n\n\n            loss = self.loss(torch.clamp(pred1, 1e-10, 1-1e-10), torch.clamp(pred2, 1e-10, 1-1e-10))  * roi_mask\n\n            if reweighting is not None:\n                loss = loss + self.reweighting_factor*loss*reweighting\n\n            return loss.sum()\n\n\n    class FlowLossProb(torch.nn.Module):\n        def __init__(self, data_spec, loss_spec, stats_name=\"flow\"):\n            super(FlowLossProb, self).__init__()\n\n            self.criterion = MSEwithROILoss(loss_spec[\"reweigthing_factor\"])\n            self.consitency_criterion = ConsistencywithROILoss(loss_spec[\"reweigthing_factor\"])\n\n            self.stats_name = stats_name\n\n        def compute_flow_consistency_loss(self, flow, flow_inverse, roi_mask, boundary_mask, reweighting):\n\n            if reweighting is not None:\n                loss_consistency = self.consitency_criterion(flow[:,0,1:,1:], flow_inverse[:,8,:-1,:-1], roi_mask[:,0,1:,1:], reweighting=reweighting[:,0,1:,1:]) \\\n                    + self.consitency_criterion(flow[:,1,1:,:], flow_inverse[:,7,:-1,:], roi_mask[:,0,1:,:], reweighting=reweighting[:,0,1:,:]) \\\n                    + self.consitency_criterion(flow[:,2,1:,:-1], flow_inverse[:,6,:-1,1:], roi_mask[:,0,1:,:-1], reweighting=reweighting[:,0,1:,:-1]) \\\n                    + self.consitency_criterion(flow[:,3,:,1:], flow_inverse[:,5,:,:-1], roi_mask[:,0,:,1:], reweighting=reweighting[:,0,:,1:]) \\\n                    + self.consitency_criterion(flow[:,4,:,:], flow_inverse[:,4,:,:], roi_mask[:,0,:,:], reweighting=reweighting[:,0,:,:]) \\\n                    + self.consitency_criterion(flow[:,5,:,:-1], flow_inverse[:,3,:,1:], roi_mask[:,0,:,:-1], reweighting=reweighting[:,0,:,:-1]) \\\n                    + self.consitency_criterion(flow[:,6,:-1,1:], flow_inverse[:,2,1:,:-1], roi_mask[:,0,:-1,1:], reweighting=reweighting[:,0,:-1,1:]) \\\n                    + self.consitency_criterion(flow[:,7,:-1,:], flow_inverse[:,1,1:,:], roi_mask[:,0,:-1,:], reweighting=reweighting[:,0,:-1,:]) \\\n                    + self.consitency_criterion(flow[:,8,:-1,:-1], flow_inverse[:,0,1:,1:], roi_mask[:,0,:-1,:-1], reweighting=reweighting[:,0,:-1,:-1])\n            else:\n                loss_consistency = self.consitency_criterion(flow[:,0,1:,1:], flow_inverse[:,8,:-1,:-1], roi_mask[:,0,1:,1:]) \\\n                    + self.consitency_criterion(flow[:,1,1:,:], flow_inverse[:,7,:-1,:], roi_mask[:,0,1:,:]) \\\n                    + self.consitency_criterion(flow[:,2,1:,:-1], flow_inverse[:,6,:-1,1:], roi_mask[:,0,1:,:-1]) \\\n                    + self.consitency_criterion(flow[:,3,:,1:], flow_inverse[:,5,:,:-1], roi_mask[:,0,:,1:]) \\\n                    + self.consitency_criterion(flow[:,4,:,:], flow_inverse[:,4,:,:], roi_mask[:,0,:,:]) \\\n                    + self.consitency_criterion(flow[:,5,:,:-1], flow_inverse[:,3,:,1:], roi_mask[:,0,:,:-1]) \\\n                    + self.consitency_criterion(flow[:,6,:-1,1:], flow_inverse[:,2,1:,:-1], roi_mask[:,0,:-1,1:]) \\\n                    + self.consitency_criterion(flow[:,7,:-1,:], flow_inverse[:,1,1:,:], roi_mask[:,0,:-1,:]) \\\n                    + self.consitency_criterion(flow[:,8,:-1,:-1], flow_inverse[:,0,1:,1:], roi_mask[:,0,:-1,:-1])\n\n            return loss_consistency\n\n        def compute_boundary_consistency(self, flow, flow_inverse, mask_boundary, roi_mask):\n            exiting_consistency = self.consitency_criterion(flow[0,10], flow_inverse[0,9], roi_mask*mask_boundary)\n\n            return exiting_consistency\n\n\n        def forward(self, input_data, output_flow):\n            roi_mask = input_data[\"ROI_mask\"]\n            boundary_mask = input_data[\"ROI_boundary_mask\"]\n\n            hm_1 = input_data[\"hm_1\"]\n            hm_0 = input_data[\"hm_0\"]\n            hm_2 = input_data[\"hm_2\"]\n\n            post_reweighting = torch.abs(hm_1 - hm_2)\n            pre_reweighting = torch.abs(hm_0 - hm_1)\n\n            stats = {}\n\n            loss_rec_1pf = self.criterion(output_flow[\"rec_1pf\"], hm_1, roi_mask, reweighting=pre_reweighting) / 4\n            loss_rec_1pb = self.criterion(output_flow[\"rec_1pb\"], hm_1, roi_mask, reweighting=pre_reweighting) / 4\n            loss_rec_1nf = self.criterion(output_flow[\"rec_1nf\"], hm_1, roi_mask, reweighting=post_reweighting) / 4\n            loss_rec_1nb = self.criterion(output_flow[\"rec_1nb\"], hm_1, roi_mask, reweighting=post_reweighting) / 4\n\n            loss_rec_1o = loss_rec_1pf + loss_rec_1pb + loss_rec_1nf + loss_rec_1nb\n\n            stats = {**stats,\n                self.stats_name + \"_loss_rec_1pf\" : loss_rec_1pf.item(),\n                self.stats_name + \"_loss_rec_1pb\" : loss_rec_1pb.item(),\n                self.stats_name + \"_loss_rec_1nf\" : loss_rec_1nf.item(),\n                self.stats_name + \"_loss_rec_1nb\" : loss_rec_1nb.item(),\n                self.stats_name + \"_loss_rec_1o\" : loss_rec_1o.item()\n                }\n\n\n            loss_rec_0f = self.criterion(output_flow[\"rec_0f\"], hm_0, roi_mask, reweighting=pre_reweighting) / 2 \n            loss_rec_0b = self.criterion(output_flow[\"rec_0b\"], hm_0, roi_mask, reweighting=pre_reweighting) / 2\n            # loss_prev_inverse = 0\n            loss_rec_2f = self.criterion(output_flow[\"rec_2f\"], hm_2, roi_mask, reweighting=post_reweighting) / 2\n            loss_rec_2b = self.criterion(output_flow[\"rec_2b\"], hm_2, roi_mask, reweighting=post_reweighting) / 2\n            # loss_post_inverse = 0\n\n            loss_rec_0o = loss_rec_0f + loss_rec_0b\n            loss_rec_2o = loss_rec_2f + loss_rec_2b\n\n\n            stats = {**stats,\n                self.stats_name + \"_loss_rec_0f\" : loss_rec_0f.item(),\n                self.stats_name + \"_loss_rec_0b\" : loss_rec_0b.item(),\n                self.stats_name + \"_loss_rec_2f\" : loss_rec_2f.item(),\n                self.stats_name + \"_loss_rec_2b\" : loss_rec_2b.item(),\n                self.stats_name + \"_loss_rec_0o\" : loss_rec_0o.item(),\n                self.stats_name + \"_loss_rec_2o\" : loss_rec_2o.item()\n                }\n\n            #Flow temporal consistency loss\n            loss_consistency_flow_0_1 = self.compute_flow_consistency_loss(output_flow[\"flow_0_1f\"], output_flow[\"flow_1_0b\"], roi_mask, boundary_mask, reweighting=None) / 9\n            loss_consistency_flow_1_2 = self.compute_flow_consistency_loss(output_flow[\"flow_1_2f\"], output_flow[\"flow_2_1b\"], roi_mask, boundary_mask, reweighting=None) / 9\n\n            loss_consistency = loss_consistency_flow_0_1 + loss_consistency_flow_1_2\n\n            stats = {**stats,\n                self.stats_name + \"_loss_consistency_flow_0_1\" : loss_consistency_flow_0_1.item(),\n                self.stats_name + \"_loss_consistency_flow_1_2\" : loss_consistency_flow_1_2.item(),\n                self.stats_name + \"_loss_consistency\" : loss_consistency.item()\n                }\n\n            total_loss = loss_rec_1o + loss_rec_0o + loss_rec_2o + loss_consistency \n\n\n            stats = {**stats,\n                \"loss_\"+self.stats_name : total_loss.item(),\n                }\n\n            return {\"loss\":total_loss, \"stats\":stats}\n\n\n    class MultiLoss(torch.nn.Module):\n        def __init__(self, flow_loss):\n            super(MultiLoss, self).__init__()\n\n            self.flow_loss = flow_loss\n\n        def forward(self, input_data, output):\n\n            loss = 0\n            stats = {}\n\n            if self.flow_loss is not None:\n                flow = self.flow_loss(input_data, output[\"flow_pred\"])\n                loss = loss + flow[\"loss\"]\n                stats = {**stats, **flow[\"stats\"]}\n\n            stats = {**stats, \"loss\":loss}\n\n            return {\"loss\":loss, \"stats\":stats}    \n\n\n# 补充资料\n\n## 1.相机标定（透视变换）\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Track/CrossView/MVFlow/pp.webp?raw=true)\n\n### 1.1 像素坐标系到相机坐标系\n\n图像坐标系平面可以认为是理想的成像面，像素坐标系则是相机传感器平面，理想情况下图像坐标平面和像素坐标平面在同一平面，则从图像坐标转换到像素坐标有两个步骤：\n\n（1）缩放，在前面的坐标系中，坐标系单位是统一的实际距离单位，但是像素坐标系的单位不一样，因此要缩放，换算到像素坐标系；\n\n（2）平移，从图像坐标系原点到像素坐标系原点。\n\n则变换为：\n\n$$\n\\left.\\left[\\begin{array}{c}u_p\\\\v_p\\\\1\\end{array}\\right.\\right]=\\left[\\begin{array}{ccc}s_x&0&c_x\\\\0&s_y&c_y\\\\0&0&1\\end{array}\\right]\\left[\\begin{array}{c}x_p\\\\y_p\\\\1\\end{array}\\right]\n$$\n\n### 1.2 从图像坐标转换到相机坐标\n\n根据小孔成像模型，则由P的相机坐标$（X_{pc}, Y_{pc}, Z_{pc}）$到图像坐标$（x_p, y_p）$有,其实就是三角形的相似：\n\n$$\n\n\\begin{aligned}y_p&=f_y\\times\\frac{Ypc}{Zpc}\\\\x_p&=f_x\\times\\frac{Xpc}{Zpc}\\end{aligned}\n\n$$\n$$\n\\left.\\left[\\begin{array}{c}x_p\\\\y_p\\\\1\\end{array}\\right.\\right]=\\frac{1}{Z_{pc}}\\left[\\begin{array}{ccc}f_x&0&0\\\\0&f_y&0\\\\0&0&1\\end{array}\\right]\\left[\\begin{array}{c}X_{pc}\\\\Y_{pc}\\\\Z_{pc}\\end{array}\\right]\n\n$$\n\n### 1.3 从相机坐标转换到世界坐标\n\n点P从世界坐标$（X_{pw}, Y_{pw}, Z_{pw}）$到相机坐标$（X_{pc}, Y_{pc}, Z_{pc}）$的变换包含了一个三维的旋转变换R以及一个平移变换t，齐次表达式为：\n\n$$\n\n\\left.\\left[\\begin{array}{c}Xpc\\\\Ypc\\\\Zpc\\end{array}\\right.\\right]=[R\\mid t]\\left[\\begin{array}{c}Xpw\\\\Ypw\\\\Zpw\\\\1\\end{array}\\right]\n\n$$\n\n其中R为3×3大小的旋转矩阵，t为3×1的平移矩阵，变换矩阵P为3×4大小的矩阵：\n\n$$\n\nP=[R\\mid t]\n\n$$\n\n最终的公式\n\n$$\n\n\\left.s\\left[\\begin{array}{c}u_p\\\\v_p\\\\1\\end{array}\\right.\\right]=\\left[\\begin{array}{ccc}f_xs_x&f_yc&cx\\\\0&f_ys_y&cy\\\\0&0&1\\end{array}\\right]\\left[R\\mid t\\right]\\left[\\begin{array}{c}Xpw\\\\Ypw\\\\Zpw\\\\1\\end{array}\\right]\n\n$$","tags":[],"folderPathname":"/Computer Vision/Track/cross_view","data":{},"createdAt":"2024-01-16T13:43:01.426Z","updatedAt":"2024-03-13T06:55:42.281Z","trashed":false,"_rev":"4gMSb--Ac"}