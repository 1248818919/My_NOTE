{"_id":"note:nRRXpwnHak","title":"Transpose Convolution","content":"# Transpose Convolution 转置卷积\n\nhttps://blog.csdn.net/qq_39478403/article/details/121181904\n\n## 1.转置卷积的背景\n\n通常，对图像进行多次卷积运算后，特征图的尺寸会不断缩小。而对于某些特定任务 (如 图像分割 和 图像生成 等)，需将图像恢复到原尺寸再操作。这个将图像由小分辨率映射到大分辨率的尺寸恢复操作，叫做 上采样 (Upsample)，如下图所示:\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic1.png?raw=true)\n\n上采样方法有很多，详见《【图像处理】详解 最近邻插值、线性插值、双线性插值、双三次插值_闻韶CSDN+博客》。然而，这些上采样方法都是基于人们的先验经验来设计的，在很多场景中效果并不理想 (如 规则固定、不可学习)。因此，我们希望神经网络自己学习如何更好地插值，即接下来要介绍的 转置卷积。\n\n## 2.转置卷积的应用\n\n曾经，转置卷积 又称反卷积 (Deconvolution)。与传统的上采样方法相比，转置卷积的上采样方式 并非预设的插值方法，而是同标准卷积一样，具有可学习的参数，可通过网络学习来获取最优的上采样方式。\n\n转置卷积 在某些特定领域具有广泛应用，比如：\n\n> - 在 DCGAN，生成器将随机值转变为一个全尺寸图片，此时需用到转置卷积。\n- 在语义分割中，会在编码器中用卷积层提取特征，然后在解码器中恢复原先尺寸，从而对原图中的每个像素分类。该过程同样需用转置卷积。经典方法有 FCN 和 U-Net。\n- CNN 可视化：通过转置卷积将 CNN 的特征图还原到像素空间，以观察特定特征图对哪些模式的图像敏感。\n\n## 3.转置卷积的推导\n\n定义一个 $4 \\times 4$ 输入矩阵 input:\n$$\n\\text { input }=\\left[\\begin{array}{cccc}\nx_1 & x_2 & x_3 & x_4 \\\\\nx_6 & x_7 & x_8 & x_9 \\\\\nx_{10} & x_{11} & x_{12} & x_{13} \\\\\nx_{14} & x_{15} & x_{16} & x_{17}\n\\end{array}\\right]\n$$\n\n再定义一个 $3 \\times 3$ 标准卷积核 kernel:\n$$\n\\text { kernel }=\\left[\\begin{array}{lll}\nw_{0,0} & w_{0,1} & w_{0,2} \\\\\nw_{1,0} & w_{1,1} & w_{1,2} \\\\\nw_{2,0} & w_{2,1} & w_{2,2}\n\\end{array}\\right]\n$$\n\n设 步长 stride $=1$ 、填充 padding $=0$ ，则按 \"valid\" 卷积模式，可得 $2 \\times 2$ 输出矩阵 output:\n$$\n\\text { output }=\\left[\\begin{array}{ll}\ny_0 & y_1 \\\\\ny_2 & y_3\n\\end{array}\\right]\n$$\n\n这里，换一个表达方式，将输入矩阵 input 和输出矩阵 output 展开成 $16 \\times 1$ 列向量 $X$ 和 $4 \\times 1$ 列向量 $Y$ ，可分别表示为:\n$$\n\\begin{aligned}\n& X=\\left[\\begin{array}{l}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_6 \\\\\nx_7 \\\\\nx_8 \\\\\nx_9 \\\\\nx_{10} \\\\\nx_{11} \\\\\nx_{12} \\\\\nx_{13} \\\\\nx_{14} \\\\\nx_{15} \\\\\nx_{16} \\\\\nx_{17}\n\\end{array}\\right] \\\\\n& Y=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n接着，再用矩阵运算来描述标准卷积运算，设有 新卷积核矩阵 C:\n$$\nY=C X\n$$\n\n经推导 (卷积运算关系)，可得 $4 \\times 16$ 稀疏矩阵 C:\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic2.png?raw=true)\n\n以下，用图 4 展示矩阵运算过程：\n\n![pic3](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic3.png?raw=true)\n\n而转置卷积其实就是要对这个过程进行逆运算，即通过 $C$ 和 $Y$ 得到 $X$ :\n$$\nX=C^T Y\n$$\n\n此时， $C^T$ 即为新的 $16 \\times 4$ 稀疏矩阵。以下通过图 5 展示转置后的卷积矩阵运算。此处，用于转置卷积的权重矩阵 $C^T$ 不一定来自于原卷积矩阵 $C$ (通常不会如此恰巧)，但其形状和原卷积矩阵 $C$ 的转置相同。\n\n![pic4](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic4.png?raw=true)\n\n最后，将 16×1 的输出结果重新排序，即可通过 2×2 输入矩阵得到 4×4 输出矩阵。\n\n## 4.转置卷积的输出\n\n### 4.1 stride = 1\n\n同样，使用上文中的 3×3 卷积核矩阵 C：\n\n![pic5](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic5.png?raw=true)\n\n输出矩阵 output 仍为:\n$$\n\\text { output }=\\left[\\begin{array}{ll}\ny_0 & y_1 \\\\\ny_2 & y_3\n\\end{array}\\right]\n$$\n\n将输出矩阵展开为 列向量 $\\mathbf{Y}$ :\n$$\nY=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n$$\n\n带入到上文中的转置卷积计算公式，则转置卷积的计算结果为：\n\n![pic6](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic6.png?raw=true)\n\n这其实等价于 先填充 padding=2 的输入矩阵 input：\n\n$$\n\\text { input }=\\left[\\begin{array}{cccccc}\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & y_0 & y_1 & 0 & 0 \\\\\n0 & 0 & y_2 & y_3 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n$$\n\n然后，转置标准卷积核 kernel:\n$$\n\\text { kernel }=\\left[\\begin{array}{lll}\nw_{2,2} & w_{2,1} & w_{2,0} \\\\\nw_{1,2} & w_{1,1} & w_{1,0} \\\\\nw_{0,2} & w_{0,1} & w_{0,0}\n\\end{array}\\right]\n$$\n\n最后，在 填零的输入矩阵 input 上使用 经转置的标准卷积核 kernel 执行 标准卷积运算，如图所示：\n\n![pic7](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic7.gif?raw=true)\n\n更一般地，对于卷积核尺寸 kernel size $=k$ ，步长 stride $=s=1$ ，填充 padding $=p=0$ 的转置卷积，其 等价的标准卷积 在原尺寸为 $i^{\\prime}$ 的输入矩阵上进行运算，输出特征图的尺寸 $o^{\\prime}$ 为:\n$$\no^{\\prime}=\\left(i^{\\prime}-1\\right)+k\n$$\n\n同时，等价的标准卷积的的输入矩阵 input 在卷积运算前，需先进行 padding' $=k-1$ 的填充，得到尺寸 $i^{\\prime \\prime}=i^{\\prime}+2(k-1)$ 。\n因此，实际上原计算公式为 (等价的标准卷积的步长 $s^{\\prime}=1$ ):\n$$\no^{\\prime}=\\frac{i^{\\prime \\prime}-k+2 p}{s^{\\prime}}+1=i^{\\prime}+2(k-1)-k+1=\\left(i^{\\prime}-1\\right)+k\n$$\n\n## 4.2 stride > 1\n\n在实际中，我们大多数时候会使用 stride $>1$ 的转置卷积，从而获得较大的上采样倍率。\n以下，令输入尺寸为 $5 \\times 5$ ，标准卷积核同上 kernel size $=k=3$ ，步长 stride $=s=2$ ，填充 padding $=p=0$ ，标准卷积运算后，输出矩阵尺寸为 $2 \\times 2$ :\n\n$$\nY=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n$$\n\n此处，转换后的稀疏矩阵尺寸变为 $25 \\times 4$ ，由于矩阵太大这里不展开进行罗列。最终转置卷积的结果为:\n\n![pic9](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic9.gif?raw=true)\n\n此时，等价于 输入矩阵同时添加了空洞和填充，再由仅转置的标准卷积核进行运算，过程如图 7 所示：\n","tags":[],"folderPathname":"/None class","data":{},"createdAt":"2023-12-23T12:13:09.324Z","updatedAt":"2023-12-23T12:46:07.303Z","trashed":false,"_rev":"WYnv1LiYd"}