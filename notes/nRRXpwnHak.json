{"_id":"note:nRRXpwnHak","title":"Transpose Convolution","content":"# Transpose Convolution 转置卷积\n\nhttps://blog.csdn.net/qq_39478403/article/details/121181904\n\n## 1.转置卷积的背景\n\n通常，对图像进行多次卷积运算后，特征图的尺寸会不断缩小。而对于某些特定任务 (如 图像分割 和 图像生成 等)，需将图像恢复到原尺寸再操作。这个将图像由小分辨率映射到大分辨率的尺寸恢复操作，叫做 上采样 (Upsample)，如下图所示:\n\n![pic1](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic1.png?raw=true)\n\n上采样方法有很多，详见《【图像处理】详解 最近邻插值、线性插值、双线性插值、双三次插值_闻韶CSDN+博客》。然而，这些上采样方法都是基于人们的先验经验来设计的，在很多场景中效果并不理想 (如 规则固定、不可学习)。因此，我们希望神经网络自己学习如何更好地插值，即接下来要介绍的 转置卷积。\n\n## 2.转置卷积的应用\n\n曾经，转置卷积 又称反卷积 (Deconvolution)。与传统的上采样方法相比，转置卷积的上采样方式 并非预设的插值方法，而是同标准卷积一样，具有可学习的参数，可通过网络学习来获取最优的上采样方式。\n\n转置卷积 在某些特定领域具有广泛应用，比如：\n\n> - 在 DCGAN，生成器将随机值转变为一个全尺寸图片，此时需用到转置卷积。\n- 在语义分割中，会在编码器中用卷积层提取特征，然后在解码器中恢复原先尺寸，从而对原图中的每个像素分类。该过程同样需用转置卷积。经典方法有 FCN 和 U-Net。\n- CNN 可视化：通过转置卷积将 CNN 的特征图还原到像素空间，以观察特定特征图对哪些模式的图像敏感。\n\n## 3.转置卷积的推导\n\n定义一个 $4 \\times 4$ 输入矩阵 input:\n$$\n\\text { input }=\\left[\\begin{array}{cccc}\nx_1 & x_2 & x_3 & x_4 \\\\\nx_6 & x_7 & x_8 & x_9 \\\\\nx_{10} & x_{11} & x_{12} & x_{13} \\\\\nx_{14} & x_{15} & x_{16} & x_{17}\n\\end{array}\\right]\n$$\n\n再定义一个 $3 \\times 3$ 标准卷积核 kernel:\n$$\n\\text { kernel }=\\left[\\begin{array}{lll}\nw_{0,0} & w_{0,1} & w_{0,2} \\\\\nw_{1,0} & w_{1,1} & w_{1,2} \\\\\nw_{2,0} & w_{2,1} & w_{2,2}\n\\end{array}\\right]\n$$\n\n设 步长 stride $=1$ 、填充 padding $=0$ ，则按 \"valid\" 卷积模式，可得 $2 \\times 2$ 输出矩阵 output:\n$$\n\\text { output }=\\left[\\begin{array}{ll}\ny_0 & y_1 \\\\\ny_2 & y_3\n\\end{array}\\right]\n$$\n\n这里，换一个表达方式，将输入矩阵 input 和输出矩阵 output 展开成 $16 \\times 1$ 列向量 $X$ 和 $4 \\times 1$ 列向量 $Y$ ，可分别表示为:\n$$\n\\begin{aligned}\n& X=\\left[\\begin{array}{l}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_6 \\\\\nx_7 \\\\\nx_8 \\\\\nx_9 \\\\\nx_{10} \\\\\nx_{11} \\\\\nx_{12} \\\\\nx_{13} \\\\\nx_{14} \\\\\nx_{15} \\\\\nx_{16} \\\\\nx_{17}\n\\end{array}\\right] \\\\\n& Y=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n接着，再用矩阵运算来描述标准卷积运算，设有 新卷积核矩阵 C:\n$$\nY=C X\n$$\n\n经推导 (卷积运算关系)，可得 $4 \\times 16$ 稀疏矩阵 C:\n\n![pic2](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic2.png?raw=true)\n\n以下，用图 4 展示矩阵运算过程：\n\n![pic3](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic3.png?raw=true)\n\n而转置卷积其实就是要对这个过程进行逆运算，即通过 $C$ 和 $Y$ 得到 $X$ :\n$$\nX=C^T Y\n$$\n\n此时， $C^T$ 即为新的 $16 \\times 4$ 稀疏矩阵。以下通过图 5 展示转置后的卷积矩阵运算。此处，用于转置卷积的权重矩阵 $C^T$ 不一定来自于原卷积矩阵 $C$ (通常不会如此恰巧)，但其形状和原卷积矩阵 $C$ 的转置相同。\n\n![pic4](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic4.png?raw=true)\n\n最后，将 16×1 的输出结果重新排序，即可通过 2×2 输入矩阵得到 4×4 输出矩阵。\n\n## 4.转置卷积的输出\n\n### 4.1 stride = 1\n\n同样，使用上文中的 3×3 卷积核矩阵 C：\n\n![pic5](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic5.png?raw=true)\n\n输出矩阵 output 仍为:\n$$\n\\text { output }=\\left[\\begin{array}{ll}\ny_0 & y_1 \\\\\ny_2 & y_3\n\\end{array}\\right]\n$$\n\n将输出矩阵展开为 列向量 $\\mathbf{Y}$ :\n$$\nY=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n$$\n\n带入到上文中的转置卷积计算公式，则转置卷积的计算结果为：\n\n![pic6](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic6.png?raw=true)\n\n这其实等价于 先填充 padding=2 的输入矩阵 input：\n\n$$\n\\text { input }=\\left[\\begin{array}{cccccc}\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & y_0 & y_1 & 0 & 0 \\\\\n0 & 0 & y_2 & y_3 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right]\n$$\n\n然后，转置标准卷积核 kernel:\n$$\n\\text { kernel }=\\left[\\begin{array}{lll}\nw_{2,2} & w_{2,1} & w_{2,0} \\\\\nw_{1,2} & w_{1,1} & w_{1,0} \\\\\nw_{0,2} & w_{0,1} & w_{0,0}\n\\end{array}\\right]\n$$\n\n最后，在 填零的输入矩阵 input 上使用 经转置的标准卷积核 kernel 执行 标准卷积运算，如图所示：\n\n![pic7](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic7.gif?raw=true)\n\n更一般地，对于卷积核尺寸 kernel size $=k$ ，步长 stride $=s=1$ ，填充 padding $=p=0$ 的转置卷积，其 等价的标准卷积 在原尺寸为 $i^{\\prime}$ 的输入矩阵上进行运算，输出特征图的尺寸 $o^{\\prime}$ 为:\n$$\no^{\\prime}=\\left(i^{\\prime}-1\\right)+k\n$$\n\n同时，等价的标准卷积的的输入矩阵 input 在卷积运算前，需先进行 padding' $=k-1$ 的填充，得到尺寸 $i^{\\prime \\prime}=i^{\\prime}+2(k-1)$ 。\n因此，实际上原计算公式为 (等价的标准卷积的步长 $s^{\\prime}=1$ ):\n$$\no^{\\prime}=\\frac{i^{\\prime \\prime}-k+2 p}{s^{\\prime}}+1=i^{\\prime}+2(k-1)-k+1=\\left(i^{\\prime}-1\\right)+k\n$$\n\n## 4.2 stride > 1\n\n在实际中，我们大多数时候会使用 stride $>1$ 的转置卷积，从而获得较大的上采样倍率。\n以下，令输入尺寸为 $5 \\times 5$ ，标准卷积核同上 kernel size $=k=3$ ，步长 stride $=s=2$ ，填充 padding $=p=0$ ，标准卷积运算后，输出矩阵尺寸为 $2 \\times 2$ :\n\n$$\nY=\\left[\\begin{array}{l}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{array}\\right]\n$$\n\n此处，转换后的稀疏矩阵尺寸变为 $25 \\times 4$ ，由于矩阵太大这里不展开进行罗列。最终转置卷积的结果为:\n\n![pic9](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic9.png?raw=true)\n\n此时，等价于 输入矩阵同时添加了空洞和填充，再由仅转置的标准卷积核进行运算，过程如图所示：\n\n![pic8](https://github.com/1248818919/My_NOTE/blob/master/assets/None%20class/Transpose%20Convolution/pic8.gif?raw=true)\n\n更一般地，对于卷积核尺寸 kernel size $=k$ ，步长 stride $=s>1$ ，填充 padding $=p=0$ 的转置卷积，其 等价的标准卷积 在原尺寸为 $i^{\\prime}$ 的输入矩阵上进行运算，输出特征图的尺寸 $o^{\\prime}$ 为:\n$$\no^{\\prime}=s\\left(i^{\\prime}-1\\right)+k\n$$\n\n同时，等价的标准卷积的输入矩阵 input 在卷积运算前，需先进行 padding' $=k-1$ 的填充; 然后，相邻元素间的空洞数为 $s-1$ ，共有 $i^{\\prime}-1$ 组空洞需插入；从而，实际尺寸为\n$$\ni^{\\prime \\prime}=i^{\\prime}+2(k-1)+\\left(i^{\\prime}-1\\right) \\times(s-1)=s \\times\\left(i^{\\prime}-1\\right)+2 k-1 \\text { 。 }\n$$\n\n因此，实际上原计算公式为 (等价的标准卷积的步长 $s^{\\prime}=1$ ):\n\n$$\no^{\\prime}=\\frac{i^{\\prime \\prime}-k+2 p}{s^{\\prime}}+1=s\\left(i^{\\prime}-1\\right)+2 k-1-k+1=s\\left(i^{\\prime}-1\\right)+k\n$$\n\n可见，通过控制步长 stride $=s$ 的大小可以控制上采样的倍率，而该参数类比于膨胀/空洞卷积的膨胀率/空洞数。\n\n## 5.小结\n\n注意: 矩阵中的实际权值不一定来自原始卷积矩阵。重要的是权重的排布是由卷积矩阵的转置得来的。转置卷积运算与普通卷积形成相同的连通性，但方向是反向的。\n\n我们可以用转置卷积来上采样，而转置卷积的权值是可学习的，所以无需一个预定义的插值方法。\n\n尽管它被称为转置卷积，但这并不意味着我们取某个已有的卷积矩阵并使用转置后的版本。重点是，与标准卷积矩阵 (一对多关联而不是多对一关联) 相比，输入和输出之间的关联是以反向的方式处理的\n\n因此，转置卷积不是卷积，但可以用卷积来模拟转置卷积。通过在输入矩阵的值间插入零值 (以及周围填零) 上采样输入矩阵，然后进行常规卷积 就会产生 与转置卷积相同的效果。你可能会发现一些文章用这种方式解释了转置卷积。但是，由于需要在常规卷积前对输入进行上采样，所以效率较低。\n\n注意: 转置卷积会导致生成图像中出现网格/棋盘效应 (checkerboard artifacts)，因此后续也存在许多针对该问题的改进工作。\n\n## 6.函数解析\n\n    torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0,   \n    groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n    \n- in_channels (int): Number of channels in the input image\n输入的通道数。\n- out_channels (int): Number of channels produced by the convolution\n输出的通道数。\n- kernel_size (int or tuple): Size of the convolving kernel\n卷积核的大小。\n- stride (int or tuple, optional): Stride of the convolution. Default: 1\n卷积的步幅。\n- padding (int or tuple, optional): dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0\n零填充将添加到输入中每个维度的两侧。\n- output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. Default: 0\n在输出形状的每个维度的一侧添加的附加大小。\n- groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n从输入通道到输出通道的阻塞连接数。\n- bias (bool, optional): If True, adds a learnable bias to the output. Default: True\n偏置。\n- dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n内核元素之间的间距\n\n","tags":[],"folderPathname":"/None class","data":{},"createdAt":"2023-12-23T12:13:09.324Z","updatedAt":"2023-12-24T05:48:04.236Z","trashed":false,"_rev":"Si-fldP06"}