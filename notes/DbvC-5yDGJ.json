{"_id":"note:DbvC-5yDGJ","title":"LSS","content":"# LSS 论文阅读笔记\n\n> Philion J, Fidler S. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16. Springer International Publishing, 2020: 194-210.\n\n## 1.Abstract\n\n自动驾驶汽车的感知能力是从多个传感器中提取语义信息，并将这些信息融合到一个单一的“鸟瞰”视角坐标系中，从而供路径规划使用。本文提出一种新的端到端网络架构，其可以直接从任意数量的相机视图中提取场景的俯视图。本文方法背后的核心思想是将每个图像的特征单独“提升”到每个相机的视锥中，然后将所有视锥“分散”到光栅化的俯视图网格中。从实验结果来看，该模型不仅能够学习如何表示图像，还能够学习如何将来自所有摄像机的预测融合到场景的单个俯视图表示中，同时对校准误差具有鲁棒性。在标准的鸟瞰视角任务上，如物体分割和地图分割，该模型优于所有基线和先前的工作。为了使其能够学习路径规划的密集表示的目标，本文证明了该模型能够通过将模板轨迹“射击”到网络输出的鸟瞰成本图中来实现可解释的端到端运动规划。\n\n## 2.Method\n\n![](https://github.com/1248818919/My_NOTE/blob/master/assets/Computer_Vision/Detect%20and%20Segment/LSS/pic1.jpg?raw=true)\n\n给定n张图片$\\{\\mathbf{X}_{k}\\in\\mathbb{R}^{3\\times H\\times W}\\}_{n}$，相机外参$\\mathbf{E}_k\\in\\mathbb{R}^{3\\times4}$,相机内参$\\mathbf{I}_k\\in\\mathbb{R}^{3\\times3}$，寻求其特征图BEV坐标系y∈RC×X×Y中场景的栅格化表示。外参矩阵和内参矩阵共同定义了从参考坐标(x, y, z)到局部像素坐标(h, w, d)对n个相机中的每一个的映射。在训练或测试期间，我们不需要使用任何深度传感器。\n\n### Lift：Latent Depth Distribution\n\n我们的模型的第一阶段对相机中的每个图像进行单独操作。单目传感器融合的挑战在于我们需要将深度转换为参考帧坐标，但与每个像素相关的“depth”本质上是模糊的。我们提出的解决方案是为每个像素在所有可能的深度生成表示。\n\n假设一张图片是（H,W）的大小，Lift操作首先为每个像素预测他们的Depth，这里的Depth是用D个点进行表示，然后是用Softmax获取概率最大的一个当成该像素的深度，表示为$\\{(h,w,d)\\in\\mathbb{R}^3|d\\in D\\}$\n\n### Splat: Pillar Pooling\n\n我们遵循pointpillars架构，通过“lift”步骤转换大的点云输出。“Pillar”是具有无限高度的voxels。我们分配并执行和池化以创建一个$C × H × W$张量，该张量可由标准CNN处理以进行鸟瞰推断。考虑到生成的点云的大小，效率对于训练我们的模型至关重要。我们不是填充每个支柱然后执行和池化，而是通过使用打包和利用和池化的“累积技巧”来避免填充。这个操作有一个解析梯度，可以有效地计算以加速自梯度.\n\n## 3.Code\n\n首先对图片进行编码。\n\n\n    class CamEncode(nn.Module):\n        def __init__(self, D, C, downsample):\n            super(CamEncode, self).__init__()\n            self.D = D\n            self.C = C\n\n            # 使用预训练的 EfficientNet-B0 模型作为特征提取器\n            self.trunk = EfficientNet.from_pretrained(\"efficientnet-b0\")\n\n            self.up1 = Up(320+112, 512) # # 创建一个上采样对象，输入通道数为 320+112，输出通道数为 512\n            self.depthnet = nn.Conv2d(512, self.D + self.C, kernel_size=1, padding=0)\n\n        def get_depth_dist(self, x, eps=1e-20):\n            return x.softmax(dim=1) # 将X第一个维度进行softmax操作，获取深度分布\n\n        def get_depth_feat(self, x): # 获取深度特征\n            x = self.get_eff_depth(x) #  # 调用 get_eff_depth 函数，获取有效的深度特征\n            # 将深度特征通过 depthnet 卷积层进行处理\n            x = self.depthnet(x)     \n\n            # x的前self.D输入get_depth_dist计算深度分布\n            depth = self.get_depth_dist(x[:, :self.D])\n            # 特征图 x 中取出从 self.D 到 self.D + self.C 的通道的数据，然后将其与深度分布 depth 进行相乘，生成新的特征图 new_x\n            new_x = depth.unsqueeze(1) * x[:, self.D:(self.D + self.C)].unsqueeze(2)\n\n            return depth, new_x\n\n        def get_eff_depth(self, x):\n        # adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n        endpoints = dict() # 定义一个字典，保存每一阶段的特征图\n \n        # Stem\n        x = self.trunk._swish(self.trunk._bn0(self.trunk._conv_stem(x))) # 卷积、批量归一化和 swish 激活函数一系列操作\n        prev_x = x\n \n        # Blocks\n        for idx, block in enumerate(self.trunk._blocks):\n            drop_connect_rate = self.trunk._global_params.drop_connect_rate\n            # 根据当前 block 的索引动态调整drop_connect_rate，一种正则化手段\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self.trunk._blocks) # scale drop connect_rate\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            # 如果当前 block 进行了下采样操作（即特征图的尺寸发生了变化），那么将前一个特征图保存到endpoints 字典中。\n            if prev_x.size(2) > x.size(2):\n                endpoints['reduction_{}'.format(len(endpoints)+1)] = prev_x\n            prev_x = x\n \n        # Head\n        endpoints['reduction_{}'.format(len(endpoints)+1)] = x  # 将最后一个特征图保存到 endpoints 字典中\n        x = self.up1(endpoints['reduction_5'], endpoints['reduction_4']) # 对最后两个阶段的特征图进行上采样操作\n        return x\n \n    def forward(self, x):\n        depth, x = self.get_depth_feat(x)\n \n        return x\n\n接下来是Lift操作\n\n    class LiftSplatShoot(nn.Module):\n        def __init__(self, grid_conf, data_aug_conf, outC):\n            # 网格配置 数据增强配置\n            super(LiftSplatShoot, self).__init__()\n            self.grid_conf = grid_conf\n            self.data_aug_conf = data_aug_conf\n            # 网格大小 起始位置 网格数量\n            dx, bx, nx = gen_dx_bx( self.grid_conf['xbound'],\n                                    self.grid_conf['ybound'],\n                                    self.grid_conf['zbound'],\n                                                  ) # xyzbound定义网格边界\n            # dbnx转换为pytorch的Parameter对象，且False规定不需要随着训练而更新此参数\n            self.dx = nn.Parameter(dx, requires_grad=False)\n            self.bx = nn.Parameter(bx, requires_grad=False)\n            self.nx = nn.Parameter(nx, requires_grad=False)\n\n            self.downsample = 16 # 下采样因子\n            self.camC = 64 # maybe摄像机的数量\n            self.frustum = self.create_frustum() # 创建视锥\n            self.D, _, _, _ = self.frustum.shape # 把视锥的第一个维度赋予D，占位符表:除却D参数不关心其值\n            self.camencode = CamEncode(self.D, self.camC, self.downsample)\n            self.bevencode = BevEncode(inC=self.camC, outC=outC)\n\n            # toggle using QuickCumsum vs. autograd\n            self.use_quickcumsum = True\n下采样方法有很多种，以下是一些常见的方法：\n\n- 最近邻插值（Nearest-neighbor interpolation）：这是最简单的一种方法，它直接选择最近的像素作为下采样后的像素值。\n\n- 双线性插值（Bilinear interpolation）：这种方法考虑了像素周围的四个像素，通过它们的加权平均值来计算下采样后的像素值。\n\n- 双三次插值（Bicubic interpolation）：这种方法考虑了像素周围的16个像素，通过它们的加权平均值来计算下采样后的像素值。\n\n- 平均池化（Average pooling）：这种方法计算一定区域内的像素值的平均值作为下采样后的像素值。这是卷积神经网络中常用的一种下采样方法。\n\n- 最大池化（Max pooling）：这种方法选择一定区域内的最大像素值作为下采样后的像素值。这也是卷积神经网络中常用的一种下采样方法。\n\n然后就是创建视锥\n\n    def create_frustum(self):\n        # make grid in image plane\n        ogfH, ogfW = self.data_aug_conf['final_dim'] # 从数据增强配置中获取图像的最终尺寸\n        fH, fW = ogfH // self.downsample, ogfW // self.downsample # 计算下采样的尺寸\n        ds = torch.arange(*self.grid_conf['dbound'], dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW)\n        D, _, _ = ds.shape\n        # torch.linspace(start, end, steps, dtype)创建一维等差数列 数列元素数量fw view:改变形状为三维1, 1, fW\n        # 为x,y轴上每一个像素点赋予坐标后，在深度方向上扩展网格，使其称为3d网格\n        xs = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW)\n        ys = torch.linspace(0, ogfH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW)\n \n        # D x H x W x 3\n        frustum = torch.stack((xs, ys, ds), -1) # torch.stack函数，将xs、ys和ds这三个张量在最后一个维度（由-1指定）上堆叠起来\n        return nn.Parameter(frustum, requires_grad=False)\n        \n接下来就是进行投影转换\n\n    def get_geometry(self, rots, trans, intrins, post_rots, post_trans):\n        \"\"\"Determine the (x,y,z) locations (in the ego frame)\n        of the points in the point cloud.\n        Returns B x N x D x H/downsample x W/downsample x 3\n        \"\"\"\n        \n        B, N, _ = trans.shape # 获取变换矩阵的形状，B：批次大小，N：相机数量\n        # undo post-transformation\n        # B x N x D x H x W x 3\n        # 将后置转换应用于视锥\n        points = self.frustum - post_trans.view(B, N, 1, 1, 1, 3)\n        # 取消后置变换，比如什么数据增强之类的操作\n        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1))\n \n        # cam_to_ego\n        #0 将点的x和y坐标乘以其Zc，然后将结果与Zc连接起来：得到新的坐标且深度都为1\n        # 摄像机坐标-->归一化摄像机坐标\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],\n                            points[:, :, :, :, :, 2:3]\n                            ), 5)\n \n        # 计算旋转矩阵和内参矩阵的逆的乘积\n        #1 ：见代码下方解释\n        # cots与内参矩阵作乘法赋予combine\n        # 此时combine成为变换矩阵：能实现图像坐标系到世界坐标系的转换\n        combine = rots.matmul(torch.inverse(intrins))\n        # 将combine改变维度与points作乘法，完成坐标系转换\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n        # 完成张量的平移——3D空间\n        points += trans.view(B, N, 1, 1, 1, 3)\n        # B N H W 3 最后的[1]被删除 3————xyz\n \n        return points\n        \n获取相机提取的特征\n\n    def get_cam_feats(self, x): \n        # 对输入的图像进行特征提取，并将提取的特征映射回原来的空间\n        \"\"\"Return B x N x D x H/downsample x W/downsample x C\n        \"\"\"\n        # 获取X图像尺寸信息后，改变维度\n        B, N, C, imH, imW = x.shape\n        x = x.view(B*N, C, imH, imW)\n        # 卷积编码：提取图像特征\n        x = self.camencode(x)\n        # 特征提取完毕，恢复输入形状：6维-->4维\n        x = x.view(B, N, self.camC, self.D, imH//self.downsample, imW//self.downsample)\n        # 调整张量的维度顺序，permute的作用\n        x = x.permute(0, 1, 3, 4, 5, 2)\n \n        return x\n\n池化操作\n\n\n    def voxel_pooling(self, geom_feats, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B*N*D*H*W\n \n        # flatten x\n        x = x.reshape(Nprime, C)\n \n        # flatten indices\n        # 将几何特征的值调整到一个新的范围后转为长整数型，变换几何特征维度\n        geom_feats = ((geom_feats - (self.bx - self.dx/2.)) / self.dx).long()\n        geom_feats = geom_feats.view(Nprime, 3)\n        # 创建批次，以记录每个几何特征对应的编号\n        #2\n        batch_ix = torch.cat([torch.full([Nprime//B, 1], ix,\n                             device=x.device, dtype=torch.long) for ix in range(B)])\n        # 拼接几何特征和批次索引 \n        geom_feats = torch.cat((geom_feats, batch_ix), 1)\n\n #2 ：创建一个形状为[Nprime//B, 1]、值全为ix的张量。这个张量的设备和数据类型与输入的x相同。 .full:torch.full(size, fill_value, dtype=None, device=None, requires_grad=False)，0:新张量的形状 1：标量，新张量元素的值 3：默认全局设备 4：是否计算新张量梯度。默认False\n\n        # filter out points that are outside box\n            kept = (geom_feats[:, 0] >= 0) & (geom_feats[:, 0] < self.nx[0])\\\n                & (geom_feats[:, 1] >= 0) & (geom_feats[:, 1] < self.nx[1])\\\n                & (geom_feats[:, 2] >= 0) & (geom_feats[:, 2] < self.nx[2])\n            # 过滤掉给定范围之外的点\n            x = x[kept]\n            geom_feats = geom_feats[kept]\n        \n[kept]布尔型的掩码，该掩码表示每个几何特征是否在给定的范围。x、y、z坐标是否在[0, self.nx[0])、[0, self.nx[1])、[0, self.nx[2])这三个区间内。x = x[kept]这行代码是在使用kept来索引x，也就是说，它会创建一个新的x，这个新的x只包含原来x中对应kept中值为True的元素。\n\n        # get tensors from the same voxel next to each other\n            # 分别计算每个几何特征在xyz、批次方向的索引，加权求和得到现行索引ranks\n            ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B)\\\n                + geom_feats[:, 1] * (self.nx[2] * B)\\\n                + geom_feats[:, 2] * B\\\n                + geom_feats[:, 3]\n            # 将ranks中的元素进行从小到大排序，并返回相应序列元素的数组下标\n            sorts = ranks.argsort()\n\n            # 将x, geom_feats, ranks数组元素按照从小到大排列\n            # # x[sorts]这行代码是在使用sorts来索引x。它会创建一个新的x，这个新的x的元素顺序与sorts中的元素顺序相对应\n            x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts]\n\n            # cumsum trick\n            # 根据self.use_quickcumsum的值来选择使用cumsum_trick函数还是QuickCumsum.apply方法来计算累积和\n            if not self.use_quickcumsum:\n                x, geom_feats = cumsum_trick(x, geom_feats, ranks)\n            else:\n                x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks)\n            # griddify (B x C x Z x X x Y)\n            # 创建零向量，形状为B x C x Z x X x Y\n            final = torch.zeros((B, C, self.nx[2], self.nx[0], self.nx[1]), device=x.device)\n            #3\n            final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 0], geom_feats[:, 1]] = x\n\n            # collapse Z\n            # 将final沿着第三个维度（即Z维度）解绑，得到一个包含多个张量的元组，每个张量都是final在Z维度上的一个切片。Z维度上的信息就被折叠到了C维度上，Z维度的大小变为1，而C维度的大小变为原来的C维度和Z维度的大小之和\n            final = torch.cat(final.unbind(dim=2), 1)\n\n            return final\n\n总结整哥pooling代码块的作用：\n\n1.算每个几何特征在xyz、批次方向的索引，加权求和得到线性索引ranks。\n\n2.将ranks中的元素进行从小到大排序，并返回相应序列元素的数组下标sorts。\n\n3.将x、geom_feats、ranks数组元素按照从小到大排列。\n\n4.根据self.use_quickcumsum的值来选择使用cumsum_trick函数还是QuickCumsum.apply方法来计算累积和。\n\n5.创建一个全零的五维张量final，形状为(B, C, self.nx[2], self.nx[0], self.nx[1])。\n\n6.将x中的值赋给final的特定位置。这些位置由geom_feats的各列决定。\n\n7.将final沿着第三个维度（即Z维度）解绑，得到一个包含多个张量的元组，每个张量都是final在Z维度上的一个切片。然后将这些张量沿着第二个维度（即C维度）拼接起来。这样，原来在Z维度上的信息就被折叠到了C维度上，Z维度的大小变为1，而C维度的大小变为原来的C维度和Z维度的大小之和。\n\n8.返回处理后的final张量。\n","tags":[],"folderPathname":"/Computer Vision/Detection","data":{},"createdAt":"2024-04-07T02:26:07.567Z","updatedAt":"2024-04-07T03:47:44.776Z","trashed":false,"_rev":"fIuEr0I-C"}